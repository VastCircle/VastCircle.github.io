[{"content":"概述 TileLink Cached (TL-C) 协议通过为主设备提供缓存共享数据块副本的能力，完善了 TileLink 协议。这些本地缓存副本必须根据实现定义的一致性策略保持一致性。本章节定义的 TL-C 标准一致性协议规定了哪些内存访问操作可以对缓存数据的副本执行，以及哪些消息可用于传输数据块的副本。实现中叠加的一致性策略则规定了在接收到内存访问操作后，如何在特定的 TileLink 代理网络中传播副本和权限。具体一致性策略的描述超出了本文档的范围。\n总的来说，TL-C 在 TileLink 协议规范中增加了以下内容：\n三种新操作 三个新通道 一种新的五步消息序列模板 十种新消息类型 新增的操作是用于创建或删除数据块缓存副本的传输操作。这些传输操作不会修改数据块的值，而是转移副本的读/写权限。传输操作与之前定义的 TL-UL 和 TL-UH 内存访问操作无缝协作，且二者在顺序上是串行化的。因为每个传输操作逻辑上要么发生在内存访问操作之前，要么发生在之后，并且所有代理对这种顺序一致认可，因此在 TileLink 网络中保持了一致性不变量。\n当内存访问操作通过 TileLink 网络时，中间缓存可能在其中嵌套递归传输操作。缓存通过首先使用传输操作获得数据块的足够权限，然后利用其一致的本地副本来处理内存访问操作。\n地址的“可缓存性”是一个属性，TileLink 的实现必须防止创建不可缓存地址的副本（见第 6.3 节）。相反，之前在 TL-UL 和 TL-UH 中定义的内存访问操作可以被主设备用来访问可缓存地址，而无需自行缓存它们。某些主设备可能选择缓存某个数据块，而同一内存层级中的其他主设备可能选择不缓存。\n下一节将概述可供设计者用于定义特定实现依赖一致性策略的基本操作、消息和权限。该规范并未强制使用某一种特定策略，而是定义了一个可供构建策略的协议基础。\n使用 TileLink 实现缓存一致性 所有基于Tilelink的一致性协议都由传输权限以读取和写入数据块的副本组成。内存访问操作需要agent获取正确的权限，然后agent才能将访问操作应用于缓存的副本。当agent想要在本地处理访问操作时，必须首先使用**传输操作(Transfer operations)**来获取必要的权限。传输操作通过网络创建或删除副本，从而修改每个副本提供的权限。\n代理的块副本拥有权限，“None\u0026quot;,\u0026ldquo;Read\u0026rdquo;,\u0026ldquo;Read+Write\u0026rdquo;。对于任何给定地址，任何给定master和拥有该地址的slave之间只有一条路径。当所有此类路径覆盖在 TileLink 网络 DAG 上时，它们会形成一棵树，其根为单个从属路径。对于每个地址，该树包含所有针对该地址的操作执行的路径。如果我们删除所有不能缓存数据的代理，我们就会留下一棵缓存代理树，描述可能缓存特定地址数据的所有位置。\n在逻辑时间的任何给定时刻，这些代理的某些子集实际上包含缓存数据的副本。这些代理形成了 CoherenceTree。包容性的 TileLink 一致性协议要求树根据内存访问操作而增长和收缩。图中的每个节点都属于描述其在树上位置的四个类别之一：\nNothing: 当前不缓存数据副本的节点。既没有读权限，也没有写权限。\nTrunk :具有缓存副本的节点，位于 Tip 和 Root 之间的路径上。对副本既没有读权限，也没有写权限。对于提示处发生的写入，该副本可能已过时。\nTip（with no Branches）:具有缓存副本的节点，用作内存访问序列化点。对其副本具有读/写权限，该副本可能包含脏数据。\nTip（with Branches）:具有缓存副本的节点，用作写入序列化点。对其副本具有读取和写入权限，该副本可能包含过去写入的脏数据。\nBranch: 具有位于提示上方的缓存副本的节点。对其副本具有只读权限。\n“TT”代 表 Trunk Tip，“T”代 表 Trunk，“B”代表Branch。\nA：根对唯一副本具有写+读权限。\nB：单个master对trunktip有写+读权限。\nC：多个master对分支有读权限。\nD：多个master对分支有读权限，部分分支被剪枝\n一致性树按照内存、L3、L2、L1的顺序自下而上生长，内存作为根节点拥有可读可写的权限，在每一层中子节点的权限都不能超过父节点的权限。其中TT代表拥有T权限的枝杈上的叶子节点，说明该节点上层只有N或B权限，相反T权限而不是TT权限的节点代表上层一定还有T/TT权限节点。\noperation three new operations are termed transfer operations (move permissions or cached copied of data through the network)\nAcquire\n在请求的主设备中创建数据块的新副本（或特定权限）。\nRelease\n将数据块的副本（或特定权限）从请求的主设备返还给从设备。\nProbe\n强制从主设备移除数据块的副本（或特定权限），并交给请求的从设备。\n获取（Acquire）操作通过延伸主干或从现有分支或末端添加新分支来扩展树结构。为了实现这一点，可能需要通过递归探测（Probe）操作修剪旧的主干或分支，才能生长新的分支。释放（Release）操作则通过自愿缩减树结构来修剪树，通常是为了响应缓存容量冲突。\nChannels 通道 A master发起获取读取或写入缓存数据块副本的权限。\n通道 B slave查询或修改master在缓存数据块上的权限，或将内存访问请求转发给master。\n通道 C master确认通道 B 消息，可能会释放数据块上的权限以及任何脏数据。也用于自愿回写脏缓存数据。\n通道 D\nslave向原始请求者提供数据或权限，授予访问缓存数据块的权限。也用于确认脏数据的自愿回写。\n通道 E master提供交易完成的最终确认，用于slave进行交易序列化。\nTL-C Messages 包含三个操作的十个消息\nPermissions Transitons 传输在逻辑上对权限进行操作，因此包含它们的消息必须指定预期结果：升级到更多权限、降级到更少权限或保持权限不变的无操作。这些变化是根据它们对特定地址的一致性树的形状的影响来指定的。我们将可能的权限转换集分为六个子集；不同的子集可用作某些消息的参数，如下小节中所定义。\nPrune 包括权限降级操作，缩小树结构，并记录先前的权限和新的较低权限。\nGrow 包括权限升级操作，扩展树结构，并记录先前的权限和新的较高权限。\nReport 包括不进行操作的情况，其中权限保持不变，但报告当前的权限状态。\nCap 包括权限变更操作，不指定原始权限是什么，而是只说明它们应该变成什么\nFlows and Waves 下图概括了3个新流程，Acquire总是触发a recursive Grant request and GrantAck response。根据块权限的状态和一致性策略，Acquire 也能触发一个或者多个Release or Probe operations.黑点的移动表明事务序列化点已受到该操作的影响。\n下图显示了一个消息流，该消息流更详细地说明了包含所有三个新操作的事务。在此流程中，主设备通过获取在目标数据块的本地副本中读取或写入数据的许可来对存储器访问操作请求做出反应。此事务完成后，主节点已获得读取或写入缓存块以及该块数据的副本的权限。其他主机被probed，迫使他们释放对该块的权限并写回其拥有的脏数据。此外，发出 Acquire 的 master 还使用 Release 来自愿释放其对缓存块的权限。通常，当高速缓存必须逐出包含脏数据的块，以便用重新填充到高速缓存中的块替换它时，就会发生这种类型的事务。此事务完成后，主服务器将失去读取或写入第二个缓存块及其数据副本的权限。如果从属设备能够使用目录跟踪哪些主设备拥有该块的副本，则此元数据已更新以反映两个块的权限更改。l\n把master1 看成l1,slave看成l2\n1.缓存主设备向从设备发送 Acquire\n2.为了为预期的响应腾出空间，同一个主设备发送 Release\n3.从设备与后备存储(backing memory)通信（如果需要）\n4.从设备使用 ReleaseAck 确认写回事务完成\n5.从设备还向其他主设备发送必要的 Probes。\n6.从设备等待接收每个发送的 Probe 的 ProbeAck\n7.从设备与后备存储通信（如果需要）。\n8.从设备向原始请求者发送 Grant\n9.原始主设备通过 GrantAck 响应，从而完成事务。\nTileLink 协议基于三个消息流：Acquire、Release 和 Grant（或类似的），这些流构成了所有涉及缓存块传输的事务基础。然而，当这些流在时间上交错或层次上组合时，会出现一些边界情况。接下来讨论的是 主设备 和 从设备 之间如何分配并发管理的责任。\nTileLink 协议并不假定存在点对点的有序消息传递。实际上，高优先级的消息可以绕过低优先级的消息，即使它们的目标是同一个设备。这意味着，从设备 充当了一个便利的同步点，所有与它连接的 主设备 都通过它来协调消息流。\n由于每个事务必须通过向从设备发送 Acquire 消息来发起，因此从设备能够方便地对事务进行排序。一个非常安全的实现方式是让从设备一次只接受一个事务，但是这样做的性能影响非常严重，因此实际上我们可以通过限制代理行为，依然保持正确的事务顺序，同时提高并发性。\n通过对代理行为施加一些限制，我们能够保证即使问题是分布式的，也能够构建出事务的总顺序。图 28 提供了关于每种操作并发性限制的概述。\nTileLink 代理的并发限制最容易通过发起或阻止请求消息来理解。每个请求消息都会生成响应消息，而响应消息最终都能向前推进。但是，在某些条件下，针对同一数据块的递归请求消息 在未接收到未处理的响应消息之前，不应被发出。\nAcquire（获取） 主设备如果该块上有挂起的Grant（授权），则不应发起Acquire请求。发起Acquire后，主设备在收到Grant之前，不应再对该块发起其他Acquire请求。\nGrant（授权） 从设备如果该块上有挂起的ProbeAck（探针确认），则不应发起Grant授权。一旦发出Grant授权，从设备在收到GrantAck（授权确认）之前，不应对该块发起进一步的Probe请求。\nRelease（释放） 主设备如果该块上有挂起的Grant授权，则不应发起Release请求。发出Release后，主设备在收到Slave确认完成写回的ReleaseAck之前，不应再发起ProbeAcks、Acquires或其他Release请求。\nProbe（探针） 从设备如果该块上有挂起的GrantAck授权确认，则不应发起Probe请求。一旦发出Probe请求，从设备在收到ProbeAck（探针确认）之前，不应对该块发起进一步的Probe请求。\n总而言之就是如果有*ack的话，就不要发起*\n未完\n附录 参考文献 tilelink_spec_1.8.1\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-11-22T16:29:39+08:00","permalink":"https://VastCircle.github.io/2024/%E4%BD%BF%E7%94%A8tilelink%E5%AE%9E%E7%8E%B0%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/","title":"使用Tilelink实现缓存一致性（cache conherence）"},{"content":"多核心的一致性问题 在一个核心修改Cache数据后，如何同步给其他核心Cache\n1、Core 1 和 Core 2 读取了同一个内存块的数据，在两个 Core 都缓存了一份内存块的副本。此时，Cache 和内存块是一致的；\n2、Core 1 执行内存写入操作：\n2.1 在写直达策略中，新数据会直接写回内存，此时，Cache 和内存块一致。但由于之前 Core 2 已经读过这块数据，所以 Core 2 缓存的数据还是旧的。此时，Core 1 和 Core 2 不一致； 2.2 在写回策略中，新数据会延迟写回内存，此时 Cache 和内存块不一致。不管 Core 2 之前有没有读过这块数据，Core 2 的数据都是旧的。此时，Core 1 和 Core 2 不一致。 3、由于 Core 2 无法感知到 Core 1 的写入操作，如果继续使用过时的数据，就会出现逻辑问题。\n需要一种机制，将多个核心的工作联合起来，共同保证多个核心下Cache一致性\n4.1 写传播 \u0026amp; 事务串行化 缓存一致性机制需要解决的问题就是 2 点：\n特性 1 - 写传播（Write Propagation）： 每个 CPU 核心的写入操作，需要传播到其他 CPU 核心； 特性 2 - 事务串行化（Transaction Serialization）： 各个 CPU 核心所有写入操作的顺序，在所有 CPU 核心看起来是一致。 第 1 个特性解决了 “感知” 问题，如果一个核心修改了数据，就需要同步给其它核心，很好理解。但只做到同步还不够，如果各个核心收到的同步信号顺序不一致，那最终的同步结果也会不一致。\n举个例子：假如 CPU 有 4 个核心，Core 1 将共享数据修改为 1000，随后 Core 2 将共享数据修改为 2000。在写传播下，“修改为 1000” 和 “修改为 2000” 两个事务会同步到 Core 3 和 Core 4。但是，如果没有事务串行化，不同核心收到的事务顺序可能是不同的，最终数据还是不一致\n4.2 总线嗅探 \u0026amp; 总线仲裁 写传播和事务串行化在 CPU 中是如何实现的呢？\n写传播 - 总线嗅探： 总线除了能在一个主模块和一个从模块之间传输数据，还支持一个主模块对多个从模块写入数据，这种操作就是广播。要实现写传播，其实就是将所有的读写操作广播到所有 CPU 核心，而其它 CPU 核心时刻监听总线上的广播，再修改本地的数据； 事务串行化 - 总线仲裁： 总线的独占性要求同一时刻最多只有一个主模块占用总线，天然地会将所有核心对内存的读写操作串行化。如果多个核心同时发起总线事务，此时总线仲裁单元会对竞争做出仲裁，未获胜的事务只能等待获胜的事务处理完成后才能执行 MESI协议 MESI 协议其实是 CPU Cache 的有限状态机，一共有 4 个状态（MESI 就是状态的首字母）：\nM（Modified，已修改）： 表明 Cache 块被修改过，但未同步回内存； E（Exclusive，独占）： 表明 Cache 块被当前核心独占，而其它核心的同一个 Cache 块会失效； S（Shared，共享）： 表明 Cache 块被多个核心持有且都是有效的； I（Invalidated，已失效）： 表明 Cache 块的数据是过时的。 在 “独占” 和 “共享” 状态下，Cache 块的数据是 “清” 的，任何读取操作可以直接使用 Cache 数据；\n在 “已失效” 和 “已修改” 状态下，Cache 块的数据是 “脏” 的，它们和内存的数据都可能不一致。在读取或写入 “已失效” 数据时，需要先将其它核心 “已修改” 的数据写回内存，再从内存读取；\n在 “共享” 和 “已失效” 状态，核心没有获得 Cache 块的独占权（锁）。在修改数据时不能直接修改，而是要先向所有核心广播 RFO（Request For Ownership）请求 ，将其它核心的 Cache 置为 “已失效”，等到获得回应 ACK 后才算获得 Cache 块的独占权。这个独占权这有点类似于开发语言层面的锁概念，在修改资源之前，需要先获取资源的锁；\n在 “已修改” 和 “独占” 状态下，核心已经获得了 Cache 块的独占权（锁）。在修改数据时不需要向总线发送广播，能够减轻总线的通信压力。\n事实上，完整的 MESI 协议更复杂，但我们没必要记得这么细。我们只需要记住最关键的 2 点：\n关键 1 - 阻止同时有多个核心修改的共享数据： 当一个 CPU 核心要求修改数据时，会先广播 RFO 请求获得 Cache 块的所有权，并将其它 CPU 核心中对应的 Cache 块置为已失效状态； 关键 2 - 延迟回写： 只有在需要的时候才将数据写回内存，当一个 CPU 核心要求访问已失效状态的 Cache 块时，会先要求其它核心先将数据写回内存，再从内存读取。 提示： MESI 协议在 MSI 的基础上增加了 E（独占）状态，以减少只有一份缓存的写操作造成的总线通信。\nMESI 协议有一个非常 nice 的在线体验网站，你可以对照文章内容，在网站上操作指令区，并观察内存和缓存的数据和状态变化。网站地址：www.scss.tcd.ie/Jeremy.Jone…\n4.4 写缓冲区 \u0026amp; 失效队列 MESI 协议保证了 Cache 的一致性，但完全地遵循协议会影响性能。 因此，现代的 CPU 会在增加写缓冲区和失效队列将 MESI 协议的请求异步化，以提高并行度：\n写缓冲区（Store Buffer） 由于在写入操作之前，CPU 核心 1 需要先广播 RFO 请求获得独占权，在其它核心回应 ACK 之前，当前核心只能空等待，这对 CPU 资源是一种浪费。因此，现代 CPU 会采用 “写缓冲区” 机制：写入指令放到写缓冲区后并发送 RFO 请求后，CPU 就可以去执行其它任务，等收到 ACK 后再将写入操作写到 Cache 上。\n失效队列（Invalidation Queue） 由于其他核心在收到 RFO 请求时，需要及时回应 ACK。但如果核心很忙不能及时回复，就会造成发送 RFO 请求的核心在等待 ACK。因此，现代 CPU 会采用 “失效队列” 机制：先把其它核心发过来的 RFO 请求放到失效队列，然后直接返回 ACK，等当前核心处理完任务后再去处理失效队列中的失效请求。\n事实上，写缓冲区和失效队列破坏了 Cache 的一致性\nCore1 指令 a = 1; // A1 x = b; // A2 Core2 指令 b = 2; // B1 y = a; // B2 因为写缓存区和失效队列允许在当前指令未完成的时候CPU继续去运行，所有就可能会破坏指令的执行顺序\n就像下面A2读取的是没有写入的数据，但是这样其实只要能够从写缓存区和Cache中去读取就行了，问题不是特别大\n附录 参考文献 12图看懂CPU缓存一致性问题\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-11-19T20:03:48+08:00","permalink":"https://VastCircle.github.io/2024/cache%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/","title":"Cache缓存一致性"},{"content":"Cache的一般设计 Cache line = Cache data block + Cache Tag ,如果一个数据可以存储在Cache中的多个地方,能够被同一个地址找到的多个Cache Line 称为Cache Set\nCache缺失的原因(3C定理:\n(1)Compulsory , 第一次访问的指令或数据肯定不会在Cache中,\n(2)Capcity,容量,\n(3)Conflict,冲突\nCache的组成方式 直接相连 组相联 并行访问和串行访问 并行访问\n当Tag的地址被读取的同时,Data部分的所有数据也能够被读出来,送到一个多路选择器中,这个多路选择器受到Tag比较结果的控制,选出对应的Data block,然后根据Block offset的值,选择合适的字节,选择字节的过程称为数据对齐(Data Alignment)\n对于i-cache,流水线的结构不会有太大的影响,可以实现每周期读取指令,因为读取指令本身就是连续的,但是对于D-cache,会增大load的延时\n串行访问\n首先访问Tag SRAM,根据Tag比较的结果,直接去访问对应的way ,这样就不需要多路选择器了,可以节省功耗\n相比于并行会增加一个周期,但是它也降低了同时访问Tag SRAM和Data RAM的延迟\n全相连 Cache的写入 在一般的RISC处理器中,Icache都不会被直接写入内容,即使是有自修改指令,也需要借助D-cache,将要改写的指令作为数据写到D-cache中,然后将D-cahce中的内容写到下级存储器中(例如L2-cache,这个存储器是被i-cache和d-cache共享的,这个过程称为clean),并将I-cache的所有内容置为无效,这样处理器再次执行的时候,就会去读取修改过的指令\nWrite Through 和 Write Back\nWrite Allocate,在发生write miss之后先把下级数据写入D-cache,再将数据写入D-cache\nNon-Write Allocate ,发生write miss 直接写入内存\nWrite Through 配合 Non-Write Allocate\nCache的替换策略 LRU(Least Recently Used) 选择最近被使用次数最少的Cache line,这样需要为每一个Cache Line设置一个Age部分,每当一个Cache line被访问时,对应的年龄部分会增大,进行替换时,替换年龄最少的那个,实际上,为了减少代价,会使用\u0026quot;伪LRU\u0026quot;的方法,将所有的way进行分组,每一组使用一个1位的年龄部分\n类似于二分法,三级年龄位可以分8个way\n随机替换 可以通过时钟算法实现近似的随机\n提高Cache的性能 Write buffer 可以将dirty的数据先放到write buffer里,等到下一级存储器有空闲的时候,将write buffer的数据写到下一级存储器中\n当读取D-cache发生缺失时,不仅需要从下级存储器中查找数据,还需要在write buffer中也查找\n流水线 对于写D-Cache来说,读取Tag SRAM和写Data SRAM只能串行的完成.\n可以将Tag SRAM的读取和比较放在一个周期,写Data SRAM放在下一个周期\n对于store指令,需要两个周期来完成,如果连续的执行store指令,任然可以获得没周期执行一条store指令的效果\n当执行load指令时,可能出现load需要的数据正好在store指令的流水线寄存器中,因此还需要一种机制来检查load指令携带的地址和store指令的流水线寄存器\n多级结构 一般在处理器中.L2 cache会使用write back的方式,但是对于L1 cache来说,write through也可以接受,可以简化流水线的设计,也便于在多核的环境中,管理存储器之间的一致性\ninclusive 和 exclusive Inclusive : 如果 l2包括了l1中的所有内容,称l2 cache是inclusive的\nexclusive:如果l2 cache 和l1 cache的内容互不相同,称l2 cache是exclusive的\ninclusive类型的cache,在覆盖l1 cache不会存在问题;inclusive类型的cache也简化了一致性(coherence)的管理,例如在多核的处理器中,当其中一个处理器改变了存储器中一个地址的数据时,如果在其他处理器的私有Cache中也保存了地址的数据,需要置为无效,此时只需要检查最低级的存储器,因为如果l2 cache没有数据,l1 cache中必然没有数据\nVictim Cache 有时候,cache中被踢出的数据在之后可能马上又要被使用,比方说对于2-way的Cache,有3个数据恰好位于同一个Cache set\nVictim Cache可以保存最近被踢出Cache的数据,因此所有的Cache set都可以利用它来提高way的个数,通常Victim Cache采用全相连的方式,容量比较小,Victim本质就是增加了Cache中way的个数,能够避免多个数据竞争Cache中有限的位置,从而降低Cache的缺失率.一般来说Victim Cache和 Cache的数据是互斥的,可以同时去查找Victim Cache和Cache,如果Cache中没有数据的话,就使用Victim Cache的数据\nFilter cache,当一个数据第一次被使用,会放在Filter cache中,当数据被再次使用,才放在Cache中,这样可以避免偶然使用的数据,从而提高Cache的利用效率\nimage-20241116195003330 预取 可以使用预取（prefetching)来猜测处理器在以后可能使用什么指令和数据，提前将其放到Cache中，可以通过硬件完成，也可以通过软件完成。\n硬件预取 在访问I-cache的一个数据块的时候，可以将它后面的数据块也取出来放在I-cache,但是由于分支预测指令的存在，可能会使得不被使用的指令进入I-cache,一方面降低I-cache实际可用的容量，一方面有占用了本来可能有用的指令，称为“cache 污染” ，可以先将预取的指令放在一个单独的缓存中（stream Buffer)\n软件预取 在程序的编译阶段，编译器可以对程序分析，进而知道哪些数据是需要进行预取的，在程序中设置预取指令，就可以在计算时直接从D-cache中找到需要的数据，这种预取需要把握预取的时机，\n并且使用软件预取时，在执行预取指令的时候，处理器需要继续执行，也就是继续能够从D-cache中取数据，而不能让预取指令阻碍后面指令的执行，这要求D-cache是non-blocking结构的\n在实现虚拟存储器器的系统中，预取指令可能会引起异常，例如发生Page Fault,虚拟地址错误（Virtual Address Falut）或者保护违例（Protection Violation),此时有两种选择，如果对异常进行处理，称这种预取指令为处理错误的预取指令（Faulting Prefetch Instruction),反之，如果不对异常处理并且抛弃掉这条预取指令，称这种预取指令为不处理错误的预取指令（NonFaulting Prefetch Instruction),此时发生异常的预取指令会变成空指令\n多端口Cache 在超标量处理器中，为提高性能，处理器需要能够每周期执行多条load/store指令，这需要多端口D-Cache,需要一些特殊的方式来避免多端口对芯片的面积和速度带来很大的负面影响\nTrue Multi-port 真正的多端口需要所有在Cache中的控制通路和数据通路进行复制，这表示它有两套地址解码器（Address Decoder),使得两个端口可以同时寻址Tag SRAM和Data SRAM,有两个多路选择器，用来同时读取两个端口的数据，比较器的数量也需要同时增加一杯，用来判断两个端口的命中情况，同时需要两个对齐器（Aligner).\nSRAM中的每个Cell也需要同时支持两个并行的读取操作。因此需要更长的访问时间，功耗也会随之增大。\nMultiple Cache Copies 和上面类似，将Cache进行复制，SRAM将不使用多端口的结构，可以基本消除对处理器周期的影响，但是浪费了很多面积，而且需要保存两个Cache的同步，即需要保证两个Cache是完全一致的\nMulti-banking 将Cache分为很多个小bank,每个bank只有一个端口，如果一个周期之内，Cache的多个端口上的访问地址位于不同bank之中，没有任何问题，当端口的地址位于同一个bank之中时，会引起bank冲突（bank conflict)\n这种方法仍然需要两个地址解码器，有两个多路选择器，两套比较器，两个对齐器（Aligner)。此时Data SRAM不需要实现多端口结构了（一个地址只会读一次），提高了速度减少了面积。但是由于需要判断Cache的每个端口是不是命中，对于Tag SRAM来说，仍旧需要提供多个端口同时读取的功能（why?），即采用多端口SRAM,或者将单端口SRAM进行复制\n当端口冲突时，当前周期Cache只能对一个端口进行响应\n可以采用更多的bank来降低bank冲突的概率，由于每个端口都会访问所有的bank,那需要更多的布线资源\nAMD opteron 的多端口 Cache 40位物理地址，48位虚拟地址\n数据块的大小是64Bits,需要6字节寻址找到其中的某一个字节，每个数据块被分为8个独立的bank,每个bank都是64位的单端口SRAM\n整个Cache的大小是64KB,采用两路组相连，每一路大小为32KB.使用“Virtually-indexed,Pyhsically-tagged\u0026quot;的实现方式，可以使用虚拟地址寻址Cache,每一路是32KB的大小，需要VA[14:0]进行寻址，前面6位寻址块内字节，所以VA[14:6]用来寻址Cache set ,\n由于每个Cache line划分为8个bank,所以使用VA[5:3]来找到某一个bank,剩下的VA[2:0]用来找到某个字节，这种方式将两个连续的64位数据放在两个相邻的不同bank中，对它访问不会存在冲突。\n大小为64KB,两路组相连，每一路都有8个4KB的bank,整个cache有16个4KB的bank,Cache的每一个端口访问时，都会同时访问两个way的数据，然后根据tag去选择命中了哪个，所以Cache的一个端口访问时，会同时访问到两个bank,每个way各一个，\nPA[39:12]就是tag部分，用来判断哪个way是命中的，每一个端口都有一个TLB.每个端口都要同时读取两个way的tag进行比较，理论上来说，每个way的Tag SRAM需要支持两个读端口，这个Cache采用将Tag SRAM复制的方法\n虚拟地址的[11:0]寻址page内部，[47：12]寻址TLB,对应PFN[39:12],它用来和Cache对应的Tag部分比较，来判断是否命中\n该Cache相比于单端口，需要两个TLB,两个Tag比较电路，两倍的Tag SRAM,除了Data SRAM没有被复制，其他都被复制了，所以面积大，但是速度快。\n附录 参考文献 版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-11-16T16:00:54+08:00","permalink":"https://VastCircle.github.io/2024/cache/","title":"Cache"},{"content":"概述 虚拟存储器的思想是对于一个程序来说,它的程序(code),数据(data)和堆栈(stack)的总大小可以超过实际物理内存的大小,操作系统把当前使用的部分内容放在物理内存中,而把其他未使用的内容放在更下一级的存储器中\n虚拟存储器空间的大小由处理器的位数决定,对于32位处理器,地址范围就是0~0xFFFFFFFF,就是4GB ,这些地址就是虚拟地址\n和虚拟存储器相对应的就是物理存储器,是在现实世界中能够使用的存储器,其中的地址就是物理地址\nMMU(memory manage unit,MMU)内存管理单元\n使用物理地址,运行程序时需要为每个程序分配一块地址空间,每个程序需要在地址空间中进行运行\n使用虚拟地址,每个程序会认为它独占了整个地址空间,这样在编写程序时不需要考虑地址的限制,由操作系统负责调度,将物理存储器动态分配给各个程序,将每个程序的虚拟地址转为物理地址.\n虚拟地址还可以带来保护(protect)和共享(share)\n地址转化 -基于分页(page)的虚拟存储器 典型的页大小是4KB ,物理地址中称为frame,page和frame的大小必须相等\n程序运行时会进行把程序从硬盘搬移到物理内存中,每次搬移的单位就是一个页\nVA[11:0]表示页内的位置,称为page offset ,VA剩余的部分表示哪个页,称为VPN(Virtual Page Number)\nPA[11:0]表示frame的位置,称为frame offset,剩余部分表示哪个frame,称为PFN(Physical Frame Number)\n从VPN到PFN,offset不需要变化\n比方说 page0 -\u0026gt; frame 2 ,offset = 4 , 那只需要把VPN换成PFN ,就是把0换成2,\n如果程序的内容没有存储在物理内存中,MMU就会产生Page Fault的异常给处理器,处理器通过异常处理程序(操作系统的一部分代码)找到替换的frame ,需要先解除frame 和 page的映射,然后把硬盘的内容搬移到frame,然后添加frame和page的映射,如果被替换的frame是dirty的,还需要先将内容搬移到硬盘中,处理完成之后返回发生异常的指令重新执行\n单级页表(线性页表) 页表(page Table,PT)用来存储从虚拟地址到物理地址的映射关系,一般页表是放在物理内存中的,需要使用虚拟地址寻址,页表内被寻址到的内容就是这个虚拟地址对应的物理地址,处理器中会有一个页表寄存器(Page Table Register,PTR)来存放当前运行程序的页表在物理内存中的起始位置,每次操作系统将程序调入物理内存都会去将PTR设置好\n两次内存访问,先使用虚拟地址访问页表,再使用物理地址进行寻址\n使用PTR和虚拟地址共同寻址页表,相当于使用它们共同组成一个地址,用来寻址物理内存\nvalid用来指示当前的page是否在物理内存中\n下图展示如何通过PTR从物理内存中定位一个页表,并且使用虚拟地址来寻址页表,从而找到物理地址,\n具体来说就是一个虚拟地址VPN[31:12],页表的起始物理地址是addr,那这个虚拟地址对应的物理地址就是addr + VPN[31:12]所对应的entry的值,就完成了相应的映射\n页表的表项数为2^32 / 4K = 2 ^20 = 1M ,需要20位来寻址,在页表中包括的所有VPN的映射关系,页表的大小是4B * 1M = 4M , 一个entry 需要32位,因为物理内存的数据位宽是32位\n程序对应的页表,连同pc和通用寄存器,组成了程序的状态,在切换程序时,需要去保存状态,该程序称为进程,用户打开程序,操作系统会分配物理内存的空间,创建页表和堆栈等,进程的页表指定了能够在物理内存中访问的地址空间\n可以通过页表通过相同的虚拟地址访问不同的物理地址 ,通过不同的虚拟地址访问相同的物理地址,实现进程的保护和共享\n多级页表 将4MB的线性页表划分为若干个更小的页表,称为子页表.操作系统在处理进程的时候,根据需求逐步放入子页表,并且子页表不再占用连续的物理内存空间.需要一个表格记录子页表在物理内存中存储的位置,称为第一级页表(level1 page Table),子页表称为第二级页表(level2 Page Table)\n一个2^20的entry的页表可以划分为2^10 entry的一级页表+2^10个2^10大小的二级页表,一个页表的表项称为PTE(Page Table Entry),当操作系统创建一个进程,就在物理内存找一块4KB空间存放一级页表,并将基地址放在PTR寄存器中,一个虚拟地址肯定能够对应一个一级页表的表项,用后10位寻址一级页表,获取二级页表,然后再用前10位获取二级页表的表项,\n页表的映射关系应该都是操作系统分的,是在异常那边进行处理的\n不同的虚拟地址会导致出现不同的页表,极端情况为(1)4M程序连续,会建立一个一级页表和一个二级页表=8KB ,(2)都是离散的,并且全部都在4MB的边界上,这样需要建立1024个二级页表+一个一级页表=4100kB\n增加级数,一级页表寻址二级页表,寻址3级页表\u0026hellip;\n2^64entry -\u0026gt; 4096个2^40entry\n​ -\u0026gt; 4096个4096个2^28 entry\n​ -\u0026gt; 4096 4096 4096 2^16 entry \u0026hellip;\n需要多次访问物理内存,\n处理器多个进程时,为进程分配的物理内存之和可能大于实际可用的物理内存,部分页可能临时存在在下一级的硬盘中,成为swap空间 ,需要用到这些页时才会被调入到物理内存\n把页从物理内存写入硬盘称为Page out ,从硬盘写入swap空间称为Page In\n利用虚拟存储器,可以管理每一个页的访问权限,只需要在页表中去设置每一个页的属性就可以了,\nPage Fault Page Fault是异常的一种,通过操作系统来进行完成\n(1)Page Fault需要访问硬盘,通常为毫米级别,与Page Fault对应的异常处理程序来说是微乎其微的\n(2)使用软件可以根据实现情况实现灵活的替换算法,找到最合适的页进行替换\n直接使用虚拟地址不能知道页位于硬盘的哪个位置,只能知道物理内存的,需要操作系统在开辟swap空间的同时,会使用一个表格记录每个页在硬盘中存储的位置,可以和页表进行合并\n如果valid为0,代表页在硬盘中,反之在物理内存里\n但是实际上物理上仍然是分开的,因为不管一个页是不是在物理内存中,操作系统都必须记录一个进程的所有页在硬盘中的位置\n(1)写通(Write Through),将改变的内容马上写回到硬件中去\n(2)写回(Write Back),只有等到地址的内容在物理内存中要被替换时,才将内容写回到硬盘\n在PTE中增加一个dirty的状态位,当页内的某个地址被写入是,dirty的状态会被置1.在需要被替换时,根据dirty位去决定是否要先写回到硬盘中去\n替换算法可以在硬件上提供支持,在PTE中增加一位来记录每个页最近是否被访问过,称为\u0026quot;使用位(use)\u0026quot;,可以周期性的去将使用位清零\n为了处理Page Fault,硬件需要\n(1)在发现Page Fault,能够产生对应类型的异常,并跳转到异常处理程序\n(2)当写入物理内存时,需要将页表中对应PTE的脏状态置1.\n(3)当store/load物理内存时,将use位置1\n小结 没有Page Fault时 处理器送出的VA送到MMU MMU使用PRT和VA[31:12]组成访问页表的地址,送到物理内存 将寻址到的PTE返回给MMU MMU判断valid=1,使用PA={PFN,VA[11:0]}访问物理地址 发生Page Fault 1~3一致\nMMU发现valid=0,触发Page Fault,处理器会跳转到Page Fault对应的异常程序中,此时MMU还会吧发生Page Fault的虚拟地址VA保存到专业的处理器,供异常处理程序使用 如果物理内存没有空闲空间,异常处理程序会根据替换算法,从物理内存找出未来可能不被使用的页,将其替换,页称为Victim Page,如果dirty为1,需要写入到硬盘 Page Fault异常处理程序会使用MMU保存的虚拟地址VA寻址硬盘,找到对应的页,将其写入到Victim page所在的位置 从异常程序返回时,引发Page Fault的指令会被重新取到流水线中,处理器会重新发送虚拟地址到MMU 程序保护 操作系统的内容不允许被用户进程随意修改,操作系统中的有一部分允许用户进程读取.操作系统相对于普通用户进程来说,应该有足够多的权限,来保证操作系统对于系统的控制权;不同进程之间一个加以保护,一个进程不能让其他的进程随便修改自己的内容\n上述条件需要操作系统和用户进程对于不同的页有不同的访问权限,通过页表可以实现,\n操作系统一般不会使用页表,而是直接访问物理内存,物理内存中的专门一部分供操作系统来使用\nARMv7架构,AP部分决定每个页的访问权限\n一旦发现当前的访问不符合规定,会产生非法访问异常,使得处理器跳转到异常处理程序,由操作系统决定如何处理非法的访问\n也可以对一级页表设置权限控制,每个一级页表可以映射4KB*1024=4MB的地址范围,可以\n00 -\u0026gt; 4MB空间不允许访问, 11 -\u0026gt; 对应的4MB空间不设限制 , 01 -\u0026gt; 需要产看第二级页表的PTE,获得页访问的权限,通过粗粒度和细粒度的组合,可以提高处理器的执行效率\n在有dcache的处理器中,在虚拟地址转化为物理地址之后先去访问dcache,需要有一部分空间是不允许缓存的\nPTE包含\n(1)PFN,表示虚拟地址对应的物理地址的页号\n(2)Valid,表示对应页当前是否在物理内存中\n(3)Dirty,表示对应页中内容是否被修改\n(4)Use,表示对应页中的内容是否被修改过\n(5)AP,访问权限控制,表示操作系统和用户程序对当前页的访问权限\n(6)Cacheable,表示对应页是否被缓存\nTLB和Cache TLB TLB(Translation Lookaside Buffer)用来缓存页表中最近使用的PTE,这样就不需要每次都去访问两次物理内存\nTLB只有时间相关性,空间相关性没有明显的规律\n一般TLB使用全相连的方式来设计\n现代处理器采用两级TLB,一级TLB采用哈佛结构,分为I-TLB和D-TLB,采用全相联,二级TLB是指令和数据共用,一般采用组相连\nTLB命中,直接返回从TLB中得到的物理地址,TLB缺失,需要访问物理内存中的页表\n页表中的PTE有效,直接从页表中得到对应的物理地址 页表中的PTE无效,需要从硬盘中去将相应的页搬移到物理内存中 现代处理器中都支持大小可变的页,由操作系统进行管理,根据不同应用的特点选用不同大小的页\nTLB缺失 (1)虚拟地址对应的页不再物理内存中\n(2)虚拟地址对应的页在物理内存中,但是PTE没有放在TLB中\nPage Table Walk:从页表中找到对应的映射关系,并将其写回到TLB中\n软件实现Page Table Walk.当发生TLB缺失,硬件把产生TLB缺失的虚拟地址保存到特殊寄存器中,产生TLB缺失的异常,在异常处理程序中,软件使用虚拟地址去寻址物理内存中的页表,找到对应的PTE,并且写回到TLB中 .为了防止在异常处理程序中又发生TLB缺失,这个程序会放在不需要进行地址转化的物理内存上 ,软件处理会冲说流水线 硬件实现Page Table Walk. 当发生TLB缺失时,自动使用当前的虚拟地址去寻址页表.硬件进行逐级寻址是比较方便的.这种方式比较适合超标量处理器,不需要打断流水线,但是如果操作系统没有在物理内存中建立好了页表,那硬件没有办法,还是得通过操作系统 采用硬件处理TLB缺失需要使用硬件状态机来寻址页表,还需要将整个流水线暂停等待MMU处理缺失,但是在处理完之后就可以直接去执行.采用软件处理,需要执行异常处理程序,而且从异常处理程序退出后,将流水线恢复到TLB缺失发生之前的状态\n发生TLB缺失,如果需要的PTE在页表中,则TLB缺失的处理时间需要十几个周期,如果发生Page Fault异常,则需要成百上千个周期\n对于TLB来说,随机替换算法是比较合适的,可以采用称为时钟算法的方法,就是通过计数器去随机取值,128的表项就可以通过7位的计数器来随机编号\nTLB的写入 在使用TLB作为页表的缓存,处理器送出的虚拟缓存会访问TLB,如果直接从TLB得到物理地址的话,会使得TLB对应的\u0026quot;use\u0026quot;set,如果是store,会使得dirty=1.但是,如果TLB采用写回,那此时不会去更新页表,所以页表的信息可能是过时的,一种方法是在Page Fault的时候,把所有TLB的表项写回到页表\n但实际上没有必要,可以认为被TLB记录的页都是要被使用的,是无法被替换的,操作系统可以记录哪些PTE被放到了TLB中,这样实际上也能够避免当物理内存中一个页被踢出了之后,还需要查找它在TLB中是否被记录了,如果有还需要置0\n操作系统也需要有能够控制dcache的能力,因为操作系统在物理内存中选择一个页进行替换的时候,如果这个页是脏的,它最新的内容不一定是在物理内存中,还有可能在dcache中.虽然说,存在在TLB的页不会被替换,那按理来说,存在在dcache的数据所对应的页也不会被替换.但是也有例外,比方说,发生TLB缺失之后,有TLB表项的会被替换,但是此时D-cache是没有发生变化的,\n对TLB进行控制 TLB是页表的缓存,如果一个页的映射关系在页表中不存在了,那么它在TLB中也不应该存在\n(1)当一个进程结束,进程的指令(code),数据(data)和堆栈(stack)占据的页表置为无效,此时TLB中可能还存在对应的PTE,可以通过ASID去吧I-TLB和D-TLB的内容置为无效\n(2)当一个进程占用的物理内存过大时,操作系统可能将进程中一部分不经常使用的页写回到硬盘中,也需要将TLB置为无效\n对TLB的管理需要包括,1.能够将I-TLB和D-TLB的所有表项置为无效 2. 能够将I-TLB和D-TLB中的某一个ASID对应的所有表项置为无效 3.能够将某个VPN对应的表项置为无效\nARM的TLB管理 (1)用来管理I-TLB的控制寄存器\n将VPN匹配的表项(entry)置为无效的控制寄存器 VPN相等 如果TLB中一个表项的Global位无效,需要ASID相等,如果Global有效,则不需要 进程中某些地址的映射信息被改变时,需要将TLB对应的表项置为无效\n将TLB中ASID匹配的所有表项置为无效的控制寄存器 ​ 当一个进程退出的时候,需要将当前进程在TLB中的所有内容都置为无效\n将TLB中所有未锁定(unlocker)状态的表项置为无效,锁定状态的表项不发生改变.为了加快处理器某些关键程序的执行时间,可以吧一些表项设为锁定状态 (2) 用来管理D-TLB的控制寄存器同理\n(3)用于将TLB的内容进行读出和写入\n使用两个寄存器来对应一个表项,data1和data0,当读取TLB时,被读取表项的内容会放在寄存器中,\n上面的寄存器都位于系统控制协处理器中,只需要通过访问协处理器的指令(MCR和MRC)就可以了\nMIPS风格的TLB管理 MIPS,TLB缺失通过软件来解决,MIPS定义了专门操作TLB的指令,使用这些指令可以直接对TLB进行操作\nMIPS处理器中,为了加快寻址页表的过程,硬件会自动将这两个部分(页表的基地址和偏移地址)放在context寄存器中,位于协处理器CP0.由于load指令无法直接使用CP0中的寄存器,首先要把context寄存器放在通用寄存器$k1中,在MIPS架构中,R26和R27只用在中断和异常中,也称为$k1和$k0\nTLBWR将entryHi和EntryLo寄存器的内容写到TLB内随机指定的一个表项中,在发生TLB缺失,会自动将当前未能转换的虚拟地址的VPN以及当前进程的ASID写入到EntryHi寄存器中\nCache的设计 TLB只是加速了从虚拟地址到物理地址的转换,但是没有加速从物理内存中取数据的过程,也可以使用cache来加速\n下面是物理Cache,和没有加入TLB其实是一样的\n因为要经过TLB,才能访问物理Cache,所以会增加流水线的延时,如果要获得和之前一样的运行频率,需要再加入一级流水线,但是这样增大了分支预测失败的惩罚,也增大了load指令的延迟\n可以直接使用Virtual Cache来缓存数据\n虚拟Cache会导致两个问题\n(1)同义问题(synonyms),也称为重名(aliasing),即多个不同的名字对应相同的物理位置,会出现Cache不同虚拟地址但实际上对应着相同的物理地址,这样就会浪费Cache的空间,而且load了数据了之后需要对其他虚拟地址的Cache都进行更改,因为实际上它们对应着同一个物理地址.\n如果Cache的容量\u0026lt;4KB,寻址的地址不会\u0026gt;12位,那么对应同一个物理地址的不同虚拟地址也会放在Cache的相同地址上,反之就可能出现同义\n要实现同时更新相同物理地址的Cache,就需要使用物理地址作为Cache的Tag部分,\n下图的结构就可以解决同义的问题,\n未完待续\n附录 参考文献 版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-11-12T20:12:09+08:00","permalink":"https://VastCircle.github.io/2024/%E8%99%9A%E6%8B%9F%E5%AD%98%E5%82%A8%E5%99%A8/","title":"虚拟存储器"},{"content":"Dcache结构图 image-20241111163907552 概述 noblockcache 中共例化了3个小模块,writebackUnit,ProbeUnit,MSHRFile\n该Cache支持主缺失和次缺失两种情况。主缺失是指某个缓存行的首次缺失，导致向主存发送回填请求；而次缺失则是指对缓存的访问也发生缺失，但目标缓存行与之前的主缺失是同一个。\n在MSHR（缺失状态保持寄存器）中，通过主缺失标签和重放队列来跟踪缺失。主缺失标签记录处于处理中的回填请求的地址，并在每次缺失时被搜索，以判断是否为次缺失。发生主缺失时，会分配一个新的主缺失标签来记录地址，发出回填请求，必要时执行驱逐（eviction），并在主缺失标签旁的链表中添加一个新的重放队列条目。重放队列条目包含关于访问的信息，包括对应的缓存行偏移量、字节宽度、用于加载的目标寄存器以及待存储的数据。\n次缺失会在对应的重放队列中添加新的条目，但不会向主存发送额外的回填请求。\nThe cache supports both primary misses and secondary misses. A primary miss is the first miss to a cache line and causes a refill request to be sent to main memory, while secondary misses are accesses which miss in the cache but are for the same cache line as an earlier primary miss.\nMisses are tracked in the MSHR using primary miss tags and replay queues. Primary miss tags hold the address of an in-flight refill request and are searched on every miss to determine if it is a secondary miss. A primary miss allocates a new primary miss tag to hold the address, issues a refill request, performs an eviction if needed, and adds a new replay queue entry to a linked list next to the primary miss tag. Replay queue entries contain information about the access including the corresponding cache line offset, the byte width, the destination register for loads, and pending data for stores. A secondary miss adds a new replay queue entry to the appropriate replay queue, but does not send an additional refill request to main memory\n模块介绍 NonBlockingDCacheModule val s1_req = Reg(new HellaCacheReq) // s1主要是打了一拍,下面其实是在决定把什么数据打一拍，第一拍就是在仲裁什么数据地址该去读data和meta // cpu_req : cpu的请求信号 when (io.cpu.req.valid) { s1_req := io.cpu.req.bits } // 写回module的请求信号 when (wb.io.meta_read.valid) { s1_req.addr := Cat(wb.io.meta_read.bits.tag, wb.io.meta_read.bits.idx) \u0026lt;\u0026lt; blockOffBits s1_req.phys := true.B } // 控制module的请求信号 when (prober.io.meta_read.valid) { s1_req.addr := Cat(prober.io.meta_read.bits.tag, prober.io.meta_read.bits.idx) \u0026lt;\u0026lt; blockOffBits s1_req.phys := true.B } // when (mshrs.io.replay.valid) { s1_req := mshrs.io.replay.bits } when (s2_recycle) { s1_req := s2_req } s1_clk_en := metaReadArb.io.out.valid //TODO: should be metaReadArb.io.out.fire, but triggers Verilog backend bug // metaReadArb按理有5个端口,但是生成的时候端口0貌似被优化了 metaReadArb.io.in(0).valid := s2_recycle metaReadArb.io.in(0).bits.idx := s2_req.addr \u0026gt;\u0026gt; blockOffBits metaReadArb.io.in(0).bits.way_en := ~0.U(nWays.W) metaReadArb.io.in(0).bits.tag := s2_req.tag metaReadArb.io.in(1) \u0026lt;\u0026gt; mshrs.io.meta_read metaReadArb.io.in(2) \u0026lt;\u0026gt; prober.io.meta_read metaReadArb.io.in(3) \u0026lt;\u0026gt; wb.io.meta_read metaReadArb.io.in(4).valid := io.cpu.req.valid metaReadArb.io.in(4).bits.idx := io.cpu.req.bits.addr \u0026gt;\u0026gt; blockOffBits metaReadArb.io.in(4).bits.tag := io.cpu.req.bits.addr \u0026gt;\u0026gt; untagBits metaReadArb.io.in(4).bits.way_en := ~0.U(nWays.W) WBU WritebackUnit 模块实现了一个缓存的写回单元，负责通过 TL-C 的 C 通道向 L2 Cache 释放替换块 (Release)。\nrefillCycles 表示在发生缓存未命中（cache miss）时，从主存或其他缓存中读取数据并将其填充到目标缓存的过程所需的时钟周期数\n需要wbu的module只有 prober 和 mshrs , prober和wbu的数据都是通过c通道进行回复的\nwbArb.io.in(0) \u0026lt;\u0026gt; prober.io.wb_req wbArb.io.in(1) \u0026lt;\u0026gt; mshrs.io.wb_req wb.io.req \u0026lt;\u0026gt; wbArb.io.out when (active) { r1_data_req_fired := false.B r2_data_req_fired := r1_data_req_fired // 返回数据并且返回tag的时候 when (io.data_req.fire \u0026amp;\u0026amp; io.meta_read.fire) { // 拉高r1_data_req_fired r1_data_req_fired := true.B // 计数器+1 data_req_cnt := data_req_cnt + 1.U } when (r2_data_req_fired) { // 在返回后的第二拍,拉高release.valid io.release.valid := true.B when(!io.release.ready) { r1_data_req_fired := false.B r2_data_req_fired := false.B // 一般都是减去2,除非refilllCycles \u0026lt;= 1 data_req_cnt := data_req_cnt - Mux[UInt]((refillCycles \u0026gt; 1).B \u0026amp;\u0026amp; r1_data_req_fired, 2.U, 1.U) } when(!r1_data_req_fired) { // We\u0026#39;re done if this is the final data request and the Release can be sent active := data_req_cnt \u0026lt; refillCycles.U || !io.release.ready } } } // 发起请求的时候进行active 和 赋值 when (io.req.fire) { active := true.B data_req_cnt := 0.U req := io.req.bits } io.data_req.valid := fire io.data_req.bits.way_en := req.way_en io.data_req.bits.addr := (if(refillCycles \u0026gt; 1) Cat(req.idx, data_req_cnt(log2Up(refillCycles)-1,0)) else req.idx) \u0026lt;\u0026lt; rowOffBits Probe 实现了Cache管理模块的状态机，主要用于处理探测请求的操作，管理Cache的状态，判断命中情况，以及处理数据写回操作,主要针对的是tag,不是data\nprober的请求输入的数据不是cpu_req,而是auto_out_b_ , 代表这是下一级存储，最直接的就是sbus对其发送的req ，比方说外设发起的读数据，或者在多核心的情况下，core0去读cache0,为了缓存一致性的问题，可能需要sbus去读cache1，然后再去写cache0等\n发起auto_out_b_*的是l2 cache\n// 9个状态 val (s_invalid :: s_meta_read :: s_meta_resp :: s_mshr_req :: s_mshr_resp :: s_release :: s_writeback_req :: s_writeback_resp :: s_meta_write :: Nil) = Enum(9) when (io.meta_read.fire) { state := s_meta_resp } // we need to wait one cycle for the metadata to be read from the array when (state === s_meta_resp) { state := s_mshr_req } when (state === s_mshr_req) { old_coh := io.block_state way_en := io.way_en // 比较有意思,如果没有rdy,不是选择wait,而是retry // if the read didn\u0026#39;t go through, we need to retry state := Mux(io.mshr_rdy, s_mshr_resp, s_meta_read) } 逻辑是比较简单的,就是当发起请求时,去meta_read,meta_read之后去读mshr,读出mshr之后,当tag_match的时候且为dirty,需要写回,所以切换到s_writeback_req ,否则到release,release之后去meta_write.写回是由wb模块控制的\nL1MetadataArray mata meta就是存储tag的地方,是一个同步ram\n// () =\u0026gt; T是函数的写法 ,无输入参数,输出参数是T类型的 class L1MetadataArray[T \u0026lt;: L1Metadata](onReset: () =\u0026gt; T)(implicit p: Parameters) extends L1HellaCacheModule()(p) { val rstVal = onReset() val io = IO(new Bundle { val read = Flipped(Decoupled(new L1MetaReadReq)) val write = Flipped(Decoupled(new L1MetaWriteReq)) val resp = Output(Vec(nWays, rstVal.cloneType)) }) val rst_cnt = RegInit(0.U(log2Up(nSets+1).W)) val rst = rst_cnt \u0026lt; nSets.U val waddr = Mux(rst, rst_cnt, io.write.bits.idx) val wdata = Mux(rst, rstVal, io.write.bits.data).asUInt // way_en是独热码的形式 val wmask = Mux(rst || (nWays == 1).B, (-1).S, io.write.bits.way_en.asSInt).asBools val rmask = Mux(rst || (nWays == 1).B, (-1).S, io.read.bits.way_en.asSInt).asBools when (rst) { rst_cnt := rst_cnt+1.U } val metabits = rstVal.getWidth // way 是路数 , sets是组数 , way_en 就是用来选择路的 // 同步mem val tag_array = SyncReadMem(nSets, Vec(nWays, UInt(metabits.W))) val wen = rst || io.write.valid when (wen) { tag_array.write(waddr, VecInit.fill(nWays)(wdata), wmask) } // read data io.resp := tag_array.read(io.read.bits.idx, io.read.fire()).map(_.asTypeOf(chiselTypeOf(rstVal))) io.read.ready := !wen // so really this could be a 6T RAM io.write.ready := !rst } metaReadArb or metaWriteArb metaReadArb仲裁输入为 mshrs , prober , wb ,mata 和 data都支持多路读取\nMSHRFile 缓存控制中的关键部分，用于处理多组MSHR（Miss Status Holding Register），协调多种请求（读、写、填充、写回等）在一级缓存（L1 Cache）中的操作。\n顶层输入的每一个req会发送到每一个mshr中，但是最终只有一个mshr会被选中 ，也就代表只有一个mshr会被写入本笔数据\nmshr.io.req_sec_val := io.req.valid \u0026amp;\u0026amp; sdq_rdy \u0026amp;\u0026amp; tag_match mshr.io.req_bits.viewAsSupertype(new HellaCacheReqInternal) := io.req.bits.viewAsSupertype(new HellaCacheReqInternal) mshr.io.req_bits.tag_match := io.req.bits.tag_match mshr.io.req_bits.old_meta := io.req.bits.old_meta mshr.io.req_bits.way_en := io.req.bits.way_en mshr.io.req_bits.sdq_id := sdq_alloc_id cacheable val cacheable = edge.manager.supportsAcquireBFast(io.req.bits.addr, lgCacheBlockBytes.U) cacheable用于检测当前请求的地址是否可以被缓存。edge.manager.supportsAcquireBFast方法检查缓存控制器是否支持快速获取操作。\nsdq 用于管理store data queue\n// 位向量寄存器，用于跟踪SDQ中的空闲条目。每一位代表一个位置，如果为1则表示该位置已被分配，若为0则表示空闲 val sdq_val = RegInit(0.U(cfg.nSDQ.W)) // 通过优先编码获取其中一个空闲的sdq的id,从左往右第一个0 // 比方说 priorityEncoder(0b0100) = 2 ,priorityEncoder(0b0111) = 2 ,独热转数值 val sdq_alloc_id = PriorityEncoder(~sdq_val(cfg.nSDQ-1,0)) // 如果不是全部为1代表有空闲,设为ready val sdq_rdy = !sdq_val.andR // 表示当前请求是否满足写入SDQ的条件 ,需要请求有效,请求能够被接受,可缓存,并且是写命令 val sdq_enq = io.req.valid \u0026amp;\u0026amp; io.req.ready \u0026amp;\u0026amp; cacheable \u0026amp;\u0026amp; isWrite(io.req.bits.cmd) // store data queue ,存放数据 val sdq = Mem(cfg.nSDQ, UInt(coreDataBits.W)) // 当满足条件,就写入相应数据 when (sdq_enq) { sdq(sdq_alloc_id) := io.req.bits.data } // 用于指示是否可以释放存储数据队列中的一个条目,当io.replay.fire（即重放请求有效且被接受）且当前重放指令是写操作（isWrite(io.replay.bits.cmd)）时，free_sdq为true，表示可以释放该条目。 val free_sdq = io.replay.fire \u0026amp;\u0026amp; isWrite(io.replay.bits.cmd) io.replay.bits.data := sdq(RegEnable(replay_arb.io.out.bits.sdq_id, free_sdq)) io.replay.bits.mask := 0.U io.replay \u0026lt;\u0026gt; replay_arb.io.out // sdq_enq代表要去分配sdq了 io_replay.valid 代表sdq使用完了 // uIntToOH(3) = 0b0100 // priotityEncoderOH把最靠近左边的1 set , 比方说 sdq_val = 0b0011, 则 priorityEncoderOH(~sdq_val(cfg.SDQ-1,0)) = 0b0010 when (io.replay.valid || sdq_enq) { sdq_val := sdq_val \u0026amp; ~(UIntToOH(replay_arb.io.out.bits.sdq_id) \u0026amp; Fill(cfg.nSDQ, free_sdq)) // reset | PriorityEncoderOH(~sdq_val(cfg.nSDQ-1,0)) \u0026amp; Fill(cfg.nSDQ, sdq_enq) // set } val replay_arb = Module(new Arbiter(new ReplayInternal, cfg.nMSHRs)) iomshr 输入是s2_req ， 回应resp，这里的resp是直接接入到MSHRfile顶层端口的，它和mshr是同一类东西，应该就是mmio,就是不经过cache的数据,但是本身mshr保存的是缓存未命中的请求以及相关状态，层次有点问题吧\nmmio_alloc_arb.io.out.ready := io.req.valid \u0026amp;\u0026amp; !cacheable val io = IO(new Bundle { val req = Flipped(Decoupled(new HellaCacheReq)) // MSHRfile发起的req val resp = Decoupled(new HellaCacheResp) // resp val mem_access = Decoupled(new TLBundleA(edge.bundle)) // 向mem发起的请求 val mem_ack = Flipped(Valid(new TLBundleD(edge.bundle))) // mem ack val replay_next = Output(Bool()) }) // 输入握手的时候，获取数据 req stage 3 when (io.req.fire) { req := io.req.bits state := s_mem_access } // 获取内存数据 when (io.mem_access.fire) { state := s_mem_ack } // 内存数据响应 when (state === s_mem_ack \u0026amp;\u0026amp; io.mem_ack.valid) { state := s_resp when (isRead(req.cmd)) { // 获取响应的数据 grant_word := wordFromBeat(req.addr, io.mem_ack.bits.data) } } // 发出去了 when (io.resp.fire) { state := s_idle } 其实这个仲裁器像是倒着用的，是把io.req信号去选择一个mshr去输出\nmshr 输入是s2_req\n每个MSHR处理一个缓存块的缺失请求\nmshr应该就是其中的一个表项,这里通过状态机来判断表项是一次缺失还是二次缺失 ,是不是太奢侈了\n该模块的主要功能是管理和协调多种缓存操作，\n处理缓存未命中时的请求。 向主存发出请求并等待响应。 维护状态机以跟踪请求的进度。 根据不同情况执行不同的控制流。 val idxMatch = Wire(Vec(cfg.nMSHRs, Bool())) val tagList = Wire(Vec(cfg.nMSHRs, Bits(tagBits.W))) // idxMatch 只有一个会拉高 val tag_match = Mux1H(idxMatch, tagList) === io.req.bits.addr \u0026gt;\u0026gt; untagBits 状态转移及相应输出 // state 是一个寄存器，用于跟踪 MSHR 的状态。状态机的主要状态包括： // s_invalid：空闲状态，表示 MSHR 未被占用。 // s_wb_req 和 s_wb_resp：处理写回操作的状态。 // s_meta_clear：清理缓存元数据。 // s_refill_req 和 s_refill_resp：处理从主存中获取数据的请求。 // s_meta_write_req 和 s_meta_write_resp：更新缓存元数据。 // s_drain_rpq：处理重放队列的请求 when (state === s_drain_rpq \u0026amp;\u0026amp; !rpq.io.deq.valid) { state := s_invalid } when (state === s_meta_write_resp) { // this wait state allows us to catch RAW hazards on the tags via nack_victim state := s_drain_rpq } when (state === s_meta_write_req \u0026amp;\u0026amp; io.meta_write.ready) { state := s_meta_write_resp } when (state === s_refill_resp \u0026amp;\u0026amp; refill_done) { new_coh := coh_on_grant state := s_meta_write_req } when (io.mem_acquire.fire) { // s_refill_req state := s_refill_resp } when (state === s_meta_clear \u0026amp;\u0026amp; io.meta_write.ready) { state := s_refill_req } when (state === s_wb_resp \u0026amp;\u0026amp; io.wb_req.ready \u0026amp;\u0026amp; acked) { state := s_meta_clear } when (io.wb_req.fire) { // s_wb_req state := s_wb_resp } when (io.req_sec_val \u0026amp;\u0026amp; io.req_sec_rdy) { // s_wb_req, s_wb_resp, s_refill_req //If we get a secondary miss that needs more permissions before we\u0026#39;ve sent // out the primary miss\u0026#39;s Acquire, we can upgrade the permissions we\u0026#39;re // going to ask for in s_refill_req req.cmd := dirtier_cmd when (is_hit_again) { new_coh := dirtier_coh } } // 首次miss的时候,赋值req, 这说明req是被保存在mshr上的,只有首次缺失才会改变 when (io.req_pri_val \u0026amp;\u0026amp; io.req_pri_rdy) { req := io.req_bits acked := false.B val old_coh = io.req_bits.old_meta.coh val needs_wb = old_coh.onCacheControl(M_FLUSH)._1 val (is_hit, _, coh_on_hit) = old_coh.onAccess(io.req_bits.cmd) // 如果命中： //更新一致性状态：将new_coh设为coh_on_hit。 //设置状态为MetaWrite：准备写回元数据。 //如果未命中但标记匹配： //保持一致性状态不变。 //状态转为Refill：准备请求新的数据。 when (io.req_bits.tag_match) { when (is_hit) { // set dirty bit new_coh := coh_on_hit state := s_meta_write_req }.otherwise { // upgrade permissions new_coh := old_coh state := s_refill_req } }.otherwise { // writback if necessary and refill 标记不匹配：判断是否需要写回旧数据，并准备加载新数据。 new_coh := ClientMetadata.onReset state := Mux(needs_wb, s_wb_req, s_meta_clear) } } // state === s_meta_write_req or s_mata_clear io.meta_write.valid := state.isOneOf(s_meta_write_req, s_meta_clear) io.meta_write.bits.idx := req_idx io.meta_write.bits.tag := io.tag io.meta_write.bits.data.coh := Mux(state === s_meta_clear, coh_on_clear, new_coh) io.meta_write.bits.data.tag := io.tag io.meta_write.bits.way_en := req.way_en // state === s_wb_req io.wb_req.valid := state === s_wb_req io.wb_req.bits.source := id.U io.wb_req.bits.tag := req.old_meta.tag io.wb_req.bits.idx := req_idx io.wb_req.bits.param := shrink_param io.wb_req.bits.way_en := req.way_en io.wb_req.bits.voluntary := true.B // state === s_refill_req io.mem_acquire.valid := state === s_refill_req \u0026amp;\u0026amp; grantackq.io.enq.ready io.mem_acquire.bits := edge.AcquireBlock( fromSource = id.U, toAddress = Cat(io.tag, req_idx) \u0026lt;\u0026lt; blockOffBits, lgSize = lgCacheBlockBytes.U, growPermissions = grow_param)._2 // state === s_drain_rpq io.meta_read.valid := state === s_drain_rpq io.meta_read.bits.idx := req_idx io.meta_read.bits.tag := io.tag io.meta_read.bits.way_en := ~(0.U(nWays.W)) // 重放接口 io.replay.valid := state === s_drain_rpq \u0026amp;\u0026amp; rpq.io.deq.valid io.replay.bits := rpq.io.deq.bits io.replay.bits.phys := true.B io.replay.bits.addr := Cat(io.tag, req_idx, rpq.io.deq.bits.addr(blockOffBits-1,0)) rpq 重放队列\n用于暂存未能成功处理的请求，比如因缺少权限、数据尚未准备好等原因导致的请求失败。 这些请求将在条件满足时重新尝试（重放）。 val rpq = Module(new NBDcacheQueue(new ReplayInternal, cfg.nRPQ)) // 无论是一次缺失还是二次缺失，数据都会存放在rpq里 rpq.io.enq.valid := (io.req_pri_val \u0026amp;\u0026amp; io.req_pri_rdy || io.req_sec_val \u0026amp;\u0026amp; sec_rdy) \u0026amp;\u0026amp; !isPrefetch(io.req_bits.cmd) rpq.io.enq.bits := io.req_bits rpq.io.deq.ready := (io.replay.ready \u0026amp;\u0026amp; state === s_drain_rpq) || state === s_invalid addr的划分 val req = Reg(new MSHRReqInternal) val req_idx = req.addr(untagBits-1,blockOffBits) val req_tag = req.addr \u0026gt;\u0026gt; untagBits val req_block_addr = (req.addr \u0026gt;\u0026gt; blockOffBits) \u0026lt;\u0026lt; blockOffBits val idx_match = Mux(io.runahead_flag \u0026amp;\u0026amp; req_block_addr =/= (io.req_bits.addr \u0026gt;\u0026gt; blockOffBits) \u0026lt;\u0026lt; blockOffBits, false.B,req_idx === io.req_bits.addr(untagBits-1,blockOffBits)) io.tag := req_tag io.idx_match := (state =/= s_invalid) \u0026amp;\u0026amp; idx_match pri_val和pri_rdy 首次缺失 仲裁器的逻辑是多个valid同时拉高,取优先级最高的,然后把相应的ready拉高\nmshr.io.req_pri_val := alloc_arb.io.in(i).ready io.req_pri_rdy := state === s_invalid alloc_arb.io.in(i).valid := mshr.io.req_pri_rdy alloc_arb.io.out.ready := io.req.valid \u0026amp;\u0026amp; sdq_rdy \u0026amp;\u0026amp; cacheable \u0026amp;\u0026amp; !idx_match 内存的输出与回复（ack和acquire) 通过TLAbiter 去选择优先级最低的一个进行输出\nTLArbiter.lowestFromSeq(edge, io.mem_acquire, mshrs.map(_.io.mem_acquire) ++ mmios.map(_.io.mem_access)) TLArbiter.lowestFromSeq(edge, io.mem_finish, mshrs.map(_.io.mem_finish)) // 可以看到，主要是通过id来识别回复的数据该回复给谁 mshr.io.mem_ack.bits := io.mem_grant.bits mshr.io.mem_ack.valid := io.mem_grant.valid \u0026amp;\u0026amp; io.mem_grant.bits.source === id.U 数据流向 读 从cpu发起数据起\n可以看到，cpu发起请求时，req和data是差了一拍的，后续会有一个s1来同步data\n然后，cpu_req的信号会进入meta_read和data_read的仲裁器，就是要读tag和data ,index 为 addr [9:6],同时，vaddr会被转换为paddr,然后，再下一拍s2_req_addr被赋值\n第一拍仲裁并输入data,mata,第二拍返回数据（s1),第三拍送给mshr(s2),第四拍写回mata和data(s3)，\n// s2在第1级流水线存在有效指令 (s1_valid)，且该指令未被取消 (~io_cpu_s1_kill)，同时该指令是一个 SFENCE 操作 (s1_sfence) 时，并且没有出现任何异常的时候，就会被赋值为1 s2_valid = s2_valid_REG \u0026amp; {_io_cpu_s2_xcpt_ma_ld_output, _io_cpu_s2_xcpt_ma_st_output, _io_cpu_s2_xcpt_pf_ld_output, _io_cpu_s2_xcpt_pf_st_output, _io_cpu_s2_xcpt_gf_ld_output, _io_cpu_s2_xcpt_gf_st_output, _io_cpu_s2_xcpt_ae_ld_output, _io_cpu_s2_xcpt_ae_st_output} == 8\u0026#39;h0; s1_clk_en \u0026lt;= _metaReadArb_io_out_valid if (s1_clk_en) begin s2_req_addr \u0026lt;= {8\u0026#39;h0,_dtlb_io_resp_paddr} end //当以下条件同时满足时，该表达式为真： //s2_valid_masked：当前请求在第2阶段有效且没有被屏蔽。 //!s2_hit：该请求在缓存中未命中。 所以说发起mshr的请求首先需要没有命中 val s2_hit = s2_tag_match \u0026amp;\u0026amp; s2_has_permission \u0026amp;\u0026amp; s2_hit_state === s2_new_hit_state mshr_valid = s2_valid_masked \u0026amp;\u0026amp; !s2_hit \u0026amp;\u0026amp; (isPrefetch(s2_req.cmd) || isRead(s2_req.cmd) || isWrite(s2_req.cmd)) // 这个过程发生在stage 2,即读出来时就会进行检查，发起读是在stage 1 def wayMap[T \u0026lt;: Data](f: Int =\u0026gt; T) = VecInit((0 until nWays).map(f)) val s1_tag_eq_way = wayMap((w: Int) =\u0026gt; meta.io.resp(w).tag === (s1_addr \u0026gt;\u0026gt; untagBits)).asUInt val s1_tag_match_way = wayMap((w: Int) =\u0026gt; s1_tag_eq_way(w) \u0026amp;\u0026amp; meta.io.resp(w).coh.isValid()).asUInt 附录 参考文献 rocket-chip学习基础篇\n香山手册\nTileLink介绍\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-11-11T16:33:27+08:00","permalink":"https://VastCircle.github.io/2024/nbdcache/","title":"NBDcache"},{"content":"Rocket chip Fronted frontend TilePRCIDomain_3.sv ICache_3 icache frontend.sv ShiftQueue fq frontend.sv Fronted frontend TilePRCIDomain_3.sv Rocket core TocketTile.sv IBuf ibuf Rocket.sv ex image-20241108211542867 mem image-20241108211632083 image-20241108211601921 wb dcache Rocket core TocketTile.sv HellaCacheArbiter_3 dcacheArb RocketTile.sv image-20241109004426330 NonBlockingDcache dcache RocketTile.sv HellaCacheArbiter_3 dcacheArb RocketTile.sv image-20241109005444805 Rocket core TocketTile.sv Boom BoomFronted frontend TilePRCIDomain.sv ICache icache BoomFrontend.sv Queue_66 f3 BoomFrontend.sv Queue_69 f4 BoomFrontend.sv 8条指令,io_enq_bits_insts_ 是单纯的截位,io_enq_bits_exp_insts_ 是做了rvc的判断\nimage-20241109103010659 image-20241109103033295 FetchBuffer fb BoomFrontend.sv BoomFrontend frontend BoomTile.sv io_cpu_fetchpacket_bits_uops_0_\nio_cpu_fetchpacket_bits_uops_1_\nio_cpu_fetchpacket_bits_uops_2_\nio_cpu_fetchpacket_bits_uops_3_\nimage-20241109101138570 BoomCore core BoomTile.sv DecodeUnit decode_units_0 有四个0-3\nRenameStage rename_stage 还有一个 RenameStage_1 fp_rename_stage,路径也是类似的\n0-3 四个\nBasicDispatcher dispatcher 分发有 0_0-3 to mem_issue_unit\n1_0-3 to int_issue_unit\n2_0-3 to fp_pipline\nIssueUnitCollapsing_2 int_issue_unit RegisterRead_1 iregister_read mem_issue int_issue都一样\n_iregister_read_io_exe_reqs_5_ alu_exe_unit_1\n_iregister_read_io_exe_reqs_4_ alu_exe_unit\n_iregister_read_io_exe_reqs_3_ csr_exe_unit\n_iregister_read_io_exe_reqs_2_ jmp_unit\n_iregister_read_io_exe_reqs_1_ mem_units_1\n_iregister_read_io_exe_reqs_0_ mem_units_0\nimage-20241109130000628 BoomCore core BoomTile.sv image-20241109130128445 LSU lsu 然后就是lsu到dcache,dcache又返回相应的数据到lsu,lsu再返回到core\n如果不是访存肯定就不走lsu那条路径\n下面就是unit完成执行之后给rob valid信号,rob应该会相应restore指令,然后给rename_stage commit的一些信息\nimg ","date":"2024-11-07T20:15:39+08:00","permalink":"https://VastCircle.github.io/2024/cpu%E8%B7%AF%E5%BE%84%E6%9F%A5%E6%89%BE/","title":"CPU路径查找"},{"content":"introduction (i) 一种适用于顺序执行核心的高性能、低开销的硬件预取技术，称为标量前推执行（𝑆𝑅𝐸）。𝑆𝑅𝐸在寄存器传输级有效预取复杂的内存访问模式，并实现了硬件优化策略，以尽量减少能量和面积的开销（如图1所示）。\n(ii) 为解决前推执行和缓存替换对有效预取的掩盖问题，我们引入了一种新的工作负载分析方法。该方法通过探索独特的工作负载特性，增强了前推技术在隐藏内存延迟方面的能力，从而缩短总执行时间。\n(iii) 利用自定义指令，我们提出了三种不同的模式，以解决前推技术的安全漏洞，并进一步提升性能。\nbackground A.标量核心中的长延迟内存访问\n顺序执行核心在面积和功耗方面相比乱序执行核心具有显著优势，使其本质上更适合需要长时间运行的场景。然而，在访问稀疏数据结构时，乱序核心可以通过诸如ROB（重排序缓冲区）、保留站、加载缓冲区和存储缓冲区等组件缓解由末级缓存未命中引起的内存访问延迟。相比之下，顺序执行核心缺乏应对内存延迟的有效策略，甚至L1缓存未命中也会显著影响性能。在严重情况下，执行时间的大部分都花费在等待内存上，导致顺序核心的平均每指令周期（CPI）可能达到数十。因此，解决内存延迟问题对于提升顺序执行核心的性能至关重要。\nB. runahead technique 然而，由于高开销，它们并不适合在顺序处理器上实现。以原始的runahead execute为例，当一条长延迟内存访问指令位于指令窗口的头部，导致指令窗口已满并阻塞流水线时，处理器会对架构寄存器文件和分支历史寄存器的状态进行检查点操作，促使处理器进入runahead mode。阻塞指令窗口的内存访问指令的目标寄存器将被标记为INV（无效），并在后续指令流中传播，以防止错误的内存请求。当内存请求返回时，处理器退出前推模式并恢复相关的架构状态。\noverview 目前处理器中的推测预取技术能够有效管理复杂的间接内存访问模式，但在微架构设计中带来了显著的硬件开销，使其不适用于小型核心。在周期精确的仿真器中实现这些技术会导致设计侵入性，并需开发专门的拦截电路。𝑆𝑅𝐸在寄存器传输级开发，通过一系列优化策略来减轻开销，从而确保在低成本的情况下实现高性能。在runahead execute进入条件中集成了一种间接内存访问检测机制，提升了预取的准确性和覆盖范围。\n在RCU中，为拦截正常执行模式下预取数据的回写，MSHR追踪每个缓存未命中请求的详细信息，包括回写位置、请求地址和顺序。在寄存器文件中构建了拦截电路（图2a）。在寄存器文件中还建立了一个CP提取和回写电路，并包含多端口的CP（图2b），通过与级联控制电路集成，利用多个周期提取和回写处理器的状态信息。\n对于在runahead阶段的内存未命中请求以及瞬态执行(transient execution)期间识别的无效内存未命中请求，会检测随后使用缺失数据的寄存器编号，并在scoreboard上重置相应位置（图2c）。开发了一个无效检测和传播机制，用于追踪负责流水线释放的寄存器编号和内存未命中地址，从而防止错误的预取请求。此外，构建了一个紧凑的dual-way cache，用于在runahead 阶段收集存储指令的存储值，确保内存指令的正确执行（图2d）。\n我们还开发了一个自定义ISA接口以增强灵活性，包括（图2e）：(i) Safe Mode，解决在前推推测执行期间因分支预测错误导致的机密数据泄漏风险；(ii) Miss Counter Table,，一个双入口表，用于自定义前推过程的终止点，以优化不同硬件和工作负载的性能；(iii) Aggressive Mode，提供禁用前推期间的FPU选项，防止在浮点运算密集型工作负载中长指令阻塞，并允许发出更多预取请求。\n微架构 选择了开源的Rocket Chip SoC作为𝑆𝑅𝐸微架构的基础。Rocket Chip包含一个低功耗的Rocket核心，支持开源的RV64GC RISC-V指令集，并使用Chisel硬件描述语言编写。它具备支持基于页面的虚拟内存的内存管理单元（MMU）、一个非阻塞数据缓存，以及带有分支预测功能的前端。\nA. The Runahead Control Unit 进入 pseudo-enter 阶段的条件通过处理来自L2缓存MSHR的未命中请求信息（回写位置）来确定。对于non-blocking cache的stall-on-use机制，在解码阶段检测数据使用的时机，使处理器能够在该周期进入pseudo-enter阶段（图3a）。在pseudo-enter阶段，处理器检查是否存在间接内存访问。如果检测到这种访问，处理器将转入runahead execution phase，通常持续十个周期（图3b）。在runahead execution phase，为了便于后续释放流水线并有效管理数据回写寄存器，必须跟踪来自L1缓存MSHR的Load-Miss和Gain-Miss信息，其中包括回写寄存器编号、请求地址和读/写指针等详细信息（图3c）。同时，为防止Load-Miss和Gain-Miss阻塞流水线，通过识别未命中回写寄存器编号来释放流水线并使对应的寄存器和地址无效（图3d）。在进入伪退出阶段时，目标是拦截与Gain-miss相关的数据回写。通过基于L1缓存MSHR中的请求回写寄存器编号、地址和读/写指针，精确拦截相同或不同块的回写请求来实现。此外，同一块内的Gain-miss会触发MSHR重放机制，可能会中断前推过程。拦截电路的扩展设计通过检测并拦截重复请求来解决此问题，以防止此类中断（图3f）。\nB. The Runahead Control FSM 为了实现对预取架构的精确控制，我们将有限状态机（FSM）机制与前推执行模式紧密集成，以增强处理器在处理长延迟内存访问时的效率。FSM通过在各状态间切换动态管理预取操作，确保处理器在内存延迟期间仍保持高效。FSM从伪进入状态开始，在该状态下，它处理来自L2缓存的未命中状态保持寄存器（MSHR）的未命中请求信息（回写位置）。此时，由于数据缓存的“stall on use”机制，处理器不仅不会完全停顿，还会继续执行指令。在这里，处理器检查流水线指令，识别出间接内存访问后，进入前推进入状态。此状态下处理器保存当前状态和寄存器，为后续从前推模式无缝恢复正常操作做好准备，以维护系统完整性（图3a）。完成前推进入状态的相关处理后，处理器直接进入前推执行状态。在前推执行状态下，处理器继续执行指令而不将结果提交至寄存器文件，通过在Load-Miss解决前预取数据来有效减少空闲时间。\n为实现此目的，FSM从L1缓存的MSHR中跟踪Load-Miss和Gain-Miss的详细信息，包括回写寄存器编号、请求地址和读/写指针。流水线被释放，对应的寄存器和地址被无效化，以防止阻塞。一旦Load-Miss数据返回，FSM便切换至前推通过状态，该状态作为中介，决定处理器应进入伪退出状态还是直接进入正常退出状态。FSM在两种情况下会进入伪退出状态：(i) 在数据回写前达到效益点，通过比较请求地址和读/写指针；或(ii) Gain-Miss计数器在数据回写后达到指定值，表明已达到效益点。在伪退出状态，FSM通过准确检测相同和不同块的重复请求来拦截与Gain-Miss相关的回写请求。此拦截机制防止了因相同块中的Gain-Miss触发重放机制而中断前推过程。FSM随后完成操作，确保所有前推执行的指令已完成或安全丢弃。\n如果前推过程中没有未解决的依赖关系，FSM将进入前推退出状态，允许处理器恢复正常处理。然而，若前推执行期间存在无法解决的依赖关系或执行异常，FSM可能会暂时切换至前推无效状态，并通过控制流水线停顿、等待Load-Miss正常返回后再退出前推模式。\nC. 多周期检查点和释放电路\n处理器状态的checkpoint and restore，包括 GHR（全局历史寄存器）、RAS（返回地址堆栈）和架构寄存器文件，对于确保超前运行中的正确操作至关重要。模式和普通模式。 GHR 和 RAS 处理分支历史记录和返回地址跟踪。当处理器进入超前运行模式时，这些结构在单个周期内设置检查点，保留分支预测和返回地址计算所需的信息。退出超前运行模式后，先前保存的分支历史记录和返回地址将被恢复，从而保持准确的控制流，而不会增加显着的性能开销。相比之下，存储处理器架构状态的架构寄存器文件涉及更多数据和复杂性。为了管理这一点，使用了多周期检查点和释放机制，这减少了对扩展模块接口的需求并降低了处理器之间的通信压力。尽管对架构寄存器文件进行检查点需要多个周期，但它与在超前运行模式和正常模式之间转换时清除和重新填充管道所需的五个周期过程相一致，从而避免了任何额外的性能损失。\nD. 预取管理单元 我们设计了预取管理单元（PMU）以检测和拦截错误的预取请求，从而使推测执行能够有效处理内存访问指令。PMU由两个主要结构组成：无效集合单元（ISU），用于拦截错误的预取地址，以及前推缓存（RC），用于在前推过程中存储存储指令的值。\nISU（无效集合单元）：Invfile用于存储无效寄存器编号和地址信息，类似于记分板。每个寄存器编号或前推缓存条目都有一个指示其有效性的位（图4 a）。Load-miss和Gain-miss的写回寄存器编号以及在前推过程中存储的无效地址，通常是Invfile的来源，相关机制检测到时，Invfile被激活。我们将来自执行阶段的RS与内存请求地址进行比较，并与Invfile中的相应位进行比对，产生三种情况（图4 b）：\n当源寄存器编号存在于Invfile中时，启动无效传播机制，设置相应的目标寄存器编号。 如果加载指令的地址有效或所有源寄存器有效，则触发无效重置机制，重置相应寄存器编号的无效位。 如果发现存储地址有效，则激活无效重置机制，重置相应地址位。 基于这些操作的结果，无效寄存器信号转变为内存访问阻塞信号和在写回阶段对处理器的流水线释放信号。无效地址信号被转发到前推缓存模块，以确定加载块是否有效命中前推缓存。\nRC（前推缓存）：前推缓存被设计为紧凑的二路关联存储结构，每个条目包含标签和数据信息，每个数据条目的大小为两个字（具体为16B）（图4 c）。在前推过程中，加载地址同时访问该缓存和L1缓存。它根据内存访问地址的索引信息选择行，匹配适当的集合，然后根据偏移信息选择字节，最后根据方式命中检索匹配的数据。命中机制涉及将内存访问地址的标签信息与前推缓存的标签信息进行比较。如果匹配，则进一步验证数据的有效性。如果有效，则生成命中信号，并用作数据选择的控制信号（图4 d）。在退出前推时，前推缓存中的所有值都被置为无效，以防止访问过时的值，直到新的前推过程重置存储的地址。对于数据替换机制，我们采用伪LRU替换策略选择最不常用的方式进行替换。\n附录 参考文献 版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-31T20:13:06+08:00","permalink":"https://VastCircle.github.io/2024/scalar_runahead_execution/","title":"Scalar_runahead_execution"},{"content":"处理器运行 总共的过程应该是在exit之后,处理器(core0)会循环执行下面的代码,主要是要向0x80001ec0写入数据1,会写入到dcache那边\n而serdesser会去l2读取0x80001ec0的数据,中间通过fbus,sbus到l2,如果读取到数据为1的话,就会相应的去发送相关exit信号\n对于fbus_serdesser那边什么时候发送的a_valid ,粗略的看了一下是它和SerialRAM里的serdesser有相互依赖的关系,最后估计会追溯到c代码那边去,从波形图看的话,发送是较为规律的\n路径 TilePRCIDomain tile_prici_domain DigitalTop.sv SystemBus subsystem_sbus DigitalTop.sv CoherenceManagerWrapper subsystem_l2_wrapper DigitalTop.sv 已经从l2读取数据\nSystemBus subsystem_sbus DigitalTop.sv TLBuffer_2 subsystem_fbus_buffer Digitaltop.sv TLInterconnectCoupler_16 subsystem_fbus_coupler_from_port_named_serial_tl_ctrl Digitaltop.sv TLSerdesser subsystem_fbus_serdesser Digitaltop.sv serdesser应该在定时发送请求\nimage-20241112002039647 AsyncQueue subsystem_fbus_out_async Digitaltop.sv image-20241112001843957 DigitalTop system ChipTop.sv image-20241112001700816 ChipTop chiptop0 TestHarness.sv SerialRAM ram TestHarness.sv image-20241111235459860 image-20241111234654487 SimTSI success_exit_sim TestHarness.sv input clock, input reset, input tsi_out_valid, output tsi_out_ready, input [31:0] tsi_out_bits, output tsi_in_valid, input tsi_in_ready, output [31:0] tsi_in_bits, output [31:0] exit ","date":"2024-10-31T13:11:58+08:00","permalink":"https://VastCircle.github.io/2024/%E5%A4%84%E7%90%86%E5%99%A8%E6%88%90%E5%8A%9F%E8%BF%90%E8%A1%8C%E7%9A%84%E6%A0%87%E5%BF%97/","title":"处理器成功运行的标志"},{"content":"路径查找 Bootrom ClockSinkDomain_1.sv ClockSinkDomain_1 bootROMDomainWrapper DigitalTop.sv PeripheryBus_1 subsystem_cbus DigitalTop.sv TLInterconnectCoupler_33 coupler_to_bootrom TLInterconnectCoupler_33.sv TLXbar_5 out_xbar PeripheryBus_1.sv TLFIFOFixer_2 fixer PeripheryBus_1.sv TLBuffer_4 buffer PeripheryBus_1.sv TLAtomicAutomata_1 atomics PeripheryBus_1.sv TLXbar_4 in_xbar PeripheryBus_1.sv image-20241104201454888 PeripyheryBus_1 subsystem_cbus DigitalTop.sv image-20241104200308864 SystemBus subsystem_sbus DigitalTop.sv image-20241104200104409 TLInterconnectCoupler aoupler_to_bus_named_subsystem_cbus SystemBus.sv TLXbar system_bus_xbar SystemBus.sv TLFIFOFixer fixer SystemBus.sv SystemBus subsystem_sbus DigitalTop.sv TilePRCIDomain tile_prci_domain DigitalTop.sv TLBuffer_15 buffer TilePRCIDomain.sv ToomTile tile_reset_domain_boom_tile TilePRCIDomain.sv TLXbar_8 tlMasterXbar BoomTile.sv image-20241104215304702 后面可以不看了\nCoherenceManagerWrapper subsystem_l2_wrapper DigitalTop.sv InclusiveCache l2 CoherenceManagerWrapper.sv TLCacheCork cork CoherenceManagerWrapper.sv image-20241031113807118 BankBinder binder CoherenceManagerWrapper.sv CoherenceManagerWrapper subsystem_l2_wrapper DigitalTop.sv memorybus subsystem_mbus DigitalTop.sv DigitalTop system chiptop.sv Dram 略\n","date":"2024-10-30T16:25:09+08:00","permalink":"https://VastCircle.github.io/2024/big_soc_%E8%B7%AF%E5%BE%84%E6%9F%A5%E6%89%BE/","title":"Big_soc_路径查找"},{"content":"Chipyard SoC 中三个最高层次是ChipTop(DUT)、TestHarness和TestDriver。ChipTop和TestHarness均由 Chisel 生成器发出。TestDriver用作我们的测试平台，是 Rocket Chip 中的 Verilog 文件。\nChipTop(DUT) ChipTop 是顶层模块，负责实例化 System 子模块，通常是 DigitalTop 的具体实例。设计的大部分内容位于 System 中。ChipTop 层中存在的其他组件通常是 IO 单元、时钟接收器和多路复用器、重置同步器以及其他需要存在于 System 之外的模拟 IP。IOBinders 负责实例化与 System 的 IO 相对应的 ChipTop IO 的 IO 单元。HarnessBinders 负责实例化测试夹具，以连接到 ChipTop 端口。大多数类型的设备和测试夹具都可以使用自定义的 IOBinders 和 HarnessBinders 进行实例化。\nDigitalTop system (\t// @[ChipTop.scala:28:35] .clock (_system_auto_implicitClockGrouper_out_clock),\t// @[ChipTop.scala:28:35] .reset (_system_auto_implicitClockGrouper_out_reset),\t// @[ChipTop.scala:28:35] .auto_prci_ctrl_domain_reset_setter_clock_in_member_allClocks_uncore_clock (clock_uncore_clock), .auto_prci_ctrl_domain_reset_setter_clock_in_member_allClocks_uncore_reset (reset_io), .resetctrl_hartIsInReset_0 (_system_auto_subsystem_cbus_fixedClockNode_out_reset),\t// @[ChipTop.scala:28:35] .resetctrl_hartIsInReset_1 (_system_auto_subsystem_cbus_fixedClockNode_out_reset),\t// @[ChipTop.scala:28:35] --- ); 自定义 ChipTop 默认的标准 ChipTop 提供了一个最小的、基本的模板，以便 IOBinders 在 DigitalTop 特性周围生成 IO 单元。对于 tapeout、集成模拟 IP 或其他非标准用例，Chipyard 支持使用 BuildTop 键指定自定义 ChipTop。一个使用非标准 IO 单元的自定义 ChipTop 示例位于 generators/chipyard/src/main/scala/example/CustomChipTop.scala。\n您还可以指定一个完全自定义的 ChipTop，该 ChipTop 不使用任何 RocketChip 或 Chipyard SoC 组件。示例位于 generators/chipyard/src/main/scala/example/EmptyChipTop.scala。可以使用以下命令构建 EmptyChipTop 示例：make CONFIG=EmptyChipTopConfig TOP=EmptyChipTop。\nSystem/DigitalTop Rocket Chip SoC 的系统模块是通过 cake-pattern 组合而成的。具体而言，DigitalTop 扩展了 System，System 扩展了 Subsystem，Subsystem 又扩展了 BaseSubsystem。\nBaseSubsystem BaseSubsystem 在 generators/rocketchip/src/main/scala/subsystem/BaseSubsystem.scala 中定义。查看 BaseSubsystem 抽象类，我们看到该类实例化了顶层总线（frontbus、systembus、peripherybus 等），但没有指定拓扑结构。该类还定义了多个 ElaborationArtefacts，这些文件是在 Chisel 细化后生成的（例如，设备树字符串和外交图可视化 GraphML 文件）。\nSubsystem 在 generators/chipyard/src/main/scala/Subsystem.scala 中，我们可以看到 Chipyard 的 Subsystem 是如何扩展 BaseSubsystem 抽象类的。Subsystem 混入了 HasBoomAndRocketTiles 特性，该特性根据指定的参数定义并实例化 BOOM 或 Rocket 瓦片。我们在这里为每个瓦片连接一些基本的 IO，特别是 hartids 和复位向量。\nSystem generators/chipyard/src/main/scala/System.scala 完成了 System 的定义。\nHasHierarchicalBusTopology 在 Rocket Chip 中定义，指定顶层总线之间的连接。 HasAsyncExtInterrupts 和 HasExtInterruptsModuleImp 添加外部中断的 IO，并将其适当地连接到瓦片。 CanHave\u0026hellip;AXI4Port 添加各种主从 AXI4 端口，添加 TL-to-AXI4 转换器，并将其连接到适当的总线。 HasPeripheryBootROM 添加 BootROM 设备。 Tops SoC Top 继承 System 类，并包含自定义组件的特性。在 Chipyard 中，这包括添加 NIC、UART 和 GPIO，以及为引导方法设置硬件。\nTestHarness TestHarness 与 Top 之间的连接是在添加到 Top 的特性中定义的方法中执行的。当这些方法从 TestHarness 中调用时，它们可以在scope of the harness内实例化模块，然后将其连接到 DUT。例如，从 CanHaveMasterAXI4MemPortModuleImp 特性定义的 connectSimAXIMem 方法，在 TestHarness 中调用时，会实例化 SimAXIMems，并将其连接到顶层的正确 IO。\n尽管这种间接方式连接顶层 IO 可能看起来不必要地复杂，但它允许设计师组合自定义特性，而无需担心任何特定特性的实现细节。\nTestDriver TestDriver 在 generators/rocketchip/src/main/resources/vsrc/TestDriver.v 中定义。该 Verilog 文件通过实例化 TestHarness、驱动时钟和复位信号以及解释成功输出来执行仿真。该文件与为 TestHarness 和 Top 生成的 Verilog 一起编译，以生成仿真器。\n`MODEL testHarness( .clock(clock), .reset(reset), .io_success(success) 总结 TestDriver就是完全的仿真文件\nTestHarness 中包含simdram等仿真组件\nChipTop 是顶层模块，负责实例化 System 子模块，通常是 DigitalTop 的具体实例\n整个system就是一个soc,包含core和外设\n附录 参考文献 Tops,Test-Harnesses,and the Test-Driver\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-29T21:56:38+08:00","permalink":"https://VastCircle.github.io/2024/chipyard%E7%9A%84%E4%B8%89%E4%B8%AAhighest_level/","title":"Chipyard的三个highest_level"},{"content":"rocket-chip 框图 rocket-chip流水线 rocket-chip Icache rocket-chip Dcache rocket-chip 代码结构 ## rocket-chip generator的一级目录结构 bootrom : 在BootROM的bootloader第一阶段所使用的代码 csrc Verilator: 仿真用的C代码 emulator Verilator :用来编译和跑仿真的工作目录 project Scala: 构建工具sbt用来构建Scala的工作目录 regression: 定义的持续的整合和一套nightly regression scripts: 用来分析仿真的输出或者处理代码文件的内容 vsim VCS: 用来编译和跑仿真的工作目录 vsrc Verilog: 代码，包含接口、测试框架和Verilog过程接口VPI chisel3 :包含Chisel自定义的各种类和规则，用来生成RTL firrtl: 存放Chisel编译器处理代码而生成的一种中间表示，由中间表示能生成Verilog代码或C++代码 hardfloat: 用chisel写成的浮点单元 riscv-tools: 支持RISC-V的一套软件，与生成RTL有关 torture: 用来生成压力测试所需的一些随机指令 ## src/main/scala: 构筑rocket-chip的代码 amba: 协议的实现代码，包括AXI4，AHB-lite，APB config: 提供能配置Generator的Scala的接口 coreplex: 包含Rocket核、系统总线、coherence agents、debug设备、中断处理、面向外部的外设、时钟同步处理和TileLink到外设总线转换 devices: 一些外设，包括debug模块和各种挂在TileLink的从设备 diplomacy: 用来扩展Chisel，通过允许对硬件进行两个阶段的阐述，可以让参数在模块之间协调传递 groundtest: 生成可综合的硬件测试平台，通过发出随机的访问存储器指令流，进行对核外的存储器系统进行压力测试 jtag: 用来生成JTAG总线接口 regmapper: 用来生成带有能访问内存映射寄存器的标准接口的从设备 rocket: 用来生成顺序核Rocket、L1指令cache和L1数据cache tile: 包含可以与Rocket核组成tile的组件，如FPU和RoCC协处理器 tilelink: 用来生成TileLink总线（协议），包含一些适配器和转其他总线（协议）的转换器 system:Rocket Chip的顶层代码包，同时也是用作测试的硬件平台的顶层代码包 unittest: 用作生成硬件测试平台来测试单独的一个个模块 util: 提供一些能被其他代码包调用的通用的Scala和Chisel结构 /src/main/scala/system TestHarness.scala TestHarness 模块通过模拟配置、接口连接和调试信号设置，构建了一个测试环境，用于验证 ExampleRocketSystem的功能\nTestGeneration.scala 这段代码定义了RISC-V处理器的测试框架，它主要由一些抽象和具体的测试套件类组成，用于生成用于RISC-V测试的Makefile脚本片段。以下是代码的主要结构与功能：\nRocketTestSuite 抽象类： 这个抽象类定义了一个通用的测试套件结构，包含测试目录、目标名称、测试用例集合等关键参数。postScript属性生成用于链接目标文件的Makefile命令模板。\nAssemblyTestSuite 类：\n这是RocketTestSuite的子类，用于定义汇编语言的测试套件。每个实例表示特定环境（如rv32ui）下的测试集合，并生成包含测试文件的Makefile片段。 BenchmarkTestSuite 类：\n用于定义基准测试套件，支持特定目录中的多个基准测试，例如性能评估用的程序。 RegressionTestSuite 类：\n包含一组用于回归测试的测试文件，通过简单定义makeTargetName来统一生成Makefile片段。 TestGeneration 对象：\n该对象定义了添加测试套件并生成Makefile片段的逻辑。通过gen方法根据测试类型和环境生成目标名称和Perl脚本，用于捕获和处理测试结果。 DefaultTestSuites 对象：\n包含多个默认测试套件的实例，这些测试套件涵盖了不同类型的RISC-V指令集扩展，例如rv32ui、rv64ui、rv32ua、rv64ua等。还包括了一些性能基准和单一回归测试的示例。\nConfigs.scala TestHarness.scala 就是testbench，而 ExampleRocketSystem.scala 就是SOC的层次，包括Core以外的其他外设\u0026amp;总线，Configs.scala 就是核心Core的配置。\nclass BaseConfig extends Config( new WithDefaultMemPort ++ new WithDefaultMMIOPort ++ new WithDefaultSlavePort ++ new WithTimebase(BigInt(1000000)) ++ // 1 MHz new WithDTS(\u0026#34;freechips,rocketchip-unknown\u0026#34;, Nil) ++ new WithNExtTopInterrupts(2) ++ new BaseSubsystemConfig ) ExampleRocketSystem.scala 对于SOC层面的设计\n/** Example Top with periphery devices and ports, and a Rocket subsystem */ class ExampleRocketSystem(implicit p: Parameters) extends RocketSubsystem with HasAsyncExtInterrupts // 提供异步外部中断接口，允许系统处理来自外部设备的异步中断信号 with CanHaveMasterAXI4MemPort // 添加 AXI4 主接口，用于连接存储器 with CanHaveMasterAXI4MMIOPort // 添加 AXI4 主接口，用于连接内存映射的 I/O 端口 with CanHaveSlaveAXI4Port // 添加 AXI4 从接口，支持与其他主设备的交互 { // optionally add ROM devices // 可选地添加 ROM 设备 // Note that setting BootROMLocated will override the reset_vector for all tiles // 设置 BootROMLocated 将覆盖所有核心的重置向量 val bootROM = p(BootROMLocated(location)).map { BootROM.attach(_, this, CBUS) } // 连接 BootROM 到 CBUS，以配置系统启动入口 val maskROMs = p(MaskROMLocated(location)).map { MaskROM.attach(_, this, CBUS) } // 将 MaskROMs 连接到 CBUS，作为只读存储器使用 override lazy val module = new ExampleRocketSystemModuleImp(this) // 延迟加载模块实现 } simAXIMem.scala Memory with AXI port for use in elaboratable test harnesses(一个mem的仿真模型)\n/src/main/scala/rocket 此 RTL 包生成 Rocket 顺序流水线核心以及 L1 指令和数据缓存。此库旨在供芯片生成器使用，该生成器在内存系统中实例化核心并将其连接到外部世界。\n附录 参考文献 rocket-chip目录\nrocketchip学习笔记\nhttps://www.cnblogs.com/gujiangtaoFuture/articles/11766114.html\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-29T17:12:22+08:00","permalink":"https://VastCircle.github.io/2024/rocket-chip%E5%AD%A6%E4%B9%A0/","title":"Rocket Chip学习"},{"content":"差异文件 ### 新加入的 rocket/RCU.scala rocket/rh_cache.scala rocket/RH_Cache.scala rocket/rh_data.scala rocket/rh_tag.scala rocket/Runahead_cache.scala ### 修改过的 rocket/BTB.scala rocket/Frontend.scala rocket/HellaCacheArbiter.scala rocket/HellaCache.scala rocket/NBDcache.scala rocket/RocketCore.scala subsystem/Configs.scala subsystem/SystemBus.scala tile/Core.scala tilelink/Bundles.scala tilelink/Edges.scala 附录 参考文献 版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-27T21:23:02+08:00","permalink":"https://VastCircle.github.io/2024/%E5%BE%AE%E6%9E%B6%E6%9E%84%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/","title":"rocket-src微架构代码解读"},{"content":"附录 参考文献 版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-26T15:23:09+08:00","permalink":"https://VastCircle.github.io/2024/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E7%AC%94%E8%AE%B09/","title":"提交"},{"content":"概述 无\n重排序缓存 一般结构 ROB是一个FIFO\n(1)Complete:表示一条指令是否执行完毕\n(2)Areg:指令在原始程序中指定的目的寄存器,逻辑寄存器\n(3)Preg:指令的Areg经过寄存器重命名之后,对应的物理寄存器编号\n(4)OPreg:指令的Areg被重命名为新的Preg之前,对应的旧的Preg,当指令发生异常(exception)进行恢复,会用到\n(5)PC:指令对应的PC值,当发生中断或异常之后,需要保存指令的PC值\n(6)Exception:指令发生异常的异常类型\n(7)Type:指令的类型会被记录到这里,当指令retire时,不同类型的指令会有不同的动作,例如store指令写D-cache(那执行在干嘛)\n在流水线的分发阶段,指令会按照进入流水线的顺序写入ROB,ROB中对应的complete会被置0,执行完成之后会被置1.指令的计算结果可以放在ROB中,也可以放在物理寄存器堆(PRF)中.\n异常的处理统一放在提交阶段\n指令表项的编号会一直随着指令在流水线中流动\ni1是除法指令,i2使用i1的结果,所以i1,i2执行时间很长\n端口需求 对于一个4-way的超标量处理器来说,在ROB中每周期可以退休的指令不少于4条,ROB选择那些Complete的指令进行退休,但是由于是顺序的,如果连续4条中出现一条not ready ,后续的就无法retire\n(1)4个读端口:retire时检测指令是否complete\n(2)8个读端口 : 在流水线的寄存器重命名阶段,需要从ROB读取4条指令的源操作数 ????\n(3)4个写端口:分发阶段需要向ROB写入4条指令 ????\n(4)最少4个写端口:在write back 阶段,需要写入最小4条指令的结果 (最少是由于很多处理器的issue width \u0026gt; machine width)\n管理处理器的状态 Architecture State , 通用寄存器的值,PC的值,存储器的值 Speculative State,超标量处理器内部的状态,例如重命名使用的物理寄存器,重排序缓存(ROB),发射队列(Issue Queue)和store buffer 等,这些状态超前于指令集定义的状态 对于采用将通用寄存器扩展进行寄存器重命名的架构,需要将目的寄存器的值从物理寄存器搬移到通用寄存器中\n对于采用通用的物理寄存器进行重命名的架构,需要将目的寄存器在物理寄存器堆中标记为外界可见的状态???\n如果退休的指令是store, 需要把store buffer对应的值写到D-cache去(难道不是写完了再退休的吗)\n如果退休的指令是分支指令,需要进行状态恢复,并且冲刷错误指令,从正确地址取地址 ,\n在提交阶段还需要对异常进行统一处理\n两种方法\n(1)使用ROB管理指令集定义的状态\n(2)使用物理寄存器管理指令集定义的状态\n使用ROB管理指令集定义的状态 (Retire Register File) 当指令退休的时候,指令的结果可以对指令集定义的状态进行更新,此时会将指令的结果从ROB中搬移到指令集中定义的逻辑寄存器中.逻辑寄存器存储了所有退休指令对应的目的寄存器的值\n一般情况下,使用ROB管理指令集定义的状态,都对应着使用数据捕捉的结构来进行发射(issue),因为指令的内容会存在在ROB和通用寄存器当中,通过数据捕捉可以在执行阶段把数据送到payload RAM ,可以直接从payload RAM去获取所有数据\n对于非捕获队列,没有payload RAM,相应的数据是直接从ROB或者通用寄存器获取,所有需要发射队列支持额外的写端口(通知操作数的位置变动),和额外的旁路网络来\n使用物理寄存器管理指令集定义的状态 当物理寄存器的结果被计算出来之后,指令的状态变成了complete,当它退休的时候,直接把相应的状态标记为 Architecture state ,直到另一条指令写入同样的目标寄存器并且退休了,就相当于直接把指令集定义的逻辑寄存器融入到物理寄存器中\n(1)当指令从ROB中退休之后,不需要把指令的结果进行搬移,便于实现低功耗\n(2)在基于ROB进行状态管理时,需要ROB开辟空间存放指令的结果,但例如store,比较指令,分支指令是没有目的寄存器的,ROB会有一部分空间浪费掉了,但是这种方法只会对于存在目的寄存器的指令分配空间\n(3)ROB是集中管理方式,指令需要从其中读取操作数,同时指令也需要把结果写入其中,需要大量的读写端口,但是使用物理寄存器可以采用cluster结构等方式来避免多端口的负面影响\n但是这样会造成寄存器重命名比较复杂 ,使用ROB管理时,只需要写入ROB就完成了重命名,但是使用物理寄存器管理,需要额外的表格存放哪些物理寄存器是空闲的,并且重映射关系的建立和释放都比较困难,并且需要一个额外的表格来存放那些物理寄存器是Architecture state (这不是加一个标志位就可以了)\n特殊情况的处理 分支预测错误,或者异常\nstore指令只有在retire阶段才能够真正改变处理器的状态(写D-cache),如果发射了D-cache缺失,会阻碍流水线中所以后面指令继续退休\n分支预测失败 以流水线的寄存器重命名为分界\n前端的状态回复(front-end recovery):将流水线中重命名阶段之前的所有指令都抹掉,将分支预测器中的历史状态标进行恢复,并使用正确的地址取指令\n后端的状态恢复(back-end recovery time):把处理器中所有内部组件(Issue Queue,Store Buffer 和 ROB)错误的指令都抹掉,恢复重命名映射表(RAT),以便那些错误指令对RAT的修改进行改正,同时被错误的指令占据的物理寄存器和ROB的空间需要被释放\n满足后端恢复的时间小于前端恢复的时间+取指和寄存器重命名的时间,不需要暂停流水线\n基于ROB重命名的架构(基于扩展的ARF进行寄存器重命名同理) 当寄存器位于ROB时,在RAT中存储在ROB的位置,位于ARF,直接进行寻址 , RAT的地址是逻辑寄存器的值,数据是存储类型及对应具体位置\n一条退休的指令将目的寄存器从ROB搬移到ARF中后,并不一定表示以后指令需要从ARF读取寄存器的值\nA :ADD R1,R2,R3 B :ADD R1,R1,R4 C :ADD R1,R1,R5 只有指令C的映射关系会写入到RAT中, 即使指令A从流水线中退休了,后续的指令也只使用指令C的结果(why???).为了实现能够在指令实现搬移之后从ARF读取寄存器的值,在ROB中的每条指令都会检查自身是否是最新的映射关系,只有当一条指令从ROB中退休的时候,发现自身也是最新的映射关系,才能够将RAT中对应的内容改为ARF状态\n从ROB中退休的指令检查自身是不是最新的映射关系:在指令退休的时候,使用目的寄存器读取RAT,读出逻辑寄存器此时对应的ROB pointer,如果发现它和当前退休指令在ROB中占据的地址是一样的,表面这条退休的指令是最新的映射关系\n在流水线中发现分支预测失败时(一般是在执行阶段),此时流水线中有一部分指令是在分支指令之前进人到流水线的，它们可以被继续执行，因此当发现分支指令预测失败时，并不马上进行状态修复，而是停止取新的指令，让流水线继续执行，这个过程称为将流水线抽干(drain out),直到分支指令之前的所有指令(包括分支指令本身)都退休。此时 ARF 中所有寄存器的内容都是正确的，同时在流水线中的所有指令都是处于错误的路径上，可以将流水线中的指令全部抹掉，然后将 RAT 中所有的内容都标记为 ARF 状态，这样处理器就从分支预测失败的状态恢复过来了，此时可以从正确的地址开始取指.\n优点:重命名易于实现,状态恢复容易\n基于统一的PRF进行重命名的架构 两个RAT (前端RAT(Speculative RAT)和后端RAT(Architecture RAT)),可以使用后端RAT对处理器进行状态恢复\n和前面类似,当发现分支指令预测失败时，并不马上进行状态修复，而是停止取新的指令，让流水线继续执行，这个过程称为将流水线抽干(drain out),直到分支指令之前的所有指令(包括分支指令本身)都退休,之后可以将流水线中的指令全部抹掉，然后将后端RAT 中所有的内容都复制到前端RAT，这样处理器就从分支预测失败的状态恢复过来了，此时可以从正确的地址开始取指.这种方法就是Recovery at Retire\n还是会遇到问题,就是分支指令之前存在D-cache缺失的指令,会等待时间过程导致分支预测失败时的惩罚(mis-prediction penalty)过大\n为了解决上述问题,可以使用checkpoint,即在每条分支指令改变处理器状态之前,把处理器的状态保存起来,然后通过分支指令编号选择性去抹除流水线错误路径的指令,然后使用checkpoint去恢复RAT,基于SRAM的RAT需要保存整个表格,基于CAM的RAT只需要保存映射表中的状态位\n还可以去选择性的分配checkpoint的资源,对于分支预测错误率比较高的才分配checkpoint,但是如果分支预测失败,还是需要采用Recovery at Retire恢复,也可以使用ROB进行恢复,因为ROB中还是保存着旧的映射关系,即记录着每条指令对于重命名映射表的修改\n异常的处理 使用ROB去顺序的执行所有的异常\n在指令即将要退休的时候,如果发生了异常就不能退休,而是要去转而处理异常\n精确异常:处理器能够知道哪条指令发生了异常,并且这条发生异常的指令之后所有的指令都不允许改变处理器的状态,这样在处理完异常之后,可以精确的进行返回,返回地方有两种,(1)返回到发生异常指令本身,重新执行指令(TLB缺失),(2)不重新执行指令,而是返回到它的下一条指令(系统调用) ,精确异常需要抹去产生异常的指令后面的所有指令,并回复处理器修改的状态\n可以采用前面所说的Recovery ai Retire来恢复异常\n还有一种方法就是WALK,通过ROB保存的旧数据来恢复\n在使用统一的PRF进行寄存器重命名的方式中,和RAT相关的还有两个表格,一个存储那些物理寄存器是空闲的,Free Register ,一个存储物理寄存器的值是否被计算出来,Busy Table\n对于Free Register Pool ,因为刚刚读取的内容不会消失,不会被退休的指令覆盖,所以只需要恢复读指针,可以利用ROB的旧映射关系来进行恢复(如果是顺序读取Free Register Pool的话其实感觉只需要回退读指针就可以了)\n对于Busy Table,由于指令运算完成之后,就可以在写回阶段写入对应的物理寄存器,所以在发生异常时,Busy Table已经进行了多次修改.还是可以通过ROB,在从ROB读取指令时,每读取一条指令,就将指令的目的寄存器在Busy Table对应的内容置为无效 ,这样后续的指令也不会使用到错误的值了\n对于统一的PRF进行重命名的架构,使用WALK的方法是合适的,因为涉及到对Free Register Pool和Busy Table的恢复\n中断的处理 中断是处理器外部发生的 ,是异步的\n(1)马上处理,当中断发生时,就将流水线中的指令全部抹掉,按照异常处理的方式进行恢复,并将流水线中最旧的指令PC值(还有其他状态寄存器)保存起来,然后跳转到对应的中断处理程序,返回时,使用保存的PC值重新取指令 .这种方式实时性最强,但是相当于之前的那些指令需要重新执行\n(2)延迟处理 .当中断发生时，流水线停止取指令，但是要等到流水线中所有的指令都退休(retire)之后才对这个中断进行处理，这样能够保证流水线中这些已有的指令不被“浪费”,而且当流水线中所有的指令都退休之后，此时流水线的状态肯定是正确的，也就不需要进行状态恢复了。\n(1)如果在流水线中的这些指令发生了 D-Cache 缺失，那么需要很长的时间才能够解决，这样导致了过长的中断响应时间。 (2)如果在流水线中发现了一条预测失败的分支指令，那么首先需要对这个情况进行处理，将处理器的状态进行恢复，这需要消耗一定的时间，也造成了中断响应时间的增大。\n(3)如果流水线中的这些指令中发生了异常(exception),那么是先对异常进行处理，还是先对中断进行处理？这需要仔细地进行权衡，但是一般来说，应该是先对中断进行处理， 因为很多类型的异常处理需要耗费很长的时间，如 D-Cache 缺失、TLB 缺失或者 Page Fault等，这样会导致中断的响应时间讨长而无法忍受。\nstore指令的处理 store指令通常在retire之前都是不写入D-cache的,它会写入store buffer,这样load指令就会从store buffer 或者D-cache去获取数据.这种方法最安全,但是一旦store指令D-cache缺失,需要等待很长的时间,会造成ROB的堵塞\n可以在store buffer中增加一个状态位,标记store指令是否具备退休的条件,这样store在缓存中有3个状态\nun-complete(未执行完毕),当store指令在分发阶段占据一个store buffer的时候标记为un-complete\ncomplete(已经执行完毕),当store指令已经得到地址和数据,但是没有变成最旧的指令,标记为complete\n(retire)离开流水线,当store指令成为最旧的指令并退休是,在store buffer标记该状态,这样store指令可以离开ROB,就不会阻碍后面的指令离开流水线,而硬件会自动将store buffer中处于retire状态的store指令写到D-cache中,并且此时store buffer中的retire的内容也会成为Architecture state的一部分\nstore buffer中的指令只有在完成写D-cache的任务之后才会释放空间,这样会造成分发之前的流水线发生阻塞,可以把已经退休的store指令存储在一个叫做write back buffer的地方,硬件会自动将write back buffer的store指令写到D-cache中\n这样write back buffer也会成为Architecture state的一部分,load指令需要在store buffer和write back buffer中查找.一旦write back buffer没有空间了,就不能再将store指令退休\nstore 指令也是顺序进入write back buffer的,但是在进入的同时需要查找有没有写入相同地址的store指令,有的话需要把前面的store指令置为无效 ,保证load能够查找罪行的数据\n对于软件处理TLB缺失的处理器,在store指令需要退休时,如果ROB中记录了TLB缺失的异常,那么store指令不能够进入write back buffer,而是需要异常的处理,需要将流水线清空,进行处理器的状态恢复.然后跳转到对应的异常处理程序中去,处理完之后重新执行store,这样可以保证所有进入write back buffer中的store指令不会产生TLB缺失\n指令离开流水线的限制 在4-way的超标量处理器中,如果ROB中最旧的四条指令都处于complete状态,理论上四条指令都能够退休\n但是 (1)每周期有四条store指令退休,意味着D-Cache或者Write Back Buffer需要支持四个写端口\n(2)每周期有四条分支指令退休,意味着没周期需要将四条分支指令的信息写回分支预测器,这需要分支预测器中是偶有部件需要支持四个写端口,同时需要能够将Checkpoint资源在每周期释放四个\n(3)如果在处理器中对 store/load 指令之间的相关性实现了预测，即预测一条 load 指令是否会和它之前的 store 指令存在相关性，在这种情况下，如果每周期有四条 load 指令退休，意味着每周期需要将四条 load 指令的信息写回到相关的预测器中，这也导致了四个写端口的需求。\n但是上述情况出现的概率很小,所以增加硬件结构不如对指令进行限制,比方说限制每次退休的分支指令只能有1条之类的\nbr_mask = 1110 (如果假设分支指令是1,那对分支指令进行异或应该能够得到1110这一串数字), st_mask = 1111 , ld_mask = 1111 ,那么代表第四条指令是第二条分支指令,再把3个结果相与,得到1110,代表只有前三条指令有资格退休\n同理,对于异常指令,也只能退休一条,所以在检测到异常指令之后,需要对后续的指令进行屏蔽\n附录 参考文献 版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-26T15:06:32+08:00","permalink":"https://VastCircle.github.io/2024/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E7%AC%94%E8%AE%B05/","title":"提交"},{"content":"概述 旁路网络:负责将FU的运算结果送到需要的地方\n每个FU都和一个1-of-M的仲裁电路一一对应,被选择的指令去读取物理寄存器堆(或者payload RAM),从而得到对应的操作数,每个仲裁电路和物理寄存器也数一一对应的\nFU的类型 ALU AGU(address generate unit) 用于计算访问存储器类型的指令在指令中携带的地址\nBRU(Branch Unit) 负责处理程序控制流(control flow)类型的指令\n负责将分支指令携带的目标地址计算出来,并且根据一定的情况来决定是否使用地址,同时对进行分支预测正确与否的判断\n对于ARM和PowerPC在每一条指令都加入的条件码,不局限于分支指令,相当于吧程序中的控制相关性使用数据相关性代替了,这样可以降低分支指令使用的频率,但是条件执行会占据指令编码的位数,减少指令中分配给通用寄存器的部分,并且可能会出现很多条无效的指令,反而可能会降低效率\n而且,如果跳转指令的条件不成立,比方说下面的ADD指令,可能就会使用错误的数据,可以通过暂停流水线或者预测的方式来解决\nselect-uOP指令 对于Intel,通过硬件插入额外的指令来选择正确的结果\n通过加入uOP来选择结果,但是这要求条件执行的指令必须成对出现,但是这样对编译器存在一定的制约,可以采取对于每一条条件指令都加入select-uOP的方法来解决,就相当于在执行完成条件指令之后,又对执行条件指令之后的值和执行条件指令之前的值进行了一次选择,将选择的值用于后续的寄存器重命名\n对于ARM指令,有一个条件寄存器CPSR,对于每一条条件指令例如ADDEQ,都需要先去判断条件寄存器的值,再决定执行不执行\nBRU还负责对于分支指令结果的检查,通过分支缓存和BRU单元的结果进行对比来实现,因为分支缓存中是保存了所有预测跳转的指令\n旁路网络 从FU的输出端到输入端架起一个通路,可以将FU的结果送到所有FU的输入端,物理寄存器堆,payload RAM,即旁路网络\n在更为现实的处理器中,在Regfile read 之后还会加入一个流水段称为 source Drive,因为对于一条指令从源操作数从物理寄存器读出来之后,还需要经过很长的一段布线,才能达到输入端,而且FU的输入端有大量的多路选择器,用来从不同的旁路网路或者物理寄存器堆选择合适的操作数,同理 FU的输出段也需要经过复杂的网路到达输入端,也需要一级,即Result drive\n简单的旁路网络 一个FU中也会有多个计算单元,一个周期只能送一条指令进FU,如果计算单元需要的周期数(latency)相等,那无所谓,但是如果不能,就可能出现在不同周期被送进来,但是在相同周期计算出结果,都想通过旁路网络进行传送\n一种解决方法就是对于一条指令正常进行唤醒和仲裁,在FU中被执行前,首先检查当前FU是否可以被自己使用(通过周期数,上周期接收了一条latency=2的指令,当周期就不要接收latency=1的指令),如果不行,放回发射队列,重新进行仲裁.\n但是这样会造成一些本来可以仲裁成功的指令被耽搁了,因此可以直接让latency作为某个值的指令就不参与仲裁的过程\n在设计发射队列时,也需要考虑是否当前的FU是能够被使用的\n对每个仲裁电路设计一个位宽为2位的控制寄存器,高位用来拦截所有latency=2的指令,低位拦截latency=1的指令,并且在发射队列的表项中增加两个信号,指示latency=1 or 2\n每个周期都需要对两位寄存器进行赋值,也要进行移位,比方说 latency = 3,就将两位寄存器赋值为10,latency = 2 ,就将两位寄存器赋值为01\n如果第一个周期选中了latency = 3的指令 , 第二个周期选中了latency = 3的指令 ,cycle 1 , a = 10 , cycle 2 , a = 11 = (10 \u0026raquo; 1 )| 10\n复杂设计的旁路网络 (1)指令 B 只能在流水线的 Execute 阶段，从指令 A 的 Result drive 阶段获得操作数。\n(2)指令 C 可以在流水线的 Source drive 阶段，从指令 A 的 Result drive 阶段获得操作数；或者指令 C 也可以在流水线的 Execute 阶段，从指令 A 的 Write back 阶段获得操作数。 (3)指令 D可以在流水线的 Source drive 阶段，从指令 A 的 Write back 阶段获得操作数。 (4)指令 E 在流水线的 RF Read 阶段读取物理寄存器堆(PRF)时，就可以得到指令 A 的结果了，因此它不需要从旁路网络中获得操作数，这里假设物理寄存器堆可以在前半个周期写人，后半个周期读取。\n对于每一个指令,不一定是在执行阶段得到旁路网络的结果\nexecute阶段的操作数除了来自于上一级流水线,还可以来自于两个FU计算的结果 ,来自于流水线的 Result Drive (B相对于A)\nSource Drive 阶段,操作数除了来自于上一级流水线,还可以来自于以前流水线的结果,分布在Result Drive(C相对A)和Write Back(D相对于A)\nA和E是不需要旁路网络的 ,即某一条指令处在RF Read 里,另一条指令处在Write back 里,就不需要进行旁路\n(1)当两条指令处在相邻周期,旁路路径只能发生在Execute 和 Result Drive\n(2)当两条指令相差一个周期,旁路路径能够发生在Source Drive和Result Drive, Execute 和Write Back 之间\n(3)当两条指令相差两个周期,旁路路径只能发生在Source Drive 和 Write Back 之间\n提供数据的指令一定得在Execute后,接受数据的指令一定要在RF read后\n操作数的选择 ScoreBoard\nFU#:记录物理寄存器从哪个FU中被计算出来,当一条指令被仲裁电路选中的时候,如果指令存在目的寄存器,就将这条指令在哪个FU中执行的信息写到表格中\nR:表示物理寄存器的值已经从FU中计算出来了,并且已经被写到物理寄存器堆中了(在写回的时候更新)\n指令B通过读取scoreBoard可以得知需要从FU中取数据\n指令C可以得知可以从PRF中取数据\n可以把读取scoreboard的过程放到流水线的RF Read阶段,使得ScoreBoard和PRF同时读取,但是这样会出现问题就是比方说指令A和指令C,如果移动到了Regfile Read阶段,指令C无法获知指令A修改的ScordBoard的值,需要加入比较逻辑,当ScoreBoard写入和读取的编号一致的话,就设置为从PRF取得操作数\n对于能够并行执行N条指令的处理器,需要2N个读端口,2N个写端口\n用最简单的方法,就是因为每个FU会把一条指令的计算结果广播送到FU输入和物理寄存器堆,同时也会送出对应的寄存器编号,所以可以直接和源寄存器进行比较就行,需要选择操作数的两个周期Source Drive 和 Execute阶段(为什么是两个阶段????)\nCluster Cluster IQ 通过将一个集中式的发射队列分成几个小的分布式发射队列,每个发射队列只对应一个仲裁电路和FU,这样每个分布式发射队列只需要存储对应的FU能够执行的指令\n(1)可以减少每个分布式发射队列的端口个数\n(2)每个分布式发射队列的仲裁电路只需要从少量的指令进行选择,可以加快每个仲裁电路的速度\n(3)分布式发射队列的容量比较小,指令被唤醒的速度也比较快\n缺点就是一个发射队列的指令对其他发射队列指令进行唤醒时,需要经历很长的走线,可能需要增加一级流水线,这样当两条存在相关性的相邻指令属于两个不同Cluster事,不能背靠背执行\n但是通过对指令进行合理的算法分配cluster,也可以做到周期的合理使用\n对于普通的集中式发射队列,需要3个周期,如果A,B,E在同一个cluster,C,D在另一个cluster那需要5个周期,但是如果A,C分到一个cluster,B,D,E分到另一个cluster中,那就只需要3个周期\n对于非数据捕捉结果的处理器,指令会先读取物理寄存器堆,需要PRF支持多个读端口,所以可以对寄存器堆也采用cluster结构,对每一个采用cluster结构的发射队列使用同一个物理寄存器堆\n原来4个FU有8个读端口,4个写端口,可以变成4个读端口,4个写端口,不过需要让两个PRF保持一致,还需要去更新另一个寄存器(那不是会影响并u行性)\n当两个存在相关性的连续指令属于两个不同的cluster时,后续的指令需要等到前面的指令更新完寄存器堆之后,才能够从寄存器堆读取操作数????? 因为旁路网络最好不好跨越两个cluster\ncluster bypass 采用cluster结构的可以直接去除流水线的Source Drive 和 Result Drive流水段\n在顺序执行的处理器,由于硬件无法调度不相关的指令,非完全的旁路网络会带来很大的负面影响,会产生大量的气泡,显然降低了处理器的性能,因此尽量会采用完全的旁路网络\n可以类比发射队列,在相邻的cluster加入一级流水线来降低路径延时,形成完全的旁路网络\n发射队列的流水和旁路网络的流水导致的延时不会叠加\n存储器指令的加速 memory diambiguation 访存地址也会存在相关性,但是这个相关性是在执行阶段计算出访存的指令之后才能够被发现 ,在解码阶段是无法被发现的\n大部分的store是按照顺序执行的(in-order),可以避免WAW相关性(why?)\nload指令可以分为\n(1)完全的顺序执行,没有WAW和WAR相关性\n(2)部分的乱序执行,顺序执行的store将程序分成不同的块,每当一条指令store的地址被计算出来,store指令和后续的store指令之间的所有load指令可以乱序执行,可以避免WAR相关性的发生\n当一条store指令地址被计算出来后,load指令就具备判断RAW相关性的条件了,每条load指令把它携带的地址计算出来之后,需要和前面所有已经执行的store指令携带的地址进行比较.通过 store buffer保存已经被仲裁电路选择倒是没有离开流水线的指令\n当store被选中时,其实就可以去允许后面的load指令参与仲裁,因为store指令地址计算的结果肯定先于load地址 存在的问题就是,如果在BCD没有被选中完毕之后,指令E被选择了,此时如果指令E和指令D的地址一致,指令D也不应该获取指令E的内容,所有需要判断出那些store指令在load的前面,哪些在后面\nPC值,但是存在向前跳转的指令 ROB,ROB是顺序的,但是由于访存指令还是少数,所以会比较稀疏 解码阶段为load/store指令分配编号 对于上述指令,部分的乱序也是会浪费很多性能\n(3)完全的乱序执行,WAR和RAW都需要在流水线中执行\n只要load指令的操作数准备好了,就可以直接发起仲裁请求了\n可以让load/store共用发射队列,但还是需要独立的仲裁电路,store指令的仲裁电路需要根据年龄,找到最旧的指令(in-order),load指令,只需要选择准备好的最老的一条指令就行了(out-of-order)\n如果分开发射队列的话,store的发射队列只需要使用FIFO结构\n需要精确的预测机制来避免RAW的相关性,例如如果发现一条LOAD指令和之前的STORE指令存在RAW相关性,就先进行记录,在后续从store buffer 中获取数据 ,这样其实也可以去减少store buffer需要的端口和比较电路\n非阻塞cache 阻塞cache:在发生cache缺失的时候,就锁定D-Cache与数据内存之间的数据通路,处理器无法执行其他的load/store指令\n非阻塞cache:在发生缺失时还是可以执行其它的load/store指令,所以需要去保存load/store相应的一些数据,比方说store的数据 ,load的目的寄存器,但是实际上访问存储器还是通过一条数据通路\n为了支持非阻塞cache,需要将那些已经产生D-cache缺失的load/store指令保存起来(MSHR(Miss Status/infornmation Holding Register))\n(1)首次缺失 ,对于一个给定的地址,访问D-Cache时第一次产生的缺失\n(2)再次缺失,首次缺失但是没有被解决,后续访问存储器的指令再次访问发生缺失的cache line ,再次缺失针对的是cache line 不是相同的地址,\nMSHR:\nV:valid ,指示当前的表项(entry)是否被占用 ,首次缺失MSHR本体的一个表项会被占用,Valid置一,直到Cache line从下级存储器被取回来.\nBlock Address : Cache line 数据块的公共地址\nIssued: 表示发生首次缺失的load/store指令是否已经开始处理,即是否向一级存储器发送读数据的请求\nLOAD/STORE Table\nValid : 表示一个表项是否被占用,无论是首次缺失还是再次缺失\nMSHR entry : 表示发生缺失的指令属于MSHR本体的哪个表项,产生缺失的指令可能会对应同一个cache line , 为了避免重复占用下一级存储器的带宽,只会占据同一个MSHR entry ,但是占据多个LOAD/STORE Table\nDest.register : 对于load指令,记录目的寄存器的编号, 对于store指令,这部分记录store指令在store buffer中的编号,一是可以找到store指令所携带的数据,以便和下级存储器中取出的数据块合并,二是能够释放store指令占据的store buffer中的空间\nType :记录访问存储器指令的类型\nOffset:访问存储器的指令所需要的数据在数据块中的位置\n当发生缺失是,首先查找MSHR的本体,如果有相同的表项,代表再次缺失,只需要写到LOAD/STORE Table ,如果没有,需要写入MSHR和LOAD/SOTRE Table\n如果满了,就无法去处理新的访问存储器指令,就阻塞了\n对于load指令,需要把数据送到对应目的寄存器,并写到D-cache里 ,\n对于store指令,需要从Store buffer中找到对应的数据,和数据块合并,然后写到D-cache 里,然后释放store buffer\n(in-cache MSHR )\n在分支预测失败之后,需要去删除LOAD/SOTRE Table正在执行的load/store指令 ,并且如果针对于一个数据块的所有load/store指令都处于分支预测的路径上,那这个数据块也不能去更新到D-cache上去\n关键字优先 就是去改进读取cache块数据的顺序 ,本来是0,1,2,3,4,5,6,7,8,可以修改为 \u0026mdash; 之类的 ,可以去把访存需要的数据提前\n提前开始 在Cache line读取到指令需要的数据之后,就可以让CPU去继续执行了 ,相比于关键字优先不需要额外的硬件,但是如果数据处在数据块比较后面的位置,那就没有太大的用处\n对于I-cache ,虽然指令需要做到顺序取出,但是由于存在分支跳转,也可以通过非阻塞的操作来加快取指,不同于D-cahce的是取出的指令必须是顺序的,如果前面的指令没有被取出来也必须进行等待直到数据被取出\n附录 参考文献 超标量处理器\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-22T13:17:32+08:00","permalink":"https://VastCircle.github.io/2024/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E7%AC%94%E8%AE%B08/","title":"执行"},{"content":"概述 只要发射队列中的一条指令的草做书都准备好了,且满足了发射的条件,就可以送到相应的FU中执行.发射队列的作用就是使用硬件保存一定数量的指令,然后从指令中找出可以执行的指令\n发射时序一般处在处理器的关键路径上,直接影响处理器的周期时间\n(1)发射队列(Issue Queue),用来存储已经被寄存器重命名,但是没有被送到FU执行的指令,也称为保留站(Reservation station)\n(2)分配(allocation)电路,用来从发射队列中找到空闲的空间,将寄存器重命名之后的指令存储在其中\n(3)选择电路(仲裁电路),发射队列中多条指令的操作数都准备好了,电路会按照一定规律,从其中找到最合适的指令,送到Fu中去\n(4)唤醒电路,当一条指令经过FU执行而得到结果数据时,会将其通知给发射队列中所有等待数据的指令,指令对应的源寄存器就会被设置为有效的状态,即为唤醒.\n集中式 or 分布式发射队列 如果所有FU共用一个发射队列,即为集中式发射队列(centralized issue queue,CIQ)\n如果每一个FU都有单独的发射队列,称为分布式发射队列(Distributed issue queue,DIQ)\nCIQ容量大,选择电路和唤醒电路复杂,电路利用率高\nDIQ会出现一个发射队列满了,其他发射队列没有满,但是最终数据被阻塞的情况,就会出现效率低下的问题\n数据捕捉 or 非数据捕捉 寄存器的数据读取时间\n数据捕捉 流水线的发射阶段之前读取寄存器,被寄存器重命名的指令会先读取物理寄存器堆,然后将读取到的值一起写入到发射队列,没有被计算出来的数据会以编号的形式写入,供唤醒时使用,会被标记为无法获得状态(non-available),这些值会通过旁路网络获取.在发射队列中,存储指令操作数的地方称为payload RAM\n一条指令被仲裁电路选中发射到FU中去,它会将目标寄存器进行广播,其他在发射队列的指令就会去对比,有相等的情况时就会在payload RAM进行标记,在FU计算完成之后会写入到payload RAM 对应的位置 .\nmachine width : 每周期实际解码和重命名的指令个数\nissue width : 每周期最多可以在FU中并行执行的指令个数\n在RISC 里 machine width \u0026lt;= issue width\n物理寄存器的端口数 = machine width * 2\n大多数源操作数会经历两读1写,从寄存器读取出来,送到发射队列,从发射队列中读取送到FU ,功耗高 ,面积大\n寄存器重命名方便 ,指令在顺利离开流水线的时候,需要将结果从重排序缓存中搬移到ARF中,采用数据捕捉的方式可以不用惯性指令结果的变化??????\n非数据捕捉 在发射阶段之后读取物理寄存器堆,被重命名之后的指令不去读取物理寄存器堆,而是直接将源寄存器堆的编号放到发射队列中去.当指令被选中时,通过编号读取物理寄存器堆,将读取值送到FU中\n寄存器堆的读端口 = issue width ,比较大\n压缩 or 非压缩 压缩 当一条指令被选中离开发射队列之后,指令上面所有的指令都会下移一格\n通过多路选择器进行压缩\n这种方式选择电路比较简单,通过优先编码选择最旧的就行了,oldest-first方法 ,但是选择电路的延时很长\n优点如下:\n1.分配电路简单,发射队列中的空闲空间总是处于上层,只需要使用发射队列的写指针,指向第一个空闲空间\n2.选择电路简单.最旧的指令存在的RAW相关性也越多,先执行可以最大程度释放和它存在RAW相关性的指令\n但是\n1.实现起来浪费面积\n2.功耗大\n非压缩 没有移动\n发射过程中的流水线 非数据捕捉结构的流水线 要被FU执行\n(1)指令所有的源操作数准备好了\n(2)指令被发射队列选中\n(3)能够从寄存器,payload RAM或者旁路网络获得源操作数\n下图发射过程被分为了唤醒(wake-up)和仲裁(Select)两个流水线阶段\n唤醒阶段,发射队列中的所有相关寄存器会被置为准备好的状态\n仲裁阶段,会使用仲裁电路选择一条最合适的指令送到FU中\ntomasulo算法:在指令执行完才对相关指令进行唤醒\n可以通过将唤醒过程提前来获得更高性能\n即在指令A被仲裁电路选中后就对其他寄存器进行唤醒,这样指令B在下一个周期就能够被仲裁\n意思是 Select 和 wake-up应该是在同一个周期的串行,A被唤醒才能够去selectB\n这种操作称为\u0026quot;原子的\u0026quot;\n拆分流水线可以使得主频升高,但是\n(1)分支预测失败,惩罚增加\n(2)cache访问的周期数增加\n(3)功耗增大\n以上是假设执行是一个周期,实际上并不止\n数据捕捉结构的流水线 可以把select和payload放在同一个流水段 ,在指令被仲裁电路选中之后,在同一个周期对发射队列其他的指令进行唤醒,同时去读取payload RAM,这两个操作是并行进行的,在这个流水段还会负责payload RAM的读取和写入,会导致处理器的周期时间变得过大.\n旁路网络这样是啥意思????\n另一种设计方式是把payload单独放成一个流水段,旁路和执行分成两个流水段,在旁路阶段,FU的结果会被送到payload RAM和FU的输入端\n分配 对于非压缩的方式设计的发射队列,需要分配电路扫描整个发射队列,找到四个空闲的表项并将四条指令写入\n可以使用一个表格来记录所有空闲表项的编号,按照FIFO的方式管理,也可以简单把发射队列分为多个部分,每个段选一个空闲编号,但是会出现问题就是如果有一个表项非空的话,会阻碍其他指令的放入,甚至由于在寄存器重命名阶段是in-order状态A的无法放入会导致后续指令都无法放入\n仲裁 最好实现oldest-first功能的仲裁\n1-of-M仲裁 可以通过指令在ROB中的位置作为指令的年龄信息,但是由于ROB是一个循环队列,所有单纯的地址是无法表征年龄的\n其实我觉得,直接比较读写地址可以的吧,读指针 \u0026gt; 写指针 , 下新上旧 , 读指针 \u0026lt; 写指针 , 上新下旧 ,其实读指针 \u0026gt; 写的时候,也代表两者不是一面的\n可以在ROB中地址前面再加入一位,称为位置值.想当于对于读写地址又加了一位\n(1)位置值相同时,ROB地址越小,对应的指令越旧\n(2)位置值不同时,ROB地址越大,对应的指令越旧,比方说情况2的 0 10 与 1 01 比较,明显是0 10 旧\n先根据是否rdy选出指令,再根据年龄进行筛选\n二分\n该电路能够得到最小的年龄值,但是还需要得到最小年龄值对应的指令,最方便的是将指令信息也一同附上去\nN of M 仲裁电路 几个FU共用一个发射队列,发射队列需要在一个周期内为没一个FU选择出一条指令,就要求有一个N of M的仲裁电路\n可以通过两级仲裁电路实现,第一级选择一条指令后对第二级进行标记,但是这样延时极大\n对每一个FU使用一个1 of M的仲裁器, 根据指令类型进行分类,这样就会存在相同类型的指令会阻塞或者一部分FU处在空闲状态的问题\n可以通过增加FU的数量解决上述问题,但是比方数两个ALU,指令该分配给哪个ALU又是一个问题,可以通过轮换分配法实现,但是这样是无法保证严格的oldest-first原则的,而且有可能会浪费FU资源\n一般来说,加减法,逻辑运算,移位运算合成一个FU,\n惩罚和除法合成一个,\n访问存储器和访问协处理器合并在一起,\n浮点运算合并在一起\n唤醒 单周期的唤醒 唤醒是指被仲裁器选中的指令将目的寄存器的编号(dst_tag)和发射队列中所有源寄存器的编号进行比较,并将那些比较结果相等的源寄存器进行标记的过程\n下面的电路是所有仲裁电路共享一个发射队列的情况, 所以发射队列只会接受到一个响应,因为同一时刻肯定只有一条指令被仲裁,每个FU都会使用一个仲裁电路\nimage-20241020213306752 (1)ValL:指令中是否存在第一个源寄存器\n(2)SrcL:指令中第一个源寄存器的编号\n(3)RdyL:指令中第一个源寄存器是否已经被唤醒而处于准备好的状态\n(4)ValR:第二个\n(5)Dest:目的寄存器的编号\n(6)Issued:一条指令被仲裁电路选中之后,可能不会马上离开发射队列,需要进行标记,这样的指令不会向仲裁电路发出请求信号\n为什么有四个仲裁电路?????是不是指多个FU,但是多个FU不是应该可以接受多个请求\n发射队列的每一个表项都会根据四个响应信号的值,将自身的目的寄存器编号送到对应的总线上去,每个仲裁电路对应一个总线\n被仲裁电路选择的指令会将它的目的寄存器编号送到对应的总线上 每一条总线的值会和发射队列中所有指令的源寄存器的编号进行比较,如果发现相等,标记为准备好的状态 当发射队列某条指令的操作数都准备好了,并且没有被仲裁电路选中过,就可以想仲裁电路发送请求信号 如果仲裁电路发现有更高优先级的指令发出请求,当前指令不会得到有效响应信号,需要再之后的周期继续发送请求信号.在一些设计中,可以轮流向多个仲裁电路发送请求.如果从仲裁电路中得到有效信号,就会吧issued置位.一条被选中的指令不会立刻离开发射队列,因为一个指令如果使用了load指令的结果,即使被仲裁电路选中,也不能离开 发射队列的指令更具响应信号,,把目的寄存器编号送到对应总线上去,用来唤醒发射队列中所有相关的源寄存器 多周期的唤醒 单周期的唤醒能够在一个周期被FU执行完毕,但是当一条指令无法在一个周期执行完毕时,需要根据她在FU中的周期数,将唤醒过程延迟\n根据唤醒的过程\n延迟广播. 发现被仲裁电路选中的指令执行周期大于1,则在选中的当前周期,不讲指令的目的寄存器编号送到总线上,而是根据选中指令需要执行的周期数(N),延迟N-1周期,才送到总线上去\n延迟广播之后可能出现tag bus产生冲突,比方说下面的MUL和ADD在同一时刻需要将目的寄存器的值送到tag broadcast bus上\n可以通过增加总线的数量,也可以利用表格,记录下FU执行指令所需要的周期数,被仲裁电路选中的指令,如果发现冲突,被选中的指令不会送到FU中执行,而是在下一个周期继续参与仲裁\n但是还是存在一个问题,就是指令B被否决(cycle 1),本身指令C是可以被仲裁的,但是C比B要新,所以这个周期被浪费掉了,所以可以先检查是否冲突,如果冲突的话就不向仲裁电路发起请求了(不发请求很奇怪啊,发请求在仲裁的时候否决行不行),但是这样访问网络和仲裁电路是串行的\n延迟唤醒 最优解就就是要去实现背对背执行,一条指令的执行和后一条指令的执行是先后的\n在比较结果相等时,不马上置为准备好的状态,而是根据指令所需要的执行周期数,进行相应周期的延时,然后再改变发射队列中源寄存器的状态\n通过移位寄存器实现延迟唤醒的效果.在解码阶段对每条指令执行周期数进行编码,称为DELAY,在将目的寄存器送到总线外,还需要将DELAY值也送到总线上去.称为DELAY bus .\nFreed :表项是否空闲\nIssued:指令是否被仲裁电路选择\nSrcL:第一个源寄存器编号\nSrcL_M:当寄存器编号比较结果相等时,置1;当接收到仲裁电路的响应信号后,清0,它是移位寄存器进行算数右移的使能标志\nSrcL_SHIFT:移位寄存器,当编号比较结果相等时,将DELAY写入移位寄存器,每周期进行算数右移\nRdy:表示第一个源寄存器是否准备好了\nSrcR_imm_valid :表示第二个操作数是否是立即数\nROB ID:指令在ROB的位置,使得其能够实现oldest-first选择\n编码形式是类似于11111000(8位),在经过3个算数右移之后最低位就是1,就是Rdy=1\n直到被仲裁电路响应或者说选择之后,SrcL_M,srcL_SHIFT都会清零,其他时候Rdy都会保持着1的状态发送请求,直到被仲裁\n推测唤醒 对于某些指令,指令在FU中执行的周期数是可以被预测的,这样才可能分配一个确定的DELAY值\n但是对于\n(1)Load指令\n(2)某些处理器的特殊情况,例如RowerPC 603处理器存在early out,即当被除数值比较小时能够被提前预测指令\n比较简单的方法就是等指令执行完了之后再去唤醒其他指令\n可以优化一下就是一般load指令在第一个周期计算地址,第二个周期访问Tag SRAM,第三个周期将读取到的数据写入目的寄存器,所以在第二个周期就可以判断,命中后去唤醒\n假设d-cache是一直命中的,就能够得到一个比较理想的情况 但是一旦指令A发生了D-cache缺失,此时B就不能停住而等待操作数,这样会使得FU无法接受其他新指令,严重影响处理器的性能.最好的办法是将指令B重新放回发射队列(Issue Queue),因为load指令在D-cache缺失之后,会到L2 cache寻找数据,此时可以假设L2 cache是命中的,并按照命中时间重新对相关寄存器进行唤醒,还是使用延迟唤醒\n对于不确定周期的指令,可以去预测指令执行的周期数,在指令得到结果之前,对相关的指令进行唤醒操作\n预测成功就执行,预测失败就去进行状态会被,被唤醒的所有寄存器需要重新设置为(not ready)状态,如果一些指令离开了发射队列,还需要从流水线中抹去,放回发射队列\n未完待续\n附录 参考文献 版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-19T20:25:19+08:00","permalink":"https://VastCircle.github.io/2024/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E7%AC%94%E8%AE%B07/","title":"发射"},{"content":"超标量处理器的寄存器重命名 对于 Dest = Src1 op ASrc2\n(1)从RAT中找到Src1和Src2对应的物理寄存器Psrc1和Psrc2\n(2)从空闲列表(Free list)中找到一个空闲的物理寄存器Pdest,将其作为指令的目的寄存器Dest对应的物理寄存器\n(3)将逻辑寄存器Dest和物理寄存器Pdest的映射关系写到RAT中\nRAT需要3个读端口(Src1,Src2和Dest作为地址) Dest这个端口用于和ROB进行交互,将之前的映射关系写入ROB中\n对于超标量,就需要成倍的端口\n(1) A,B存在RAW相关性 , 所以r0对应的物理寄存器之列来自于指令A对应的P30,不来自于从RAT读取的值\n(2)A,B,D存在WAW相关性,\nA.在写入RAT时,如果多条命令有同一个目标寄存器,那映射关系实际上写的还是最新的那条指令\nB.在将旧映射关系写入ROB的时候,如果发现一个周期内有多条指令都使用的同一个目的寄存器,此时写入到ROB中的旧映射关系不再来自于RAT读取的值,还是直接来自于和他存在WAW相关的指令,例如指令B的物理寄存器来自r0,或者所物理寄存器来自于P31\n(3)B,D存在WAR相关性,通过寄存器重命名可以客服\n解决RAW相关性 意思就是如果在同一周期进行寄存器重命名,对于源寄存器,应该获取当前赋值给目的寄存器的物理寄存器(P31),而不是之前的物理寄存器(P25),所以需要进行组内相关性检查,由于此时是顺序的,所以相关性检查和顺序处理器相似,只需要比较源寄存器与目的寄存器的编号就可以了\n解决WAW相关性 对写RAT进行检查 在寄存器重命名周期,如果存在多个指令的目的寄存器都相等的情况,那么只有最新的那条指令的映射关系才运行写入到RAT中 ,可以通过检查目标寄存器来实现,例如对于dst0只要和dst1,dst2,dst3中的任意一个存在相同的情况,就不需要将dst0对应的映射关系写到RAT中\n对写ROB进行检查 为了能够释放掉那些不再使用的物理寄存器,同时可以对处理器的状态进行恢复,每条指令需要从RAT中读出他以前对应的物理寄存器,并将其写到ROB当中,如果两条指令存在WAW,那么比较新的这条指令对应的就的物理寄存器就直接来自与比较旧的那条指令,而不是RAT中\n通过比较指令与前面指令的目的寄存器就可以实现\nRAT的SRAM结构 读优先 : 当前写入的数据在下一个周期才能被读取\n写优先:当前写入的数据在当前数据就能够被读取\n对于RAT,需要做到先读,读完再修改,所以采取读优先\n对于读取目的寄存器,由于本来就要获取目的寄存器之前对应的物理寄存器,所以读优先是必须的\n对于读取源寄存器,其实需要得到新的映射,需要使用之前的RAW相关性的检查和处理电路\n特殊情况的标记 对于没有一个目标寄存器和两个源寄存器的指令,采取以下方式\n(1)根据需要重命名的目的寄存器个数,觉得当前周期需要从空闲列表读取数字的个数\n(2)使用目的寄存器读取RAT时,目的寄存器不存在的指令不会读取RAT\n(3)使用源寄存器读取RAT时,源寄存器不存在的指令不会读取RAT\n(4)在RAW和WAW相关性检查时,如果源寄存器和目的寄存器不存在,那就忽略\n寄存器重命名的恢复 使用checkpoint对RAT进行恢复 SRAM的最小存储单元(Main Bit Cell,MBC), (Checkpoint Bit Cell ,CBC)\n当需要对RAT进行状态保存时,将MBC的内容复制到指定的CBC中(Allocation),当对RAT进行状态恢复时,将对应CBC的内容复制到MBC中(Restore)\n使用WALK对RAT进行恢复 对每一条指令,在ROB中都储存了这条指令之前对应的物理寄存器,利用这个信息,可以将RAT的状态逐步\u0026quot;倒回去\u0026quot;,使得那些处在错误路径上的指令,对RAT的修改都进行修复\nROB中储存着物理寄存器,逻辑寄存器,之前的物理寄存器,一条一条回退到之前的物理寄存器,应该就可以做到\n使用Architecture State对RAT进行恢复 在流水线提交阶段有一个RAT,,叫做aRAT(architecture RAT),它所保存的物理寄存器和逻辑寄存器的映射是完全正确的\n对于如下的指令,在重命名阶段的RAT,对于r1的映射应该是P34,但是实际上此时指令D是处在推测阶段,是有可能被冲刷掉的,但是对于aRAT,它保存的就是已经提交的指令之间的映射,例如R1对应P31,它的状态是完全正确的\n所以说,可以通过aRAT进行恢复,具体就是在分支预测失败时,让指令继续执行,直到分支指令变成最旧的一条指令,那此时所保存的状态就是分支指令之前的指令所得到的状态,再复制,就能够恢复了\n等到分支指令变到流水线最旧的指令,才恢复RAT的另一个好处就是,如果在一条分支指令之前存在异常或者另一个分支预测失败,那这条分支指令就不会被处理,也避免了一些无用功\n分发 (1) 发射队列 (out-of-order),指令在送到FU中被执行之前,先被放到一个缓存中,每个FU都对应一个发射队列,\n只要一条指令的所有源操作数都准备好了,就可以直接送到FU中执行,不用理会指令的原始顺序,在多发射处理器中,需要从缓存中找到多个空闲的表项\n(2)发射队列(in-order),分支指令和store指令是按照顺序执行的,该队列就是FIFO\n(3)重排序缓存(ROB),将乱序拉回顺序\n分发就是将寄存器重命名之后的指令写到发射队列和重排序队列当中\n附录 参考文献 版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-19T11:46:41+08:00","permalink":"https://VastCircle.github.io/2024/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E7%AC%94%E8%AE%B06/","title":"寄存器重命名(超标量+过程恢复)"},{"content":"概述 WAW 和 WAR (写后写 和读后写) 可以通过更换寄存器的名字来解决相应冲突\n存在原因 (1)有限个数的寄存器\n(2)循环体,很容易出现写后写冲突\n(3)代码重用,一些小函数被频繁的调用\n物理寄存器(Physical Register)和逻辑寄存器(Logical Register或者 architecture Register)物理寄存器数量多于逻辑寄存器\n重命名映射表 (Register Renaming Table, Register Alias Table ,RAT)空闲寄存器列表(Free Register List)\n寄存器重命名的方式 (1)将逻辑寄存器(architecture Register File,ARF)扩展来实现\n(2)使用统一的物理寄存器(Physical Register File,PRF)实现\n(3)使用ROB实现\nROB 将ROB作为物理寄存器,存储所有**推测状态(speculative)**的结果,使用逻辑寄存器(ARF)存储所有正确的结果\n当一条指令被写入ROB中的一个表项(entry)时,表项的编号即为物理寄存器,这样将逻辑寄存器和表项建立了关系,\nROB存储着所有没有离开流水线的指令结果,逻辑寄存器(ARF)存储着所有\u0026quot;最新\u0026quot;离开流水线的指令结果\n重命名映射表用来指示每一个逻辑寄存器的值是位于ROB中还是位于ARF中\n缺点 (1)即使没有目的寄存器也会占用ROB的一个表项,代表物理寄存器的浪费\n(2)对应ROB和ARF需要有多个读端口来支持多条指令的访问\nARF扩展 可以使用一个独立的存储部件来存储流水线中所有指令的结果,只有那些存在目的寄存器的指令才会占据该部件,称为 PRF(Physical Register File),PRF和ROB类似,只是在没有目标寄存器的指令不会占据PRF,寄存器重命名时存在目的寄存器的指令会占据PRF的空间,在退休时,结果会从PRF搬移到ARF中\n重命名映射表用来指示每一个逻辑寄存器的值是位于PRF中还是位于ARF中,需要保存PRF的地址空间\n使用统一的PRF 存储所有推测的和正确的寄存器值,\n使用空闲列表记录PRF哪些寄存器处在空闲状态\n当指令被寄存器重命名,并且存在目的寄存器的时候,就会占据PRF当中的一个寄存器,该寄存器会经历值未被计算,值被计算但是没有退休,退休三个过程\n通过重命名映射表存储每个逻辑寄存器和物理寄存器的对应关系\n寄存器重命名时,\n源寄存器:查找重命名的映射表(RAT),找出对应物理寄存器的编号\n目的寄存器:给目的寄存器指定一个空闲状态的物理寄存器,并且该关系会被更新到RAT中\n指令退休之后释放物理寄存器\n一条指令之后在退休的时候,结果才会被外部看到,推测时是无法被外界看到的,需要使用另外一个RAT,存储所有\u0026quot;退休\u0026quot;状态的指令和物理寄存器的对应关系(啥时候释放?),外部只能通过查找这个RAT,找到逻辑寄存器对应的物理寄存器\n**只有后续的指令不使用物理寄存器之后,物理寄存器才能够变成空闲.**可以采取比较保守的方式,就是当一个指令和后面的莫条指令都写到同一个目的寄存器时,前面指令的物理寄存器可以释放了,**所以在ROB中除了记录逻辑寄存器当前对应的物理寄存器之外,还需要存储它之前对应的物理寄存器,**以便在指令退休的时候,将旧映射关系释放\n优点 (1)寄存器的值只需要被写入一次?\n(2)源寄存器的值只能存储在一个地方,即PRF中\n重命名映射表(RAT) RAT是一个表格,使用逻辑寄存器作为地址寻址,对于指令的源寄存器,可以从表格中得到对应的物理寄存器的编号\n对指令的目的寄存器来说,会将物理寄存器编号写到这个表格,即建立映射关系\n可以使用多端口的SRAM(sRAT)和CAM(cRAT)实现,CAM(内容寻址的存储器)\nSRAM表项个数等于逻辑寄存器的个数,里面存放对应物理寄存器的编号,位宽为log(物理寄存器数量)\nCAM表项个数等于物理寄存器的个数,里面存放对应逻辑寄存器的编号,位宽为log(逻辑寄存器数量),寻址时逻辑寄存器的编号会和每个表项进行对比,返回对应的地址\n使用SRAM寻址功耗小,面积小\n由于对于cRAT进行checkpoint只需要保存状态位(V),而不需要将整个cRAT进行保存,能够大大减少checkpoint电路的面积,当checkpoint数量大时,反而cRAT具有优势\n基于SRAM的重命名映射表 checkpoint需要把整个sRAT都保存下来\n对于4-way的超标量处理器,每周期最多需要对四条指令进行寄存器重命名,sRRAT需要8个读端口和4个写端口(每条指令包含2个源寄存器和1个目的寄存器)\n新写入到sRAT的值会覆盖掉原来旧的对应关系,需要记录下来\n(1)方便指令在退休的时候,将对应的物理寄存器变为空闲状态???? (还是无法理解,按理来说覆盖了说明该逻辑寄存器又分配了新的物理寄存器,那原来那个确实可以删除了)(覆盖的时候后面的指令还没有退休,是有可能无效的(分支失败异常之类的,那后面分配的必定是要被还原的,所以物理寄存器是应该在后面指令退休的时候再变成空闲状态))\n(2)当一条指令之前存在异常或者分支预测失败时,需要从流水线中被抹去 ,同时这条指令对于RAT的修改需要被恢复过来,通过将旧的映射关系保存下来,可以协助RAT的修复\n缺点就是无法使用多的checkpoint\n只要预测的足够准,就不怎么需要checkpoint ,就可以去减少checkpoint的数量,但是如果预测错了又没有checkpoint,那对于RAT的恢复也会很麻烦\n??? RAT里面的值不是应该也是保存在ROB里的吗,那保存ROB不就行了,为什么还要RAT\n基于CAM的重命名映射表 任意时刻,每个逻辑寄存器都只有一个物理寄存器与之对应,可以使用一个有效位(V)表示\ncRAT需要8个读端口和4个写端口(每条指令包含2个源寄存器和1个目的寄存器)\nSRAM + CAM ,SRAM用来存储每个物理寄存器对应的逻辑寄存器,CAM用来进行内容的比较\n需要等到后面写入到同一个逻辑寄存器的指令退休(retire)的时候,才可以将这个逻辑寄存器之前对应的物理寄存器变为空闲状态\n并不是一个物理寄存器对应的有效位为0,就表示物理寄存器是空闲状态,有可能是这个映射关系刚刚被覆盖了.通过使用ROB和空闲列表可以管理物理寄存器何时变为空闲\n在分支指令寄存器重命名之前,将cRAT的有效位保存起来;在流水线的后续阶段,发现分支指令预测失败是,将分支指令对应的checkpoint写回到cRAT的有效位就完成恢复了(why?这样能保证恢复映射关系?)\n恢复时可能把一些本身处在非空闲状态的物理寄存器变成了空闲状态,因为非空闲状态的物理寄存器有效位也可以是0\n有可能在进行checkpoint保存的时候为0,到了状态恢复的时候变成1了.典型情况是物理寄存器在变为空闲之后又被新的指令使用了.但是该指令处在分支预测失败或者异常路径上.应该是要恢复为0的\n举例 在指令F进行寄存器重命名时,需要对cRAT进行Checkpoint保存\n分支F被发现了分支预测失败,对cRAT进行状态恢复前 ,在F解码时,就保存了此时的状态,所以如果预测失败写回,就直接恢复了那之前的表\n每次在流水线的寄存器重命名阶段遇到分支2指令时,都会从表7.2找出一个空闲的GC来存储此时的有效位(V),并将GC的编号放在分支指令的信息中,这样当得到分支预测的结果之后,就可以根据编号来找到与之对应的GC.在分支预测失败之后直接进行相应的复原\n为了保证正确性,在分支预测失败进行恢复的时候,需要对空闲列表(free list)也进行状态恢复,那些别占用的物理寄存器都将重新变为空闲的状态(通过恢复free list的读指针)\n对cRAT进行状态恢复,就是要还原出逻辑寄存器真正对应的物理寄存器,因为会续分支预测失败路径上的指令可能会修改对应关系,需要进行纠正.对于物理寄存器的空闲管理交给ROB和free list\n附录 参考文献 超标量处理器设计\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-18T19:25:31+08:00","permalink":"https://VastCircle.github.io/2024/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E7%AC%94%E8%AE%B04/","title":"寄存器重命名(方式+映射表)"},{"content":"分支预测的目标地址预测 对于直接跳转的分支指令,由于它的偏移值(offset)是以立即数的形式固定在指令中,目标地址是固定的,只需要记录分支指令的目标地址即可.\n对于间接分支跳转,大部分是CALL和Return ,所以可以进行一定程度的预测\n直接跳转类型的分支预测 (1)当分支指令不发生跳转时,\n目标地址 = 当前分支指令的PC值 + Sizeof(fetch group)\n(2)当发生跳转时\n目标指令 = 当前分支指令的PC值 + Sign_Eextend(offset)\nBTB 通过BTB(Branch Target Buffer)(相当于一个cache)使得多个PC值共用一个空间来存储目标地址,\nindex + tag ,\nBTA (Branch Target Address)分支目标地址\n可以使用组相联的BTA来提高分支预测的准确率\n如果已经被替换了,那该跳哪去 : 先跳再冲刷吗?\nimage-20241016185417711 partial-tag BTB 如果映射到BTB中的指令中只有一条,那可以简化tag的部分,只使用很小的一部分,\n这种方法实际上也是在赌,减少了tag的大小,万一出现了重合,那就会出现目标地址预测失败的情况,但实际上,如果出现了重合,即使不减少tag,仍然会导致预测失败.\nimage-20241016185852818 和之前类似,仍然可以采取一定运算,来降低tag的位数,比方说异或\n我比较好奇,如果tag没有对上,对于直接相连来说,本身也无法得到正确地址,那为什么不直接把tag删除了\n应该是组相联有用吧\nimage-20241016190929007 BTB缺失的处理 停止执行 暂停取指,直到目标地址被计算出来\n对于直接跳转指令,在解码阶段就可以分离出偏移值\n解码阶段分离指令 停止取指会造成气泡,其实就是导致流水线停滞\nimage-20241016192510520 继续执行 使用顺序的PC值去指令\n计算出的地址和原来PC不一致,就冲刷流水线,重新开始取指\n这么做会浪费功耗\n间接跳转类型的分支预测 CALL/Return 指令的分支预测 CALL的地址一般也是固定的,所以也可以通过BTB进行预测\nReturn的目标地址是不固定的,但是Return的目标地址总是等于最近一次执行的\n但是Return指令的目标地址,是按照CALL指令执行的相反顺序排列的\n所以可以做一个存储器,保存最近执行的CALL指令的下一条指令的地址,这个存储器是后进先出的(Last In First Out,LIFO),原理与堆栈类似,称为返回地址堆栈(Return Address Stack,RAS)\nCALL/Return 指令分支预测 RAS工作条件 (1)需要及时保存PC + 4的值, 指令类型只有在解码阶段才能获知,因此可以在BTB中多加一项来保存分支指令的类型,这样在后续取到这一条指令就可以获知分支指令类型\n(2)在对Return指令进行目标地址预测是,能够选择RAS的数据,而非BTB的数据,按照上面的方法就可以做到\n将指令类型存储到BTB中 RAS满了 如果函数层次过深,就会出现RAS无法继续存放的问题\n(1)不保存CALL了,这样下一次Return 就会出现分支预测失败,并且还要求RAS指针不发生改变\n(2)继续按照顺序向RAS写入,此时最旧的会被覆盖掉.最后一次return 可能会出现分支预测失败,但是也是可能性事件,比方说递归函数\n可以通过带计数器的RAS来扩展RAS的容量, 即对于相邻的CALL,如果是同一条指令,就存放在RAS的同一个地址,再用计数器进行标识\n其他指令的预测 case指令 image-20241016210205293 使用基于局部历史的分支预测方法,把PHT换成了Target Cache,\n每当分支指令执行一次,就将目标地址写到Target Cache 中\n小结 分支预测使用 BHR , GHR和饱和计数器配合进行分支指令方向的预测\n使用 BTB, RAS和 Target Cache对分支指令的目标地址进行预测\n完整的分支预测方法 , decoupled BTB : 将分支指令的方向预测独立于BTB ,本身不会被记录到BTB的分支指令也会被记录(不跳转的分支也会记录到BTB)\n预测为发生跳转,但是发生了BTB缺失,比发生分支预测失败的情况好,可以节省功耗\n完整的分支预测方法 分支预测失败的恢复 处在错误路径上的指令有可能已经将处理器中某个部位的内容进行了更改,例如寄存器重命名阶段的重命名映射表(mapping table),需要对操作进行撤销,即分支预测失败时的恢复\n分支预测检查 (1) 解码阶段可以检查直接跳转的正确性,可以得到分支指令的方向和目标地址,\n对于间接跳转,即使得知预测错误,也无法得到正确的地址,但是可以通过流水线暂停来避免抹掉指令造成的功耗浪费\n(2)在读取物理物理寄存器的阶段,读取到寄存器的值,就可以得到目标地址是否错误,进行重新取指令,\n还是需要对不必要的指令进行抹去,对于进入发射队列的指令,可能比较困难,需要选择性的进行抹去\n(3)在执行阶段,任何分支指令的结果都可以被计算出结果,可以进行检查,但是造成的惩罚(penalty)是最大的.需要清除在这条分支指令之后进入流水线的所有数据\n基于ROB的恢复 在乱序执行中,在这条分支指令之前的数据也会在发射队列或者执行中,可以采取重排序缓存(ROB)对处理器进行状态恢复 (ROB是顺序存储指令的)\n当发生分支指令预测失败时,将信息记录在ROB对应的表项(entry)中,并且暂停流水线的取指令,但是让流水线继续执行,当这条指令变为最旧的指令后,冲刷掉流水线中的所有数据,重新取指令. 缺点就是停滞时间会比较长.\n基于checkpoint的状态恢复 checkpoint, 发现分支指令,并且在分支指令之后的指令更改处理器的状态之前,将处理器的状态保存起来,包括寄存器重命名中使用的映射表(mapping table),预测跳转的分支指令对应的下一条指令的PC等.在寄存器重命名阶段进行.\n需要将流水线中所有处于分支预测失败路径上的指令抹去. 需要一种机制识别哪些指令处在错误的路径上,可以通过编号实现,(编号可以在顺序阶段就编号完成),编号之后就可以获知哪些指令位于分支指令后面\n分支指令的编号个数决定了最多可以在流水线中存在的分支指令个数:假设处理器中最多支持128条指令存在于流水线中,按照每五条指令存在一条分支,最多后128/5 = 26 条分支指令存在与流水线中,需要5位\n所有在流水线中的分支指令会被分配一个编号值,编号会被保存在FIFO中,称为编号列表(tag list)\n可以使用 (free tag list 和 tag list)来进行设计\n编号值不再被使用 : 分支指令成功retire , 分支预测失败 (分支预测失败之后就要根据编号来冲刷流水线了,所有编号可以回收了)\n流水线抹去 (1)发射之前的所有指令需要全部被抹去\n(2)流水线的发射阶段以及之后的流水段中,使用比寻找分支指令之后的指令全部抹去\ntag list 是顺序保存对应标号的 , 所以 比方说监测到分支指令3 预测失败,所以 0 ,1, 4 都需要被直接清除 , 因此通过广播编号值及将ROB中对应的指令置为无效\n一个周期内使用所有编号去抹去ROB的指令是不现实的,可以采取一个周期广播一个编号的方式 , 因为从取指到发射还是需要经过几个周期的,只要在这之前重排序缓存和发射队列指令被抹去了就行了\n编号值在解码阶段分配最合适 ,因为此时已经知道属于分支指令了\n对于多条分支指令,通过控制第二条分支指令及其后面的所有指令在本周期不能进入解码阶段,可以避免使用多端口的FIFO来进行赋值\nPTAB (Prediction Target Address Buffer) 通过将分支指令的预测值保存到一个缓存中,使得其在执行阶段进行分支预测是否正确的检查时能够正确调用,并且可以只保存方向预测为跳转的分支指令 PTAB , (Prediction Target Address Buffer)\n它不是本身就在BTB中吗,为啥还要一个buffer\n(1)valid , 表示PTAB中某个表项是否被占用, 当分支指令写入PATB时,置1,当完成检查之后,Reset\n(2)Predict Address,分支指令被预测的目标地址\n(3)Next PC, 分支指令的下一条PC , 如果预测错误,就直接使用其作为正确地址取指\n怎么去找PTAB对应的表项 ? 用 PC吗 ? 或者说用 Next PC吗\n写PTAB可以在取指阶段就完成\n自修改代码一般都会去清空分支预测器和I-Cache\n超标量处理器的分支预测 由于超标量取一个地址,会取出多条指令,所以如果只使用取指令时的地址进行分支预测,相当于只是对指令你个组中的第一条指令进行分支预测\n可以使用公共地址寻址分支预测器 (对于4-way超标量处理器[31:4]),因为多数情况下,实际只有一条分支指令 .在BTB中需要记录下分支指令在四条指令中的位置,避免错误使用它的结果 (为什么指令会出现非对齐存储?)\n目标地址的预测 要对指令组的所有指令进行分支预测,需要得到所有指令的PC值,需要使用3个加法器实现PC地址的获取,\n但是由于需要同时获取四个PC值对应的目标地址,需要BTB支持四个读端口,即使采用交疊避免真正的多端口,但是硬件利用率还是较低\n在分支指令的方向预测完毕之后,利用结果信息再进行目标地址的预测,可以避免对于BTB部件的多端口需求,,这种方法对于方向预测和目标地址预测是串行的\n对于RISC指令,大多数指令是直接跳转类型,目标地址无需预测,在取指之后实际就可以被计算出来.实现这样的功能需要进行预解码\n目标方向的预测 对于基于局部历史的分支预测方法来说,需要PHT和BHT支持多个读端口,可以通过交疊(interleaving)模拟实现多端口\n对于全局历史的分支预测,由于一个周期内进行分支预测的多条指令对应的GHR是不同的,需要进行特殊的处理\n交疊 : 7位地址Addr[6:0],通过Addr [1:0]进行寻址bank ,通过Addr[6:2]寻址bank对应的内容 ,就是使用多个单端口的存储器去组成多端口的功能\n附录 参考文献 超标量处理器设计\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-16T18:26:42+08:00","permalink":"https://VastCircle.github.io/2024/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E7%AC%94%E8%AE%B03/","title":"分支预测(目标地址预测)"},{"content":"概述 分支预测需要的内容 方向，决定跳转与否\n目标地址 决定跳转的目的地，riscv中有两种体现形式\nPC + 立即数，跳转范围受限。\n寄存器跳转，预测风险难度高，但是除了RETURN/CALL,一般建议不使用间接跳转。\n分支预测的解码 快速分辨出哪条指令是分支指令\nI-cache得出结果可能需要多个周期，这些周期无法得到准确的预测结果\n解码+分支预测放在一个周期 ， 严重影响周期时间\n快速解码 可以在指令从L2 cache 写入到I-cache时进行快速解码,(pre-decode),然后将指令否是分支的信息也写入I-cache.\n分支预测的最好时机是在当前周期得到去指令地址的时候\n可以直接通过PC值来进行分支预测，那就不需要进行解码了，但是只能够知道它是分支指令\npc分支预测 分支预测的方向预测 跳转 （taken)和不发生跳转（not token）\n一bit的跳转预测 image-20241015182744711 基于两位饱和计数器（2-bit saturating counter) 根据分支前两次的结果预测下一次的结果\n状态机 基于两位饱和计数器 （1）计数器处于饱和状态，分支指令本次被预测发生跳转\n（2） 计数器处于不饱和状态，分支指令预测发生跳转\n（3） 计数器处于不饱和状态，分支指令预测不发生跳转\n（4） 计数器处于饱和状态，分支指令被预测不发生跳转\n初始状态位于 strongly not taken 或者 weakly not taken\n状态机处于饱和状态，只有两次预测失败才会改变预测的结果\n对于以下的情况，该种方法能够有50%的成功预测率\nimage-20241015184801299 TTNTNTNTNT 对于这种情况，预测还是有问题，就是始终进入不了饱和，那还是相当于1bit\n另外两种预测方法 情况1是如果两次连续的跳转，就直接变成饱和的强跳转，那就需要两次不跳转才能预测为不跳转\n情况2是如果两次连续的不跳转，就直接变成饱和的不跳转，那就需要两次跳转才能预测为跳转\n利用格雷码降低功耗，减少出错的概率\n对于一般的for循环，TTTTTTTTTTTTTTN ,只会出现2次预测失败 ，开始时 weakly not taken , 当再次执行for循环，第一次就会预测成功\n存储方式 每一个PC需要一个两位的饱和计数器， 32 位 PC需要 2^30 * 2b 存储器 ， 使用如下方法存储（PHT（Pattern History Table））: 使用 PC的一部分进行存储\n别名 （aliasing) 不同PC有相同的饱和计数器，导致相互之间的干扰\n中立别名 ： 分支指令的方向一致\n破坏性别名 ： 分支指令的方向不一致\n使用PC值的一部分来寻址饱和计数器 image-20241015194600012 避免别名的方法 —— 哈希表 哈希表能够压缩32位PC到一个比较小的值\nimage-20241015194955893 更新时间点\n（1）在流水线的取指令阶段，进行分支预测，根据预测的结果更新PHT 预测的结果更新PHT肯定不合理\n（2） 在流水线的执行阶段，当分支指令的方向被计算出来时，更新PHT\n（3） 在流水线的提交阶段，当分支指令要离开流水线是，更新PHT\n对于2,3，分支指令可能在PHT更新之前就被取过很多次了，会影响结果，但是影响的不多\n在乱序执行中，即使在执行阶段得到了一条分支指令的结果，也无法保证该结果是正确的，因为分支指令可能位于分支预测失败的路径上，所以（3）是最保险的\n顺序执行不会吗 ？ 不会 ，主要是乱序执行有可能前面的指令后于后面指令的执行 ，这样后面的指令不一定会执行\n基于局部历史的分支预测 BHR(Branch History Register):分支历史寄存器\n通过一个寄存器记录一条分支指令在过去的历史状态\nn位BHR记录n次结果\nBHR和PHT一一对应，BHR有多少种取值，PHT有多少表项（entry)\n结果从BHR右侧移入，对应的BHR值改变对应的PHT\n那就相当于把一个PC对应的表项有进行了细分 -\u0026gt; 一个BHR和多个PHT\n如果一个序列，连续相同的数有p位，则虚了的循环周期为p,只要BHR不小于p,就可以做完美预测\n寻址 将所有分支指令的BHR组合在一起称为分支历史寄存器表(Branch History Register Table,BHT)\n如果进行全寻址的话 ， 1个PC值 需要 N位BTR + 2^N * 2 位PHT , 2^n 就需要 2^n（ N + 2^N * 2）\n所以需要PC部分值来寻址\nimage-20241016102002253 1个PHT\nPC部分值寻址PHT,PC通过hash处理寻址BHT\nimage-20241016104245339 异或（XOR)法\n位拼接法和异或法 基于全局历史的分支预测 对一条分支指令进行分支预测，考虑前面分支指令的执行结果\n需要一个全局历史寄存器（GHR(global history register)),记录最近执行的所有分支指令的结果、\n最理想的情况是对每条分支指令都使用一个PHT\n一个全局寄存器 + 每一条分支指令对应的PHT\nimage-20241016144951117 量变引起质变，当局部BHR少到只剩下一个的时候，就是全局GHR\nimage-20241016144931624 总结 两种分支预测方法\n局部历史分支预测：基于分支指令自身在过去的执行状况来进行分支预测，对每一条分支指令都使用分支历史寄存器（BHR),并使用了由两位饱和计数器组成的PHT(Pattern History Table)来捕捉每一个BHR的规律，使用BHR和PHT配合进行分支预测\n全局历史分支预测：基于一条分支指令之前的一些分支指令的执行状况来进行分支预测，使用全局历史寄存器（GHR)记录所有分支指令的执行情况，由两位饱和计数器组成的PHT(Pattern History Table)来捕捉每一个GHR的规律,使用GHR和PHT配合进行分支预测\n竞争的分支预测 竞争的分支预测原理图 竞争的分支预测_更详细的原理图 理想情况下每一条分支指令都有一个CPHT(choice PHT)\nCPHT中的两位饱和计数器 当P1预测正确,P2预测错误时,计数器减1 当P1预测错误,P2预测正确时,计数器加1 当P1和P2预测结果一致时,不管预测正确与否,计数器保持不变 对于每一条指令,在GHR内容不同时,会导致使用不同的分支预测方法,所以将PC值与GHR进行相应运算再去寻址CPHT的地址.\n分支预测的更新 历史寄存器 （1）在流水线的取指令阶段，进行分支预测，根据预测的结果更新\n（2） 在流水线的执行阶段，当分支指令的方向被计算出来时，更新,分支指令可能在错误预测的路径上,造成错误\n（3） 在流水线的提交阶段，当分支指令要离开流水线是，更新 ,最保险的方法,但是浪费了性能\n一条分支指令b在时间t被分支预测,在时间 $t + \\Delta t$ 从流水线退休, 任何在 $ \\Delta t $内的时间被预测的分支指令都不会从分支指令的结果受益\nimage-20241016153738302 采取方法1更新 ,但是会出现分支预测失败的情况 ,即使后续的分支指令使用的错误的 GHR ,由于他们在预测失败的路径上,都会从流水线中被抹去\n修复GHR错误值的方法 提交(commit)阶段修复法 前端阶段Speculative GHR, 提交阶段放置一个 Ritired GHR, 在前端推测失败之后,需要等待分支指令退休的时候,将后端的GHR写到前端的GHR中,然后根据这条分支指令所指定的目标地址,重新取指令执行.\n该方法的缺点是会造成分支预测失败时惩罚的增大,(why?)\n利用提交阶段的GHR修复分支预测器的GHR checkpoint修复法 在取指令阶段更新GHR时,可以把旧的GHR值保存起来,保存的内容称为checkpoint GHR .一旦分支指令的结果在流水线中被计算出来,就可以对分支指令的分支预测是否正确进行检查.如果分支预测正确,说明GHR中的值是正确的,如果预测失败,将这条分支指令对于的checkpoint GHR恢复到前端的GHR中,并从这条分支指令正确的目标地址开始取指令执行\n我的理解是把原来的GHR和分支预测结果的反向结合然后放进fifo中,在预测失败时把这个值取出来\n如果是顺序执行,读取存储器的方式也可以用FIFO,\n方式二是对方式一的一种补充,使得能够在执行阶段也去实现恢复\n利用checkpoint的方法会GHR进行修复 修复BHR错误值的方法 方式和修复GHR基本是类似的,并且BHR很少出现一条分支指令在流水线的提交阶段更新BHR,流水线中又出现了这条分支指令使用BHR进行分支预测的情况,除非循环体很短\nimage-20241016181356130 两位饱和寄存器 由于饱和寄存器一般是处在饱和状态的,所以选择在分支指令退休的时候更新PHT的饱和计数器,也不会产生很大的负面影响\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-15T14:08:47+08:00","permalink":"https://VastCircle.github.io/2024/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E7%AC%94%E8%AE%B02/","title":"分支预测(概述+方向预测)"},{"content":"超标量处理器概览 超标量处理器的流水线 顺序执行 in-order pipline 假设流水线每周期可以从 I-Cache 中取出两条指令来执行，则称为2-way 的超标量处理器，在指令经过解码之后，需要根据自身的类型，将两条指令送到对应的 FU 中执行，这个过程称为发射(Issue)。在这个阶段，指令会读取寄存器而得到操作数，同时根据指令的类型，将指令送到对应的 FU 中进行执行。在执行阶段使用了三个 FU:第一个 FU 用来执行 ALU 类型的指令，第二个 FU 用来执行访问存储器类型的指令，第三个 FU 用来执行乘法操作，因为要保证流水线的写回(Write back)阶段是顺序执行的，因此所有 FU 都需要经历同样周期数的流水线，ScoreBoard 用来记录流水线中每条指令的执行情况，例如一条指令在哪个 FU 中执行，在什么时候这条指令可以将结果计算出来等， 一个典型的 ScoreBoard如下。\nscoreboard P: Pending,表示指令的结果还没有写回到逻辑寄存器中。\nF:一条指令在哪个 FU 中执行，在将指令结果进行旁路时会使用这个信息。\nResult Position:在这个部分记录了一条指令到达 FU 中流水段的哪个阶段，3 表示指令处于 FU 流水线的第一个流水段，1 表示指令到达 FU 流水段的最后一个阶段， 0 表示指令处于流水线的写回阶段，在流水线的发射阶段，会将指令的信息写到ScoreBoard 中，同时，这条指令会查询 ScoreBoard 来获知自己的源操作数是否都准备好了，在这条指令被送到 FU 中执行之后的每个周期，都会将这个值右移一位，这样使用这个值就可以表达出指令在 FU 中执行到哪个阶段，对于执行 ALU 类型指令的第一个 FU 来说，当指令到达 3 时，就可以将它的结果进行旁路了；而对于执行乘法指令的第三个 FU 来说，只有当指令到达 1 时，才可以将它的结果进行旁路。本书采取的应该是第二种。\nimage-20241014210458774 阻塞发生在译码级\n指令能够跳转到发射级的条件是scoreboard 对应处在级为2\n？一发就发两条，两条必须要同步吗 应该只是由于下条导致的等待\n指令D不能提前发射应该就是由于需要等待指令C发射\n指令C无法进入执行是由于前递的问题，需要等待指令A的前递\nimage-20241014212008795 乱序执行 乱序执行流水线 解码(Decode)阶段：为了在乱序执行时解决 WAW 和 WAR 这两种相关性，需要对寄存器进行重命名(register renaming),这个过程可以在流水线的解码(Decode)阶段完成，也可以单独使用一个流水段来完成，处理器中需要增加物理寄存器堆(Physical Register File, PRF)来配合对指令集中定义的寄存器( Architecture Register File,ARF)进行重命名，PRF 中寄存器的个数要多于 ARF。\n**Dispatch(分发):**在这个阶段，被重命名之后的指令会按照程序中规定的顺序，写到发射队列(Issue Queue)、重排序缓存(ROB)和 Store Buffer 等部件中，如果在这些部件中没有空闲的空间可以容纳当前的指令，那么这些指令就需要在流水线的重命名阶段进行等待，这就相当于暂停了寄存器重命名以及之前的所有流水线，直到这些部件中有空闲的空间为止。分发阶段可以和寄存器重命名阶段放在一起，在一些对周期时间要求比较紧的处理器中，也可以将这个部分单独使用一个流水段。\n发射(Issue)阶段：一旦指令的操作数准备好了，就可以从发射队列中离开，送到对应的 FU 中执行，因此发射阶段是流水线从顺序执行到乱序执行的分界点。每个 FU 都有自己的流水线级数，在这种流水线中，由于每个 FU 的执行周期数都不相同，所以指令在流水线的写回(Write Back)阶段是乱序的，在这个阶段，一条指令只要计算完毕， 就会将结果写到 PRF中，由于分支预测失败( mis-prediction)或者异常( exception)的存在，PRF 中的结果未必都会写到 ARF 中，因此也将 PRF 称为 Future File。\nRegister File Read(读取寄存器):被仲裁电路选中的指令需要从物理寄存器堆(Physical Register File,PRF)中读取操作数，一般情况下，被仲裁电路选中的指令可以从PRF 中得到源操作数，当然还有“不一般”的情况，那就是指令不能从 PRF 中得到操作数， 但是却可以在送到 FU 中执行之前，从旁路网络(bypassing network)中得到操作数，事实上很大一部分指令都是通过旁路网络获得操作数的，这也为减少 PRF 的读端口提供了可能。由于超标量处理器每周期需要执行好几条指令，PRF 所需要的端口个数也是比较多的，多端口寄存器堆的访问速度一般都不会很快，因此在现实世界的处理器中，这个阶段都会单独使用一个流水段。\n提交(Commit)阶段：为了保证程序的串行结果，指令需要按照程序中规定的顺序更新处理器的状态，这需要使用一个称为重排序缓存(ROB)的部件来配合，流水线中的所有指令都按照程序中规定的顺序存储在重排序缓存中，使用重排序缓存来实现程序对处理器状态的顺序更新，一条指令在这个阶段，会将它的结果从 PRF 搬移到 ARF 中，同时重排序缓存也会配合完成对异常(exception)的处理，如果不存在异常，那么这条指令就可以顺利地离开流水线， 并对处理器的状态进行更改，此时称这条指令退休(retire)了，一条指令一旦退休，它就再也不可能回到之前的状态了。\n因为 store 指令需要写存储器，如果在流水线的写回阶段就将 store 指令的结果写到存储器中，那么一旦由于分支预测失败或者异常等原因，需要将这条 store 指令从流水线中抹掉时，就没有办法将存储器的状态进行恢复了，因为存储器中原来的值已经被覆盖， Store Buffer(SB),来存储 store 指令没有退休之前的结果，store 指令在流水线的写回阶段，会将它的结果写到 Store Buffer 中，只有一条 store 指令真的从流水线中退休的时候，才可以将它的值从 Store Buffer 写到存储器中。使用了这个部件之后，Load 指令此时除了从 D-Cache 中寻找数据，还需要从 Store Buffer 中进行查找，这样在一定程度上增加了设计的复杂度。\n在重排序这里也会处理异常 ， 如果没有异常就会写入ARF, 并成功退休，但是无论有没有异常都会写入SB。退休了才可以去修改相应状态。\n发射阶段选择相应的指令并且送到FU,被选择的指令才会去读取物理寄存器\n写回阶段进行统一旁路，为什么我写的RISCV有这么多的旁路网络 ?\nimage-20241015131538323 image-20241014212008795 附录 参考文献 超标量处理器设计\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处\n","date":"2024-10-14T16:43:30+08:00","permalink":"https://VastCircle.github.io/2024/%E8%B6%85%E6%A0%87%E9%87%8F%E5%A4%84%E7%90%86%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/","title":"超标量处理器概览"},{"content":"Tomasulo\u0026rsquo;s algorithm Tomasulo‘s algorithm创新 Tomasulo算法的主要创新包括硬件实现的寄存器重命名、为所有执行单元设计的保留站（reservation stations），以及一个公共数据总线（CDB），通过该总线计算出的值可以广播到所有可能需要它们的保留站。这些创新使得指令能够实现更好的并行执行，避免在使用记分板或其他早期算法时可能导致的停滞.\nTomasulo_Architechure CDB总线 公共数据总线（CDB）将保留站直接连接到功能单元。根据Tomasulo的设计，它“在保持优先顺序的同时促进并发执行” 。这带来了两个重要影响：\n功能单元可以直接访问任何操作的结果，而无需通过浮点寄存器。这使得多个等待同一结果的单元可以继续执行，而不必等待解决对寄存器文件读端口的争用问题。 危险检测和控制执行是分布式的。保留站负责控制指令何时可以执行，而不是依赖一个专门的危险单元来进行统一管理。 指令顺序 指令是按顺序发出的，因此即使它们是乱序执行的（即非顺序执行），指令序列的效果（如指令引发的异常）仍然会按照顺序执行处理器中的顺序发生。这确保了乱序执行不会影响程序的正确性和预期行为\n寄存器重命名 Tomasulo算法通过寄存器重命名来实现正确的乱序执行。所有的通用寄存器和保留站寄存器要么保存真实值，要么保存占位符值。如果在发射阶段某个目标寄存器的真实值不可用，则最初会使用占位符值。占位符值是一个标签，指示哪个保留站将生成真实值。当功能单元完成计算并在公共数据总线（CDB）上广播结果时，占位符将被真实值替换。\n每个功能单元都有一个保留站。保留站保存执行单条指令所需的信息，包括操作和操作数。当功能单元空闲且指令所需的所有源操作数均为真实值时，功能单元便开始处理指令。\n附录 参考文献 乱序执行CPU\nwikipedia Tomasulo\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-12T16:08:08+08:00","permalink":"https://VastCircle.github.io/2024/%E4%B9%B1%E5%BA%8F%E6%89%A7%E8%A1%8Ccpu/","title":"乱序执行CPU"},{"content":"Abstract 当今的高性能处理器通过乱序执行来容忍长延迟操作。然而，随着延迟的增加，如果我们要继续容忍这些延迟，指令窗口的大小必须增加得更快。本文提出先行(runahead)执行是提高乱序处理器内存延迟容忍度(memory latency tolerance)的有效方法，而不需要不合理的大指令窗口。超前执行可解除因长延迟操作而阻塞的指令窗口的阻塞，从而使处理器能够在程序路径中提前执行,这会导致数据在需要之前就被预取到缓存中。\nintroduction 乱序执行处理器上的超前执行不会将长延迟操作“移开”（这需要在指令窗口中缓冲它及其后面的指令），而是将其扔出指令窗口。\n当指令窗口被一个长延迟操作阻塞时，架构寄存器文件(architectural register file)的状态会被checkpoint保存。然后处理器进入“超前运行模式”。它为阻塞操作分配一个虚假结果并将其扔出指令窗口。阻塞操作后的指令被**获取、执行，并以伪退休（pseudo-retired）**的方式从指令窗口中移除。当阻塞操作完成时，处理器重新进入“正常模式”。此时，它会恢复之前保存的检查点状态，并从阻塞操作开始重新获取和执行指令。 伪退出(pseudo-retire):指令按照传统意义上的方式执行和完成，只是它们不更新架构状态。\nimage-20241012153516456 Runahead 的好处来自于将被长延迟操作阻塞的小指令窗口转换为非阻塞窗口，从而使其具有大得多的窗口的性能。\n在这篇论文中，仅评估了runahead mode对于在二级缓存失效的内存操作的表现，尽管它也可以在任何阻塞指令窗口的长延迟操作上启动。基于英特尔奔腾4处理器的机器模型，该处理器拥有128个条目的指令窗口。 首先展示了当前的乱序执行引擎无法容忍长延迟的主存访问时间。接下来，展示了runahead mode如何更好地应对这些延迟，并且能够达到一个具有更大指令窗口的机器的性能。\nRelate work 暂无\nOut-of-order execution and memory latency tolerance(乱序执行和内存容忍度) Instruction and scheduling windows 乱序执行比顺序执行更能容忍缓存缺失，因为它能够调度与缓存缺失无关的操作。乱序执行的机器通过两个窗口实现这一点：指令窗口和调度窗口。 指令窗口保存所有已解码但尚未提交到架构状态的指令(ROB)，其主要目的是保证指令按顺序退休，以支持精确异常。 调度窗口包含指令窗口中的一部分指令，其主要目的是每个周期搜索那些准备好执行的指令，并对它们进行调度执行(类似于发射队列)。\n当一个长延迟操作发生时，它会阻塞指令窗口，直到操作完成。尽管后续的指令可能已经执行完成，但它们无法从指令窗口中退休(顺序)。如果操作的延迟时间足够长，并且指令窗口不够大，指令会在窗口中堆积，最终导致指令窗口被填满。此时，机器会停顿并停止向前执行。\nMemory latency tolerance 取指理想 变 调度窗口 L2理想程度 指令窗口\n图 1 显示了七台不同机器的指令窗口停滞的周期百分比。每个栏顶部的数字是机器的IPC。该数据是所有模拟基准的平均值。\n具有完整指令窗口停顿的周期的百分比 Runahead 的性能优势来自于将指令提取到提取引擎的缓存中，并执行未命中一级或二级缓存的独立加载和存储。\nImplementation of runahead execution in an out-of-order processor 在本节中，我们描述了在乱序处理器上实现超前执行的情况，其中指令在被调度后并在执行之前访问寄存器文件。Intel Pentium 4 处理器 [13]、MIPS R10000 微处理器 [30] 和 Compaq Alpha 21264 处理器 [18] 是这种微架构的例子。在其他一些微架构中，例如 Intel Pentium Pro 处理器 [12]，指令在放入调度器之前访问寄存器文件。\nFrontend RAT(Register Alias Table)用于寄存器重命名，并包含架构寄存器到物理寄存器的推测映射。\nRetirement RAT 包含指向包含已提交架构值的物理寄存器的指针。它用于在分支错误预测和异常之后恢复状态。\nimage-20241010195821755 Entering runahead mode **当内存操作在二级缓存中未命中且该内存操作到达指令窗口的头部时，处理器进入超前执行模式。**导致进入超前执行模式的指令地址会被记录。为了在从超前运行模式退出时正确恢复架构状态，处理器对架构寄存器文件的状态进行checkpoint。出于性能原因，处理器还检查分支历史寄存器和返回地址堆栈的状态。\n架构寄存器文件的checkpoint可以通过复制提交寄存器别名表（Retirement RAT）指向的物理寄存器内容来完成，但这可能需要时间。为了避免因复制导致的性能损失，处理器可以在正常模式下不断更新checkpoint的架构寄存器文件。当非超前指令从指令窗口中提交时，它会将其结果更新到检查点寄存器文件中的架构目标寄存器。这样检查点操作不会浪费任何时钟周期。\n尽管Retirement RAT 在正常模式下指向架构寄存器状态，但在超前运行模式下它指向伪架构寄存器状态并反映伪退休指令更新的状态\nExecution in runahead mode 无效位和指令:每个物理寄存器都有一个与其关联的无效（INV）位，以指示它是否具有虚假值。任何源自设置了无效位的寄存器的指令都是无效指令。 INV 位用于防止使用虚假数据进行虚假预取和分支解析。 如果存储指令是无效的，它会在runahead期间将一个 INV 值引入内存映像。为了处理runahead mode下数据值(和 INV 值)通过内存的通信，我们使用一个小的“runahead cache”，它与一级数据缓存并行访问。\nINV 值的传播:引入 INV 值的第一条指令是导致处理器进入runahead mode的指令,如果这条指令是加载指令，它会将其物理目的寄存器标记为 INV。如果它是存储指令，则会在runahead cache中分配一行，并将其目标字节标记为 INV。\n任何无效的指令在调度或执行后写入寄存器时，会将该寄存器标记为 INV。任何有效的操作在写入寄存器时，会重置其目的寄存器的 INV 位。\n其实意思就是因为此时存储是没有得到相应结果的,所以后续与目的寄存器相关的指令都是无效的,从第一条无效的指令衍射开\nRunahead store operations and runahead cache 先行存储(store)指令不会将其结果写入任何地方??。因此，依赖于有效先行存储的先行加载被视为无效指令并被丢弃。由于寄存器数量有限，因此将先行存储(store)的结果转发到先行加载(load)对于高性能至关重要。 如果存储及其相关加载都在指令窗口中，则此转发是通过当前乱序处理器中已存在的store buffer来完成的(应该是cache那边的buffer)。 如果超前运行加载依赖于已经pseudo-retired的超前运行存储（这意味着该存储不再位于store buffer???前面是说的runahead store是不会将结果写入任何地方的），则它应该从某个其他位置获取存储的结果。1是写入data cache (提高复杂度,并且可能会占据其他有效指令的位置),2是弄一个大的fully-associative buffer。\n使用 runahead cache 来保存伪退休先行存储的结果和 INV 状态 ，提供指令之间的数据和INV状态的通信,被逐出的缓存行不会写入其他地方。为了支持存储和加载之间 INV 位的正确通信，store buff中的每个条目和runahead cache 中的每个字节都有一个相应的 INV 位。runahead cache 的每个字节还有另一个与其关联的位（STO 位），指示存储是否已写入该字节。仅当访问的字节由存储写入（设置了 STO 位）并且访问runahead cache 有效时，对超前运行高速缓存的访问才会导致命中。\n更新 INV 和 STO的规则:\n当有效的先行存储完成执行时，它将其数据写入其store buffer entry（就像在普通处理器中一样）并重置该条目的关联 INV 位。同时，它查询数据缓存，如果数据缓存未命中，则向内存层次结构发送预取请求。 当一个无效的先行存储被scheduled时，它会set其相关store buff条目的 INV 位。 当一个有效的先行存储离开指令窗口时，它会将其结果写入runahead cache，并重置已写入字节的 INV 位。同时，它还会设置已写入字节的 STO 位。 当一个无效的先行存储离开指令窗口时，如果其地址有效，它会设置写入字节的 INV 位和 STO 位 先行存储从不将结果写入数据缓存??????。 当存储操作的地址无效时，存储操作会被简单地视为一个空操作（NOP）。由于加载操作无法识别与这些无效存储操作的依赖关系，它们可能会错误地从内存中加载一个陈旧的值。这个问题可以通过使用内存依赖预测器来缓解，**预测器可以识别无效地址存储操作与其依赖的加载操作之间的依赖关系。**一旦依赖关系被识别，如果存储操作的数据值是无效的，则加载操作会被标记为无效（INV）；如果存储操作的数据值是有效的，则可以将其forward给加载操作。\nRunahead load operations runahead load invalid :\n源自无效的物理寄存器\n依赖于store buffer中标记为无效（INV）的存储操作\n依赖于一个已经伪退休且是无效（INV）的存储操作(runahead cache)\n有效load会并行访问3个结构 ： data cache , runahead cache , store buffer .\n加载操作命中store buffer ，并且命中的条目被标记为有效，那么加载操作会从store buffer获取数据。 加载操作命中store buffer ，并且命中的条目被标记为无效（INV），那么加载操作会将其物理目标寄存器标记为无效（INV）。\n只有当加载指令访问的cache line有效且其访问的任何字节的 STO 位被set时，该加载才被视为在runahead cache 中命中。 如果load在store buffer未命中但在runahead cache 命中，则它会检查在runahead cache 访问的字节的 INV 位。如果没有INV 位set ，将使用runahead cache 中的数据。如果任意一个源数据字节被标记为 INV，则将其目标寄存器标记为INV。 如果load在store buffer和runahead cache 都未命中，但在data cache中命中，则它将使用data cache中的值，并被视为valid(data cache 应该是不涉及runahead的)。然而，由于以下两个原因，它实际上可能是无效的????：1）它可能依赖于具有 INV 地址的store，(依赖于无效的store 就不应该被判定为有效啊)或者 2）它可能依赖于一个 INV store，该store在runahead cache中将其目标字节标记为 INV，但由于冲突，相应的runahead cache被释放(意思就是,实际上load是无效的,但是由于load所访问的runahead cache被别的指令释放了,比方说另一个store把cache 给挤掉了)。然而，这两种情况都是罕见的，不会显著影响性能。\n如果加载在所有三个结构中都未命中，它会向L2 cache 发送请求以获取其数据。如果该请求在L2 cache 中命中，则数据将从L2 cache 传输到L1 cache ，加载完成其执行。如果请求在L2 cache 中未命中，加载会将其目标寄存器标记为 INV，并像导致进入runahead mode的加载那样(未命中L1 cache)从调度器中移除。该请求会发送到内存像一个未命中 L2 缓存的正常加载请求一样。\nstore buffer \u0026gt; runahead cache \u0026gt; data cache \u0026gt; L2 cache\nExecution and prediction of branches 在runahead mode中，分支的预测和解决方式与正常模式完全相同，唯一的区别是：具有 INV 源(寄存器标记为INV)的分支（与所有分支一样）被预测并以推测的方式更新全局分支历史寄存器，但与其他分支不同，它永远无法被解决。???如果分支预测错误，处理器在获取到该分支后将始终处于错误路径，直到遇到一个与控制流无关的点。我们将获取到错误预测的 INV 分支的程序中的点称为“分歧点”。分歧点的存在不一定对性能有害分歧点在runahead mode中出现得越晚，性能提升就越好。\n前置模式下分支预测器表的训练策略:\n(1)始终训练分支预测器表。如果一个分支首先在前置模式下执行，然后在正常模式下执行，这种策略将导致同一个分支对分支预测器进行两次训练。因此，预测器表的性能得到了增强，计数器可能会失去滞后效应。\n(2)不在前置模式下训练分支预测器。这会导致前置模式下的分支预测准确率降低，从而降低性能，并使分歧点更接近前置入口点。\n(3)第三种选择是始终在前置模式下训练分支预测器，但同时使用一个队列将前置模式下分支的结果传递给正常模式。在正常模式下，如果存在预测，则使用该队列中的预测来进行分支预测。如果一个分支使用来自队列的预测进行预测，则不会再次训练预测器表。\n(4)前置模式和正常模式使用两个独立的预测器表，并在进入前置模式时将正常模式的表信息复制到前置模式。这一选项在硬件实现上成本较高，但我们进行了模拟以确定第一种选项的双重训练策略有多重要。\n我们的结果显示，与第四种选择相比，二次训练分支预测器表条目并没有显著降低性能(方法1)。\nInstruction pseudo-retirement during runahead mode. 在runahead mode下，指令按照程序顺序离开指令窗口。如果某条指令到达指令窗口的队头，它将被考虑进行pseudo-retire。 如果被考虑pseudo-retire的指令是无效的（INV），它会立即从窗口中移除。 如果指令是有效的，它需要等待执行完毕（此时它可能变为无效的），并将结果写入物理寄存器文件。在pseudo-retire时，一条指令会释放为其执行分配的所有资源。\n无论是有效还是无效的指令，在它们离开指令窗口时都会更新退休重命名表（Retirement RAT）。退休重命名表不需要存储与每个寄存器关联的无效（INV）位，因为物理寄存器已经各自关联了无效位。\nExiting runahead mode 可以随时启动退出预运行模式的过程。为了简化处理，我们将退出预运行模式的操作与处理分支预测错误的方式相同。处理器中的所有指令都会被flush，相关的缓冲区会被释放。检查点保存的架构寄存器文件会复制到物理寄存器文件的预定区域。前端和退休阶段的寄存器重命名表（RATs）也会修复，以指向保存架构寄存器值的物理寄存器。这种恢复通过重新加载相同的硬编码映射到两个别名表来实现。预运行缓存中的所有行都将失效（并且 STO 位被清零），在退出预运行模式时，检查点保存的分支历史寄存器和返回地址栈将被恢复。处理器会从导致进入预运行模式的指令地址开始获取指令。\n我们的策略是在阻塞的加载指令从内存中取回数据时退出runahead mode。另一种策略是通过使用定时器提前退出，这样可以消除部分流水线填充或窗口填充的开销。我们发现，对于某些基准测试，这种替代策略表现良好，而在其他基准测试中表现不佳。总体上，提前退出的效果略差。提前退出对于某些基准测试表现较差的原因是，如果处理器不尽早退出预运行模式，可能会生成更多的二级缓存丢失预取请求。\n知识点补充 store buffer 分支预测 Architectural Register 架构寄存器是指每个CPU独有的一组全局寄存器，这些寄存器不与其他CPU共享。它们可以存储任意类型的数据，并且能够在CPU内部的线程之间实现快速通信。\n参考文献 paper-reading\narchitectural register\n浅谈乱序执行CPU\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-10T14:23:54+08:00","permalink":"https://VastCircle.github.io/2024/runahead_execution_an_alternative_to_very_large_instruction_windows_for_out-of-order_processors/","title":"Runahead_Execution_An_Alternative_to_Very_Large_Instruction_Windows_for_Out of Order_Processors"},{"content":"chipyard 从下载到构建 git clone https://github.com/ucb-bar/chipyard.git cd chipyard git checkout 1.10.0 ## 为了使得clone顺利，把http都换成ssh ，使用命令 find . -name \u0026#34;.gitmodules\u0026#34; -type f -exec sed -i \u0026#39;s/https:\\/\\/github.com\\//git@github.com:/g\u0026#39; {} + ## 同步 git submodule sync ## 运行初始化脚本 ./build-setup.sh ## 导入conda环境 source ./env.sh ## 初始化software ，例如coremark ./scripts/init-software.sh 配置一个2核心soc chipyard 配置文件 chipyard的配置文件是在chipyard/generators/chipyard/src/main/scala/config中，\nclass MyCoreConfigs extends Config( new freechips.rocketchip.subsystem.WithNBigCores(2) ++ // single rocket-core new chipyard.config.AbstractConfig) 在sim/verilator界面去执行命令,可以生成文件 simulator-chipyard-MyCoreConfig\nmake CONFIG=MyCoreConfig 裸机编译riscv #include \u0026lt;stdio.h\u0026gt; int main(void) { printf(\u0026#34;Hello, World!\\n\u0026#34;); return 0; } $ riscv64-unknown-elf-gcc -fno-common -fno-builtin-printf -specs=htif_nano.specs -c hello.c $ riscv64-unknown-elf-gcc -static -specs=htif_nano.specs hello.o -o hello.riscv $ spike hello.riscv Hello, World! -fno-common ​ 默认情况下，C语言会将未初始化的全局变量放在一个“common”区域，可以被多个文件共享。-fno-common 禁止这种行为，要求每个未初始化的全局变量必须在一个文件中定义。\n-fno-builtin-printf ​ 禁用编译器内置的 printf 函数，强制使用标准库中的 printf 函数\n-specs=htif_nano.specs ​ htif_nano.specs 可能是为特定硬件平台（例如 RISC-V）的模拟环境或硬件接口（HTIF）准备的编译和链接配置，确保生成的代码可以在特定环境中运行\n-static\n强制使用静态链接库，而不是动态链接库。所有需要的库代码都会在编译时直接链接到生成的可执行文件中，而不是在运行时动态加载。\n生成波形 make run-binary-debug BINARY=test.riscv 应该是要重新编译前文生成的bin文件\n## 方法1 make run-binary-debug BINARY=test.riscv CONFIG=MyCoreConfig ## 方法2 ./simulator-chipyard-RocketConfig $RISCV/riscv64-unknown-elf/share/riscv-tests/isa/rv64ui-p-simple 在output/chipyard.harness.TestHarness.MyCoreConfig 可以看到hello.vcd\n使用 gtkwave可以打开hello.vcd 查看\nrocket chip tiles 每个Rocket核心都与一个页表遍历器、L1 指令缓存和 L1 数据缓存组合成一个RocketTile\n每个 CPU 块都有一个 L1 指令缓存和 L1 数据缓存。这些缓存的大小和关联性可以配置。默认RocketConfig 使用 16 KiB、4 路组关联指令和数据缓存\nMemory System 这些图块(Tiles)连接到SystemBus，后者将其连接到 L2 缓存组。然后，L2 缓存组连接到MemoryBus，后者通过 TileLink 到 AXI 转换器连接到 DRAM 控制器\nMMIO 对于 MMIO 外围设备，SystemBus连接到ControlBus和PeripheryBus\nControlBus连接标准外围设备，如 BootROM、平台级中断控制器 (PLIC)、核心本地中断 (CLINT) 和调试单元\nBootROM BootROM 包含第一阶段引导加载程序，即系统复位后运行的第一条指令。它还包含设备树，Linux 会使用它来确定连接了哪些其他外设，具体在 /generators/rocket-chip/bootrom\n#define DRAM_BASE 0x80000000 .section .text.start, \u0026#34;ax\u0026#34;, @progbits .globl _start _start: csrwi 0x7c1, 0 // disable chicken bits li s0, DRAM_BASE csrr a0, mhartid la a1, _dtb jr s0 .section .text.hang, \u0026#34;ax\u0026#34;, @progbits .globl _hang _hang: csrwi 0x7c1, 0 // disable chicken bits csrr a0, mhartid la a1, _dtb csrwi mie, 0 1: wfi j 1b .section .rodata.dtb, \u0026#34;a\u0026#34;, @progbits .globl _dtb .align 5, 0 _dtb: .ascii \u0026#34;DTB goes here\u0026#34; linker.ld\nSECTIONS { ROM_BASE = 0x10000; /* ... but actually position independent */ . = ROM_BASE; .text.start : { *(.text.start) } . = ROM_BASE + 0x40; .text.hang : { *(.text.hang) } . = ROM_BASE + 0x80; .rodata.dtb : { *(.rodata.dtb) } } 第一条指令应该是从0x10000开始\n源码解读 variables.mk ifeq ($(SUB_PROJECT),chipyard) SBT_PROJECT ?= chipyard MODEL ?= TestHarness VLOG_MODEL ?= $(MODEL) MODEL_PACKAGE ?= chipyard.harness CONFIG ?= RocketConfig CONFIG_PACKAGE ?= $(SBT_PROJECT) GENERATOR_PACKAGE ?= $(SBT_PROJECT) TB ?= TestDriver TOP ?= ChipTop endif SUB_PROJECT 这对应于chipyard/generators 目录中的项目之一。更正式地说，它是由相应生成器目录中的 build.sbt 文件中的条目之一以及 Chipyard 根目录中的主 build.sbt 文件定义的\nconstellation icenet rocketchip testchipip hwacha \u0026hellip;\nSBT_PROJECT 这对应于要构建的芯片的顶级存储库。这是定义许多更高级别的构造的地方，例如测试工具和测试平台。\nMODEL 该模型是 Chisel 应该使用的项目的顶级模块。通常，这应该定义为与测试工具相同，但不一定必须如此。\nVLOG_MODEL 这是 FIRRTL/Verilog 应该使用的项目的顶层模块。与 MODEL 一样，这通常与测试工具相同，但不一定需要如此。\nMODEL_PACKAGE 这是用于查找 CPU 整体模型的 Scala 包。这应该对应于 Scala CPU 配置文件中的包 。\nCONFIG 这定义了项目应使用的参数。通常，这用于选择 SBT_PROJECT 中定义的 CPU 配置之一。\nCONFIG_PACKAGE 这是定义 Config 类的 Scala 包。该文件必须包含 Config 的类定义，这意味着对象 Config 必须存在。\nGENERATOR_PACKAGE 这是定义 Generator 类的 Scala 包。该文件必须包含 Generator 的类定义，这意味着 Generator 对象必须存在。\nTB 这定义了测试台包装器，该包装器延伸到测试工具上，以允许在 Verilog 模拟器中进行模拟。默认是TestDriver\n路径位于generators/rocket-chip/src/main/resources/vsrc/TestDriver.v\n里面包含着这个\n`MODEL testHarness( .clock(clock), .reset(reset), .io_success(success) TOP 这是该项目的顶层模块。通常，这是由测试工具实例化的模块。 例如chiptop\n在sim文件里make 发生了什么 ## 拷贝镜像 cp -f /chipyard/resources/testchipip/bootrom/bootrom.rv64.img chipyard/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/bootrom.rv64.img ## Chipyard环境中生成基于MyCoreConfig配置的模拟代码，同时输出生成过程的日志以便后续调试和查看。 (set -o pipefail \u0026amp;\u0026amp; cd CHIPYARD_HOME \u0026amp;\u0026amp; java -cp CHIPYARD_HOME/.classpath_cache/chipyard.jar chipyard.Generator --target-dir CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig ## 指定生成代码的输出目录 --name chipyard.harness.TestHarness.MyCoreConfig ## 指定生成文件的名字 --top-module chipyard.harness.TestHarness ## 指定生成代码的顶层文件 --legacy-configs chipyard:MyCoreConfig ## 指定代码的配置 | tee CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.chisel.log) ## 执行了一个 Chipyard 项目模型生成过程。它读取设计的 FIRRTL 文件和相关注解，生成指定的 SFC 模型和注解文件，以便后续仿真或综合使用。 cd CHIPYARD_HOME \u0026amp;\u0026amp; java -cp CHIPYARD_HOME/.classpath_cache/tapeout.jar barstools.tapeout.transforms.GenerateModelStageMain --no-dedup ## 禁用模块去重 --output-file CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.sfc ## 指定生成SFC文件的路径,用于存储生成的模型数据 --output-annotation-file CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.sfc.anno.json ## 生成模型的注解文件,将附加信息保存为json文件 --target-dir CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/gen-collateral ## 指定生成的附加文件的目标目录.gen-collateral 用于存放存放其他的生成数据和文件。 --input-file CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.fir ## 输入的 FIRRTL 文件，用于描述整个设计的结构和逻辑。 --annotation-file CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.appended.anno.json ## 输入注解文件，包含了与 FIRRTL 文件相关的额外信息。 --log-level error ## 设置日志等级为 error，只输出错误信息，以减少不必要的日志信息。 --allow-unrecognized-annotations ##允许工具跳过无法识别的注解，保证生成过程顺利进行。 -X firtool \\ --format=fir \\ # 指定输入文件为 FIRRTL 格式 --dedup \\ # 启用模块去重，减少重复模块 --export-module-hierarchy \\ # 导出模块的层次结构，便于理解模块关系 --emit-metadata \\ # 生成包含设计元数据的文件 --verify-each=true \\ # 每次转换后验证 FIRRTL，确保转换正确性 --warn-on-unprocessed-annotations \\ # 对无法识别的注解给出警告，便于调试 --disable-annotation-classless \\ # 禁用无明确类型的注解，避免生成不确定信息 --disable-annotation-unknown \\ # 禁用未知类型的注解，确保生成过程可控 --mlir-timing \\ # 记录 MLIR 的执行时间，便于性能分析 --lowering-options= \\ # 控制降低选项（留空使用默认设置） --repl-seq-mem \\ # 启用序列内存替换，将高层内存结构转换为可综合内存 --repl-seq-mem-file=CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.mems.conf \\ # 指定内存替换配置文件，描述生成的内存模块 --repl-seq-mem-circuit=TestHarness \\ # 指定内存替换的电路名称 --annotation-file=CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.sfc.anno.json \\ # 输入注解文件，提供生成过程所需的额外信息 --split-verilog \\ # 将生成的 Verilog 拆分为多个文件，便于管理 -o CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/gen-collateral \\ # 指定输出目录，存放生成的文件 CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.sfc.fir # 输入 FIRRTL 文件，包含电路的高层次描述 ## 这条命令运行 uniquify-module-names.py 脚本，用于对模块名称进行唯一化。其目的在于解决模块名重复问题，确保在文件列表和层次结构中，每个模块的名称都是唯一的，便于 Verilog 生成和仿真流程的管理。 CHIPYARD_HOME/scripts/uniquify-module-names.py \\ --model-hier-json CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/model_module_hierarchy.json \\ # 输入：模型模块的层次结构 JSON 文件 --top-hier-json CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/top_module_hierarchy.json \\ # 输入：顶层模块的层次结构 JSON 文件 --in-all-filelist CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/gen-collateral/filelist.f \\ # 输入：包含所有模块的文件列表 --dut ChipTop \\ # 指定待验证的顶层模块为 ChipTop --model TestHarness \\ # 指定模型的顶层模块为 TestHarness --target-dir CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/gen-collateral \\ # 指定目标目录，用于存放输出文件 --out-dut-filelist CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.top.f \\ # 输出：顶层模块的文件列表 --out-model-filelist CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.model.f \\ # 输出：模型模块的文件列表 --out-model-hier-json CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/model_module_hierarchy.uniquified.json \\ # 输出：唯一化后的模型模块层次结构 JSON 文件 --gcpath CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/gen-collateral # 指定生成文件的路径（用于在唯一化过程中查找生成文件）\\ ## 复制TestDriver.v到gen-collateral文件夹 cp -f CHIPYARD_HOME/generators/rocket-chip/src/main/resources/vsrc/TestDriver.v CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/gen-collateral ## 运行 split-mems-conf.py 脚本，将包含所有内存配置的文件分离为顶层模块和模型模块各自独立的内存配置文件。使顶层模块 ChipTop 和模型模块 TestHarness 各自拥有独立的内存配置文件，便于后续的综合和仿真过程。 CHIPYARD_HOME/scripts/split-mems-conf.py \\ --in-smems-conf CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.mems.conf \\ # 输入：包含所有内存的配置文件 --in-model-hrchy-json CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/model_module_hierarchy.uniquified.json \\ # 输入：唯一化的模型模块层次结构 JSON 文件 --dut-module-name ChipTop \\ # 指定顶层模块名称为 ChipTop --model-module-name TestHarness \\ # 指定模型模块名称为 TestHarness --out-dut-smems-conf CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.top.mems.conf \\ # 输出：用于顶层模块 ChipTop 的内存配置文件 --out-model-smems-conf CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.model.mems.conf # 输出：用于模型模块 TestHarness 的内存配置文件 ### 这条命令使用 MacroCompiler 工具，根据内存配置生成顶层模块的内存宏单元。通过指定 synflops 模式，可以确保生成的内存结构符合同步触发器的合成要求，并生成对应的 Verilog 文件，用于后续的仿真或综合步骤。 cd CHIPYARD_HOME \u0026amp;\u0026amp; java -cp CHIPYARD_HOME/.classpath_cache/tapeout.jar barstools.macros.MacroCompiler \\ -n CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.top.mems.conf \\ # 输入内存配置文件，为顶层模块生成内存宏单元 -v CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/gen-collateral/chipyard.harness.TestHarness.MyCoreConfig.top.mems.v \\ # 输出 Verilog 文件，生成的内存宏单元描述 -f CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.top.mems.fir \\ # 输入 FIRRTL 文件，用于描述内存结构 --mode synflops # 指定生成模式为 synflops，用于同步触发器合成 ### 这条命令与之前的命令类似，但这是针对模型模块 TestHarness 的内存配置。它使用 MacroCompiler 工具生成该模块的内存宏单元，确保生成的内存结构符合同步触发器的合成要求。生成的 Verilog 文件将用于后续的仿真或综合过程。 cd CHIPYARD_HOME \u0026amp;\u0026amp; java -cp CHIPYARD_HOME/.classpath_cache/tapeout.jar barstools.macros.MacroCompiler \\ -n CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.model.mems.conf \\ # 输入：模型模块的内存配置文件 -v CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/gen-collateral/chipyard.harness.TestHarness.MyCoreConfig.model.mems.v \\ # 输出：生成的 Verilog 文件，描述模型模块的内存宏单元 -f CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.model.mems.fir \\ # 输入：用于描述模型模块内存结构的 FIRRTL 文件 --mode synflops # 指定生成模式为 synflops，以支持同步触发器的合成 ## 这条命令使用 Verilator 工具生成一个基于 TestDriver 顶层模块的 C++ 模拟器 verilator --main --timing --cc --exe \\ -CFLAGS \u0026#34; -O3 -std=c++17 -ICHIPYARD_HOME/.conda-env/riscv-tools/include -ICHIPYARD_HOME/tools/DRAMSim2 -ICHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/gen-collateral -DVERILATOR -include CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig.plusArgs\u0026#34; \\ # 编译标志，包含优化选项、标准和包含路径 -LDFLAGS \u0026#34; -LCHIPYARD_HOME/.conda-env/riscv-tools/lib -Wl,-rpath,CHIPYARD_HOME/.conda-env/riscv-tools/lib -LCHIPYARD_HOME/sims/verilator -LCHIPYARD_HOME/tools/DRAMSim2 -lriscv -lfesvr -ldramsim \u0026#34; \\ # 链接标志，指定库路径和链接的库 --threads 1 --threads-dpi all -O3 --x-assign fast --x-initial fast \\ # 设置线程数，优化和快速初始化选项 --output-split 10000 --output-split-cfuncs 100 --assert -Wno-fatal \\ # 输出分割设置、启用断言，禁止致命警告 --timescale 1ns/1ps --max-num-width 1048576 \\ # 设置时间尺度和宽度限制 +define+CLOCK_PERIOD=1.0 +define+RESET_DELAY=777.7 +define+PRINTF_COND=TestDriver.printf_cond \\ # 定义宏，包括时钟周期和重置延迟 +define+STOP_COND=!TestDriver.reset +define+MODEL=TestHarness \\ # 进一步定义条件和模型 +define+RANDOMIZE_MEM_INIT +define+RANDOMIZE_REG_INIT +define+RANDOMIZE_GARBAGE_ASSIGN +define+RANDOMIZE_INVALID_ASSIGN +define+VERILATOR \\ # 启用随机化选项 --top-module TestDriver --vpi \\ # 指定顶层模块为 TestDriver，启用 VPI -f CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/sim_files.common.f \\ # 输入文件列表 -o CHIPYARD_HOME/sims/verilator/simulator-chipyard.harness-MyCoreConfig \\ # 指定输出的模拟器文件 -Mdir CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig \\ # 指定模型目录 -CFLAGS \u0026#34;-include CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig/VTestDriver.h\u0026#34; # 额外的编译标志，包含 VTestDriver 头文件 ## 建立VtestDriver.mk touch CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig/VTestDriver.mk ## 编译DRAMSim2的源文件 make -C CHIPYARD_HOME/tools/DRAMSim2 libdramsim.a ## 使用VTestDriver.mk来生成可执行文件simulator-chipyard.harness-MyCoreConfig make VM_PARALLEL_BUILDS=1 -C CHIPYARD_HOME/sims/verilator/generated-src/chipyard.harness.TestHarness.MyCoreConfig/chipyard.harness.TestHarness.MyCoreConfig -f VTestDriver.mk 附录 参考文献 An Introduction to Declarative CPU Design and FPGA Development using the Chipyard SoC Design Framework\nchipyard手册\nhttps://www.cnblogs.com/hwzhao/p/17363380.html\nTops,Test-Harnesses,and the Test-Driver\n版权信息 本文原载于 vastcircle.github.io，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。\n","date":"2024-10-07T16:15:07+08:00","permalink":"https://VastCircle.github.io/2024/chipyard-learning/","title":"Chipyard Learning"},{"content":"安装Hugo ubuntu 系统使用\nsudo apt install hugo 使用以下命令进行验证\nhugo version 创建 Hugo 网站 通过上述命令安装 hugo 程序后，就可以通过 hugo new site 命令进行网站创建、配置与本地调试了。\nhugo new site robin-site 配置主题 当通过上文命令创建我们的站点后，需要进行主题配置，Hugo 社区有了很丰富的主题，可以通过官网 Themes 菜单选择自己喜欢的风格，查看预览效果，选择后可以进入主题项目仓库，一般都会有很详细的安装及配置说明。\n官方主题网站: https://themes.gohugo.io/\n主题推荐:\nPure: https://themes.gohugo.io/hugo-theme-pure/ 关联主题仓库 https://github.com/reuixiy/hugo-theme-meme/blob/main/README.zh-cn.md\n我们可以将主题仓库直接 git clone 下来进行使用，例如在根目录robin-site下运行以下代码，即可下载pure主题.\ngit clone https://github.com/xiaoheiAh/hugo-theme-pure themes/pure 这种方式有一些弊端，当之后自己对主题进行修改后，可能会与原主题产生一些冲突，不方便版本管理与后续更新。官方更推荐使用的是将原主题仓库 fork 到自己的账户，并使用 git submodule 方式进行仓库链接，这样后续可以对主题的修改进行单独维护。\ncd robin-site/ git init git submodule add https://github.com/pseudoyu/pure themes/pure 然后在根目录下的 config.toml文件中添加新的一行:\ntheme = \u0026#34;pure\u0026#34; 更新主题 如果是 clone 了其他人的博客项目进行修改，则需要用以下命令进行初始化：\ngit submodule update --init --recursive 如果需要同步主题仓库的最新修改，需要运行以下命令：\ngit submodule update --remote hugo-theme-meme主题配置 ## 安装meme git submodule add --depth 1 https://github.com/reuixiy/hugo-theme-meme.git themes/meme ## 替换配置 rm config.toml \u0026amp;\u0026amp; cp themes/meme/config-examples/zh-cn/config.toml config.toml zozo 主题配置 git submodule add https://github.com/varkai/hugo-theme-zozo themes/zozo rm config.toml \u0026amp;\u0026amp; cp themes/zozo/config.toml config.toml https://gojun.me/posts/hello-hugo-blog/\nHugo-theme-stack主题配置 https://stack.jimmycai.com/guide/getting-started\n新建博文 完成后，可以通过 hugo new 命令发布新文章。\nhugo new posts/test.md --- title: \u0026#34;Test\u0026#34; date: 2022-10-21T19:00:43+08:00 draft: true --- 这个命令会在 content 目录下建立 post 目录，并在 post 下生成 test.md 文件，博文书写就在这个文件里使用 Markdown 语法完成。博文的 front matter 里draft 选项默认为 true，需要改为 false 才能发表博文，建议直接更改上面说的archetypes 目录下的 default 文件，把 draft: true 改为 draft: false，这样生成的博文就是默认可以发表的。\n生成网页 为了查看生成的博客的效果，我们在本地编辑调试时可以通过 hugo server 命令进行本地实时调试预览，无须每次都重新生成。在cmd中运行以下命令，即我们可以通过浏览器 http://localhost:1313/ 地址访问我们的本地预览网页。\nhugo server -D 但此时只能在本地访问，如果想发布到 Github Pages ， 还需要借助 GithubPages 工具。\n配置文件 打开配置config.toml可以看到很多的参数可以配置，这里只描述最基本的内容，不同的主题可能会支持不同的参数配置，具体请看对应主题的说明文档。baseURL是站点的域名。title是站点的名称。theme是站点的主题。还有关于评论和打赏的相关配置，这些配置都可以参考官网主题的说明。\n每次发布的时候，都需要先执行hugo，把新写的文档按照主题进行渲染，所有生成的文件默认都在当前pulic的子目录下，可以在config里面配置到其他目录。然后把所有新的文件提交到github。提交代码之后，要等一段时间才生效。\ngithub actions 部署 两个仓库 如果想使用 Github Actions 自动部署 hugo 博客，则最起码需要创建两个 Github 的仓库。\n第一个，便是存储博客 .md 源文件的地方，其实就是 hugo 系统； 第二个，则是部署 Github Pages 的仓库，仓库名必须是 \u0026lt;username\u0026gt;.github.io，这是 github 官方要求的。 最终版 主题 使用的是大佬美化后的版本 Mantyke/Hugo-stack-theme-mod。\n因为还是想用github工作流，不使用vercel,所以接下来结合前面的多篇文章操作,第一步是clone fork 之后的仓库，然后修改remote为一个创建好的私人仓库\ngit clone git@github.com:VastCircle/Hugo-stack.git git remote set-url origin git@github.com:VastCircle/hugostack.git 之后通过一系列的git操作将网页部署到gh-pages分支上\nrm -rf public git add . git commit -m \u0026#39;hugo project init\u0026#39; git push -u origin master ## create a new orphand branch (no commit history) named gh-pages git checkout --orphan gh-pages ## Unstage all files git rm -rf --cached $(git ls-files) ## Add and commit that file git add . git commit -m \u0026#34;INIT: initial commit on gh-pages branch\u0026#34; ## Push to remote gh-pages branch git push origin gh-pages ## Return to master branch git checkout master ## Add the gh-pages branch of the repository. It will look like a folder named public git subtree add --prefix=public git@github.com:VastCircle/hugostack.git gh-pages --squash ## Pull down the file we just committed. This helps avoid merge conflicts git subtree pull --prefix=public git@github.com:VastCircle/hugostack.git gh-pages ## Push the public subtree to the gh-pages branch git subtree push --prefix=public git@github.com:VastCircle/hugostack.git gh-pages 貌似失败了\n再来一次 这次把public作为一个独立的仓库，通过.gitignore去屏蔽public ,使得 主仓库不包括 public ,\nrm -rf public ## 主仓库 git add . git commit -m \u0026#39;hugo project init\u0026#39; git push -u origin master ## 推送仓库 hugo cd public git remote add origin https://github.com/VastCircle/VastCircle.github.io.git git add . git commit -m \u0026#34;INIT: initial commit on public\u0026#34; git push -u origin master shell 脚本\n#deploy.sh #!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026#34; # Build the project. hugo # if using a theme, replace by `hugo -t \u0026lt;yourtheme\u0026gt;` # Go To Public folder cd public # Add changes to git. git add -A # Commit changes. msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master # Come Back cd .. 添加 github action .github/workflows/deploy-site.yaml\nname: deploy on: push: branches: [\u0026#34;master\u0026#34;] workflow_dispatch: # schedule: # # Runs everyday at 8:00 AM # - cron: \u0026#34;0 0 * * *\u0026#34; # Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages permissions: contents: read pages: write id-token: write # Allow one concurrent deployment concurrency: group: \u0026#34;pages\u0026#34; cancel-in-progress: true # Default to bash defaults: run: shell: bash jobs: # BUild job build: runs-on: ubuntu-latest env: HUGO_VERSION: 0.134.0 TZ: America/Los_Angeles steps: - name: Checkout uses: actions/checkout@v3 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo id: pages uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.134.0\u0026#39; extended: true - name: Build Hugo env: # For maximum backward compatibility with Hugo modules HUGO_ENVIRONMENT: production HUGO_ENV: production run: hugo --minify - name: Deploy Web id: deployment uses: peaceiris/actions-gh-pages@v3 with: PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} EXTERNAL_REPOSITORY: VastCircle/VastCircle.github.io PUBLISH_BRANCH: master PUBLISH_DIR: ./public commit_message: ${{ github.event.head_commit.message }} 如何编写博客 正如同其他的博客，使用 Markdown 语言来编写博客。Markdown 流行且极易上手，因此这里就不多介绍语法，如果不会的可以自己搜索了解。\n使用 Hugo 创建文章 在博客根目录下运行：\nhugo new post/untitled.md 为什么要用 hugo 来新建而不是创建一个 .md 文件呢？这是因为使用 hugo 创建会自动使用已填入 Front Matter的模板。\nFront Matter 用于标识文章的标题、时间等信息，hugo 就是据此来生成静态页面。关于属性的含义和用法可以参考 Hugo 中文文档。\n模板可以在 \\archetypes\\default.md 下找到：\n--- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; # 标题，创建时自动填充 description: # 文章简介 date: {{ .Date }} # 日期，创建时自动填充，格式同 2023-01-15T12:00:00+08:00 image: # 文章的封面，留空就是没有，填文章所在位置的相对地址，通常放在同目录下， math: # 是否启用 KaTex，填 true 启用 license: # 文章尾部显示的协议，false 为隐藏，其他作为内容，留空就是使用 config.yaml 里默认的 hidden: false # 是否隐藏，一般用不到 comments: true # 因为 bug 所以这个属性只要存在，不管是 true 还是 false 都会导致回复无法显示，需要删掉 draft: true # 是否为草稿，建议改为 false 或者删掉这个属性以防止忘记修改，毕竟我们一般都是写好了才部署到服务器上 --- 为了方便，我参考网络以及 stack-mod 的功能对模板进行了一些改造：\n--- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; slug: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; description: date: \u0026#34;{{ .Date }}\u0026#34; lastmod: \u0026#34;{{ .Date }}\u0026#34; image: cover.png math: license: hidden: false draft: false categories: [\u0026#34;\u0026#34;] tags: [\u0026#34;\u0026#34;] --- ## 附录 ### 参考文献 ### 版权信息 本文原载于 [reincarnatey.net](https://blog.reincarnatey.net)，遵循 CC BY-NC-SA 4.0 协议，复制请保留原文出处。 因此我们可以编写一个批处理程序来快速帮我们生成文章：\ncreate_post.sh：\n#!/bin/bash # 输出提示信息 echo \u0026#34;【创建文章】\u0026#34; # 读取用户输入的 Slug read -p \u0026#34;请输入Slug: \u0026#34; input # 获取当前日期 current_date=$(date +%Y%m%d) # 使用 Hugo 创建新文章 hugo new post/$current_date-$input/index.md # 暂停，提示用户操作完成 read -p \u0026#34;按任意键继续...\u0026#34; 用此批处理程序生成的文章会创建在 \\content\\post\\2023\\0115-test\\index.md，便于我们整理文章资料，同时后续在同目录下存放文章的封面也不会导致内容很乱。\n使用 Hugo 创建类别、标签 创建 Categories 和 Tag 也同理：\nhugo new categories/testcat.md hugo new tags/testtag.md hugo 会自动应用 \\archetypes\\categories.md 和 \\archetypes\\tags.md 的模板，但是这两个模板都不太好，建议都改为：\n--- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; slug: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; description: image: cover.png style: background: \u0026#34;#2a9d8f\u0026#34; color: \u0026#34;#fff\u0026#34; --- create_Categories.sh：\n@echo off echo 【创建类别】 set /p input= 请输入类别名: hugo new categories/%input%/_index.md pause 生成的categories就是在改变如下图所示的界面\nimage1 create_tag.sh：\n@echo off echo 【创建标签】 set /p input= 请输入标签名: hugo new tags/%input%/_index.md pause 注意：如果创建多级文件夹时文章文件名不是 index.md 或者类别、标签文件名不是 _index.md 的话，设置封面图片会出现问题。\n引用 https://hk.v2ex.com/t/1009591\nhttps://jianzhnie.github.io/post/hugo_site/\nhttps://hyrtee.github.io/2023/start-blog/\nhttps://smc.im/post/deploy-hugo-blog-with-github-actions/\n建站技术 | 使用 Hugo + Stack 简单搭建一个博客\nhttps://kaichu.io/posts/my-first-post/\nstack 手册\n部署hugo 踩过的坑_\nhttps://xrg.fj.cn/p/hugo-stack%E4%B8%BB%E9%A2%98%E6%9B%B4%E6%96%B0%E5%B0%8F%E8%AE%B0/\n","date":"2024-10-06T21:57:38+08:00","permalink":"https://VastCircle.github.io/2024/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/","title":"博客搭建"}]