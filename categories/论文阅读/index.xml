<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>论文阅读 on VastCircle's blog</title><link>https://VastCircle.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link><description>Recent content in 论文阅读 on VastCircle's blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 06 Feb 2025 20:42:37 +0800</lastBuildDate><atom:link href="https://VastCircle.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/index.xml" rel="self" type="application/rss+xml"/><item><title>Spatial_Memory_Streaming</title><link>https://VastCircle.github.io/2025/spatial_memory_streaming/</link><pubDate>Thu, 06 Feb 2025 20:42:37 +0800</pubDate><guid>https://VastCircle.github.io/2025/spatial_memory_streaming/</guid><description>&lt;h2 id="摘要">摘要&lt;/h2>
&lt;p>先前的研究表明，应用程序的内存访问模式存在较大的空间变化。然而，现代内存系统使用小的固定大小的缓存块，因此无法利用这些变化。增加缓存块大小不仅会极大地增加引脚和互连带宽的需求，还会增加在共享内存多处理器中发生虚假共享的可能性。本文表明，商业工作负载中的内存访问通常表现出跨越大内存区域（例如，几kB）的重复布局，并且这些访问以可以通过基于代码的关联来预测的模式重复出现。我们提出了一种称为空间内存流的实用片上硬件技术，该技术识别代码相关的空间访问模式，并在需求缺失之前将预测的块流式传输到主缓存中。通过对商业和科学应用进行周期精确的全系统多处理器仿真，我们展示了空间内存流平均可以预测58%的L1缓存缺失和65%的芯片外缓存缺失，平均性能提高37%，最佳性能提升可达到307%。&lt;/p></description></item><item><title>Gaze_into_the_Pattern__for_Hardware_Prefetching</title><link>https://VastCircle.github.io/2025/gaze_into_the_pattern__for_hardware_prefetching/</link><pubDate>Sun, 02 Feb 2025 22:53:52 +0800</pubDate><guid>https://VastCircle.github.io/2025/gaze_into_the_pattern__for_hardware_prefetching/</guid><description>&lt;h2 id="摘要">摘要&lt;/h2>
&lt;p>硬件预取（Hardware prefetching）是隐藏长数据访问延迟最常用的技术之一。为了解决硬件预取面临的挑战，架构师提出在空间区域（spatial region）的粒度上检测并利用空间局部性。当一个新的区域被激活时，他们尝试基于系统级环境特征（如触发指令或数据地址）查找类似的先前访问区域，以进行足迹预测。然而，我们发现这种基于上下文的预测无法捕捉访问模式的本质特征，导致灵活性和实用性受限，并使预取性能次优。在本文中，我们受到存储访问的时间特性的启发，注意到空间足迹内部的时间相关性是空间模式的关键特征。为此，我们提出了一种简单高效的硬件空间预取器——Gaze，它巧妙地利用足迹内部的时间相关性来高效表征空间模式。同时，我们发现利用由空间流（spatial streaming）生成的空间足迹时，存在一个独特但未解决的挑战，即这些足迹表现出极高的访问密度。因此，我们进一步为 Gaze 设计了一个专门的两阶段方法，以缓解传统方案中常见的过度预取（over-prefetching）问题。我们进行了全面且多样化的实验，结果表明 Gaze 能够在更广泛的场景中有效提升性能。具体而言，与最新的低成本方案 PMP 和 vBerti 相比，Gaze 在单核环境下分别提升 5.7% 和 5.4%，在八核环境下分别提升 11.4% 和 8.8%。&lt;/p></description></item><item><title>Scalar_vector_runahead</title><link>https://VastCircle.github.io/2025/scalar_vector_runahead/</link><pubDate>Thu, 23 Jan 2025 22:05:34 +0800</pubDate><guid>https://VastCircle.github.io/2025/scalar_vector_runahead/</guid><description>&lt;p>标量向量运行前 （SVR） 通过搭载处理器上执行的现有指令，在简单的有序内核上提取高内存级并行性，从而导致将来的不规则内存访问。SVR 执行多个瞬态、独立、并行的内存访问实例及其链，这些实例从预测的归纳变量的不同值启动，以将相互独立的内存访问彼此相邻移动，以隐藏依赖的停顿。SVR 的硬件开销仅为 2 KiB，性能比基准 3 宽的有序内核高 3.2× 比完全无序内核高 1.3× 倍，同时能耗减半。将开销增加到 9 KiB 以应对更大的寄存器文件，SVR 可以将相对于乱序内核的加速比扩展到 1.7×。&lt;/p></description></item><item><title>Vector_runahead</title><link>https://VastCircle.github.io/2024/vector_runahead/</link><pubDate>Tue, 17 Dec 2024 14:50:55 +0800</pubDate><guid>https://VastCircle.github.io/2024/vector_runahead/</guid><description>&lt;h2 id="摘要">摘要&lt;/h2>
&lt;p>内存墙对许多现代工作负载的性能造成了重大限制。这些应用程序具有复杂的依赖间接内存访问链，即使是最先进的微架构预取器也无法获取。结果是，当前无序超标量处理器的大部分时间都处于停滞状态。但是，为了实现高内存级别的并行性，标准的提前执行会在缓存未命中之前跳过。在现代工作负载中，这意味着它只预取每个依赖链中的第一个缺少缓存的load 。我们认为，这不是一个根本的限制。如果 runahead 是在 cache 未命中时停止以生成依赖链load ，那么如果它可以同时在多个 cache 上停止，则可以重新获得性能。&lt;/p></description></item><item><title>Decoupled_Vector_Runahead</title><link>https://VastCircle.github.io/2024/decoupled_vector_runahead/</link><pubDate>Tue, 17 Dec 2024 12:28:27 +0800</pubDate><guid>https://VastCircle.github.io/2024/decoupled_vector_runahead/</guid><description>&lt;h2 id="摘要">摘要&lt;/h2>
&lt;p>我们提出了解耦矢量预取 （DVR），这是一种内核内预取技术，与主应用程序线程分开执行，它利用大量内存级并行性来提高具有间接内存访问功能的应用程序的性能。DVR 在运行时动态推断循环边界，识别跨步负载，并矢量化作为间接链一部分的后续指令。它会主动为将来的负载发出内存访问，即使无序内核尚未停止，也会将其数据带入 L1 缓存，从而为主线程提供及时的预取。DVR 可以在运行时调整矢量化程度，在内部循环的多次调用中对同一间接内存访问链进行矢量化，并有效地处理沿矢量化链的分支发散。DVR 作为按需、推测性、按顺序、轻量级硬件子线程与内核内的主线程一起运行，并且产生的最小硬件开销仅为 1139 字节。相对于大型超标量 5 宽无序基线和 Vector Runahead（一种用于加速乱序处理器上的间接内存访问的最新微架构技术），DVR 为一组图形分析、数据库和 HPC 工作负载提供了 2.4× 和 2× 的性能。&lt;/p></description></item><item><title>Secure_prefetching_for_secure_cache_systems</title><link>https://VastCircle.github.io/2024/secure_prefetching_for_secure_cache_systems/</link><pubDate>Sat, 14 Dec 2024 13:17:45 +0800</pubDate><guid>https://VastCircle.github.io/2024/secure_prefetching_for_secure_cache_systems/</guid><description>&lt;h2 id="摘要">摘要&lt;/h2>
&lt;p>像Spectre及其变种这样的瞬态执行攻击可能通过缓存层次结构导致信息泄漏。缓解推测执行攻击的技术分为两类：基于延迟的技术和不可见推测技术。像GhostMinion这样的基于不可见推测的技术是高性能且安全的技术，可以缓解所有类型的推测执行攻击。与缓存系统类似，硬件预取器也可能导致推测性信息泄漏。为了解决这个问题，GhostMinion提倡在缓存系统中基于严格排序的提交时预取。我们的实验表明，GhostMinion缓存系统与硬件预取器之间的互动产生了负面影响，导致不同缓存层次之间的冗余流量。这些流量会引起争用，并增加丢失延迟，从而导致性能下降。接下来，我们观察到，由GhostMinion强制执行的提交时预取导致性能损失，因为它影响了预取器的及时性。我们首次对先进的预取技术和安全缓存系统之间的互动进行了彻底分析。在此基础上，我们提出了两种微架构解决方案，确保在设计安全预取器时能够提供高性能，同时保证安全的缓存系统。第一种解决方案通过非推测性更新缓存层次结构时检测和过滤冗余流量。第二种解决方案确保预取器的及时性，以弥补在提交时触发预取请求的延迟，从而实现既安全又高效的预取器。总体而言，我们的改进是安全的，并且在硬件预取器和安全缓存系统之间提供了协同作用。我们的实验表明，在使用先进的预取器的情况下，我们的过滤器始终能提高像GhostMinion这样的安全缓存系统的性能（对于单核系统提高1.9%，对于多核系统提高19.0%，使用的是性能最好的预取器）。我们还观察到过滤器与我们提出的安全预取器之间的协同效应，进一步提高了性能，单核和多核系统分别提高了6.3%和23.0%（相比于最好的预取器）。我们的改进极为轻量，每个核心的存储开销为0.59 KB。&lt;/p></description></item><item><title>ParaVerser_Harnessing_Heterogeneous_Parallelism_For_Affordable_Fault_Detection_in_Data_Centers</title><link>https://VastCircle.github.io/2024/paraverser_harnessing_heterogeneous_parallelism_for_affordable_fault_detection_in_data_centers/</link><pubDate>Sat, 07 Dec 2024 20:28:43 +0800</pubDate><guid>https://VastCircle.github.io/2024/paraverser_harnessing_heterogeneous_parallelism_for_affordable_fault_detection_in_data_centers/</guid><description>&lt;h1 id="paraverser利用异构并行性实现数据中心中经济实惠的故障检测">ParaVerser：利用异构并行性实现数据中心中经济实惠的故障检测&lt;/h1>
&lt;h2 id="abstract">abstract&lt;/h2>
&lt;p>数据中心运营商已经意识到，由于有缺陷的硅计算单元导致的无声数据损坏是大规模流行的。已经部署了软件扫描仪来缓解该问题，但要么覆盖率低，要么需要数月时间，从而导致长时间不正确行为。相比之下，汽车中使用的冗余机制使所需的功率和面积增加了一倍，因此无法实际部署在服务器空间中。我们推出了 ParaVerser，这是一种高覆盖率、低开销的服务器硬件级错误检测解决方案。通过较小的架构修改，我们使异构服务器级处理器中的传统核心能够充当检查器核心，从而利用异构性、扩展频率和重复运行中固有的并行性来提供节能的错误检查。通过将 big.LITTLE 型无序超标量核心与有序超标量核心动态耦合，在相同保证的情况下，我们相对于典型锁步系统将能源开销降低了 70%，而性能仅下降 4.3%，每核心面积开销为 1064B 。&lt;/p></description></item><item><title>An_Event_Triggered_Programmable_Prefetcher_for_Irregular_Workloads</title><link>https://VastCircle.github.io/2024/an_event_triggered_programmable_prefetcher_for_irregular_workloads/</link><pubDate>Fri, 29 Nov 2024 23:39:07 +0800</pubDate><guid>https://VastCircle.github.io/2024/an_event_triggered_programmable_prefetcher_for_irregular_workloads/</guid><description>&lt;h1 id="针对不规则工作负载的事件触发可编程预取器">针对不规则工作负载的事件触发可编程预取器&lt;/h1>
&lt;h2 id="abstract">abstract&lt;/h2>
&lt;p>许多现代工作负载需要处理大量数据，通常伴随不规则的内存访问。现有架构在处理这些工作负载时表现不佳，因为现有的预取技术无法捕捉内存访问模式，导致这些应用程序严重依赖内存。尽管已经有一些技术可以通过显式配置预取器的遍历模式来显著提高性能，这些方法的适用性通常局限于特定的数据结构。&lt;/p></description></item><item><title>Scalar_runahead_execution</title><link>https://VastCircle.github.io/2024/scalar_runahead_execution/</link><pubDate>Thu, 31 Oct 2024 20:13:06 +0800</pubDate><guid>https://VastCircle.github.io/2024/scalar_runahead_execution/</guid><description>&lt;h3 id="introduction">introduction&lt;/h3>
&lt;p>(i) 一种适用于顺序执行核心的高性能、低开销的硬件预取技术，称为标量前推执行（𝑆𝑅𝐸）。𝑆𝑅𝐸在寄存器传输级有效预取复杂的内存访问模式，并实现了硬件优化策略，以尽量减少能量和面积的开销（如图1所示）。&lt;/p></description></item><item><title>Runahead_Execution_An_Alternative_to_Very_Large_Instruction_Windows_for_Out of Order_Processors</title><link>https://VastCircle.github.io/2024/runahead_execution_an_alternative_to_very_large_instruction_windows_for_out-of-order_processors/</link><pubDate>Thu, 10 Oct 2024 14:23:54 +0800</pubDate><guid>https://VastCircle.github.io/2024/runahead_execution_an_alternative_to_very_large_instruction_windows_for_out-of-order_processors/</guid><description>&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>当今的高性能处理器通过乱序执行来容忍长延迟操作。然而，随着延迟的增加，如果我们要继续容忍这些延迟，指令窗口的大小必须增加得更快。本文提出先行(runahead)执行是提高乱序处理器内存延迟容忍度(memory latency tolerance)的有效方法，而不需要不合理的大指令窗口。超前执行可解除因长延迟操作而阻塞的指令窗口的阻塞，从而使处理器能够在程序路径中提前执行,这会导致数据在需要之前就被预取到缓存中。&lt;/p></description></item></channel></rss>